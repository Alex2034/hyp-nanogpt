W1215 18:53:46.014702 139909995403072 torch/distributed/run.py:779] 
W1215 18:53:46.014702 139909995403072 torch/distributed/run.py:779] *****************************************
W1215 18:53:46.014702 139909995403072 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1215 18:53:46.014702 139909995403072 torch/distributed/run.py:779] *****************************************
using device: cuda:0
using device: cuda:1
Training DataLoader: total number of tokens: 611289391 across 1 files
Validation DataLoader: total number of tokens: 6150326 across 1 files
Logs for this run will be stored in: runs/15.12_68037_euc/
Writing logs to: runs/15.12_68037_euc/tensorboard_logs
step:0/20000 val_loss:7.0863 train_time:1.25s step_avg:nanms
[rank0]:W1215 18:55:22.780959 140625734043456 torch/_dynamo/convert_frame.py:762] [0/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1215 18:55:22.780959 140625734043456 torch/_dynamo/convert_frame.py:762] [0/8]    function: 'forward' (train_gpt2_hyp.py:306)
[rank0]:W1215 18:55:22.780959 140625734043456 torch/_dynamo/convert_frame.py:762] [0/8]    last reason: GLOBAL_STATE changed: grad_mode 
[rank0]:W1215 18:55:22.780959 140625734043456 torch/_dynamo/convert_frame.py:762] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1215 18:55:22.780959 140625734043456 torch/_dynamo/convert_frame.py:762] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1215 18:56:24.626648 140625734043456 torch/_dynamo/convert_frame.py:762] [1/96] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1215 18:56:24.626648 140625734043456 torch/_dynamo/convert_frame.py:762] [1/96]    function: 'forward' (train_gpt2_hyp.py:218)
[rank0]:W1215 18:56:24.626648 140625734043456 torch/_dynamo/convert_frame.py:762] [1/96]    last reason: L['self'].attn.rotary.seq_len_cached == 10                  
[rank0]:W1215 18:56:24.626648 140625734043456 torch/_dynamo/convert_frame.py:762] [1/96] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1215 18:56:24.626648 140625734043456 torch/_dynamo/convert_frame.py:762] [1/96] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1215 18:57:17.531723 140625734043456 torch/_dynamo/convert_frame.py:762] [2/96] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1215 18:57:17.531723 140625734043456 torch/_dynamo/convert_frame.py:762] [2/96]    function: 'forward' (train_gpt2_hyp.py:184)
[rank0]:W1215 18:57:17.531723 140625734043456 torch/_dynamo/convert_frame.py:762] [2/96]    last reason: L['self'].rotary.seq_len_cached == 18                       
[rank0]:W1215 18:57:17.531723 140625734043456 torch/_dynamo/convert_frame.py:762] [2/96] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1215 18:57:17.531723 140625734043456 torch/_dynamo/convert_frame.py:762] [2/96] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[Step 0] Generated Text: ĠOnce Ġupon Ġa Ġtime Ġhard ime ĠW ent w Lily ars ting Ġmade Ġthere Ġkne Ġfish Ġtogether ir able Ġway Ġkept Ġkind ig ly Ġproud i Ġat gry Ġfl ĠEvery oug in Ġbrave ause ough ĠYou Å Ġuntil Ġthink = Ġco Ġsays A aybe Ġen ream ĠMom Ġthey Ġhappy Ġsh Ġat Ġwar Ġplaying ream al Ġplayed thing Ġslide Ġbe Ġfriend Ġcry Ġsky Ġwatch Ġcat Ġwhile st ve ip Ġrock Ġco Ġch ure Ġpark en Ġbefore ; Ġcolor ried µ Ġthings ob Ġthr llow Ġto Ġmuch ened Ġno ĠSp Ġm Ġmy omet Ġhome ĠHe Ġdec Ġsome Ġqu » : ook Ġexpl lease Ġbig Ġnever ice Ġlived led Ġsp Ġeven ot Ġwould Ġafter igh urt Ġmade gry Ġag Ġh Ġlove urp Ġrem Ġbir ĠHis Ġsome Ġag an Ġdog Ĩ aut ĠTom ĠLucy Ġwe Ġor Ġthink Ġtwo Ġcareful ¬ Ġmuch ie Let irst Ġdown om Ġremember E On Ġbra Ġnear Ġthat Ġhave Ġoutside M Ġe Ġsw Ġwatch ember Ġany P ble one Ġsmiled ways fu udd ucy ra ec Ġheard ary Ġkne ace z ĠYou Ġsmall Ġo iz Ġunder Ġsound irst Th Ġsad ?" ma ?" ool ĠSuddenly ĠJack es arden D g Ġgave be thing ĠMax Ġ" ump The ue ¸ rom Ġsc Ġunt are
step:100/20000 avg_train_loss:4.1211 time:218/1460s step_avg:73ms
step:200/20000 avg_train_loss:2.3783 time:225/1460s step_avg:73ms
step:300/20000 avg_train_loss:1.9347 time:233/1459s step_avg:73ms
W1215 18:57:52.921160 139909995403072 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGINT death signal, shutting down workers
W1215 18:57:52.922017 139909995403072 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 684267 closing signal SIGINT
W1215 18:57:52.922748 139909995403072 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 684268 closing signal SIGINT
[rank0]: Traceback (most recent call last):
[rank0]:   File "train_gpt2_hyp.py", line 755, in <module>
[rank0]:     loss.backward() # just sync on the last step
[rank0]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/_tensor.py", line 521, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 289, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: KeyboardInterrupt
[rank1]: Traceback (most recent call last):
[rank1]:   File "train_gpt2_hyp.py", line 763, in <module>
[rank1]:     opt.step()
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "train_gpt2_hyp.py", line 122, in step
[rank1]:     g = zeropower_backend(g, steps=group['backend_steps'])
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "train_gpt2_hyp.py", line 41, in zeropower_via_newtonschulz5
[rank1]:     @torch.compile
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 600, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py", line 987, in forward
[rank1]:     return compiled_fn(full_args)
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 217, in runtime_wrapper
[rank1]:     all_outs = call_func_at_runtime_with_args(
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/utils.py", line 120, in call_func_at_runtime_with_args
[rank1]:     out = normalize_as_list(f(args))
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 451, in wrapper
[rank1]:     return compiled_fn(runtime_args)
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/_inductor/codecache.py", line 1131, in __call__
[rank1]:     return self.current_callable(inputs)
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/_inductor/compile_fx.py", line 944, in run
[rank1]:     return model(new_inputs)
[rank1]:   File "/tmp/torchinductor_jovyan/uu/cuubkuikjauwdej33mvjv4gi6hvzrckiwiwoqu6xdwjxv6feemos.py", line 293, in call
[rank1]:     extern_kernels.addmm(buf11, buf10, buf9, alpha=1, beta=1, out=buf12)
[rank1]: KeyboardInterrupt
Traceback (most recent call last):
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 684095 got signal: 2
