{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# # Muon optimizer\n",
    "\n",
    "def zeropower_via_svd(G, steps=None):\n",
    "    U, S, V = G.svd()\n",
    "    return U @ V.T\n",
    "\n",
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G.bfloat16()\n",
    "    X /= (X.norm() + eps) # ensure top singular value <= 1\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = A @ X\n",
    "        X = a * X + b * B + c * A @ B\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Some warnings:\n",
    "    - This optimizer assumes that all parameters passed in are 2D.\n",
    "    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n",
    "    parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n",
    "    - We believe it is unlikely to work well for training with small batch size.\n",
    "    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n",
    "    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n",
    "\n",
    "    Arguments:\n",
    "        lr: The learning rate used by the internal SGD.\n",
    "        momentum: The momentum used by the internal SGD.\n",
    "        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n",
    "        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n",
    "        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n",
    "                 backend='newtonschulz5', backend_steps=5):\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            zeropower_backend = zeropower_backends[group['backend']]\n",
    "\n",
    "            # generate weight updates in distributed fashion\n",
    "            total_params = sum(p.numel() for p in group['params'])\n",
    "            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n",
    "            curr_idx = 0\n",
    "            for i, p in enumerate(group['params']):\n",
    "                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n",
    "                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n",
    "                    g = p.grad\n",
    "                    assert g is not None\n",
    "                    state = self.state[p]\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        state['momentum_buffer'] = torch.zeros_like(g)\n",
    "                    buf = state['momentum_buffer']\n",
    "                    buf.mul_(momentum).add_(g)\n",
    "                    if group['nesterov']:\n",
    "                        g = g.add(buf, alpha=momentum)\n",
    "                    g = zeropower_backend(g, steps=group['backend_steps'])\n",
    "                    g *= max(1, g.size(0)/g.size(1))**0.5\n",
    "                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n",
    "                curr_idx += p.numel()\n",
    "\n",
    "            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n",
    "            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n",
    "\n",
    "            # deserialize and apply updates\n",
    "            curr_idx = 0\n",
    "            for p in group['params']:\n",
    "                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n",
    "                p.data.add_(g, alpha=-lr)\n",
    "                curr_idx += p.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Our own simple Distributed Data Loader\n",
    "\n",
    "def _peek_data_shard(filename):\n",
    "    # only reads the header, returns header data\n",
    "    with open(filename, \"rb\") as f:\n",
    "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
    "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
    "    if header[0] != 20240520:\n",
    "        print(\"ERROR: magic number mismatch in the data .bin file!\")\n",
    "        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n",
    "        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n",
    "        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n",
    "        exit(1)\n",
    "    assert header[1] == 1, \"unsupported version\"\n",
    "    ntok = header[2] # number of tokens (claimed)\n",
    "    return ntok # for now just return the number of tokens\n",
    "\n",
    "def _load_data_shard(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
    "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
    "        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "        assert header[1] == 1, \"unsupported version\"\n",
    "        ntok = header[2] # number of tokens (claimed)\n",
    "        # the rest of it are tokens, stored as uint16\n",
    "        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n",
    "    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n",
    "    return tokens\n",
    "\n",
    "class DistributedDataLoader:\n",
    "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # glob files that match the pattern\n",
    "        self.files = sorted(glob.glob(filename_pattern))\n",
    "        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n",
    "\n",
    "        # load and validate all data shards, count number of tokens in total\n",
    "        ntok_total = 0\n",
    "        for fname in self.files:\n",
    "            shard_ntok = _peek_data_shard(fname)\n",
    "            assert shard_ntok >= num_processes * B * T + 1\n",
    "            ntok_total += int(shard_ntok)\n",
    "        self.ntok_total = ntok_total\n",
    "\n",
    "        # kick things off\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_shard = 0\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "\n",
    "    def advance(self): # advance to next data shard\n",
    "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance current position and load next shard if necessary\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.advance()\n",
    "        return x.cuda(), y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# with open(sys.argv[0]) as f:\n",
    "#     code = f.read() # read the code of this file ASAP, for logging\n",
    "\n",
    "# import random\n",
    "# import datetime\n",
    "# import time\n",
    "\n",
    "# import glob\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# import torch.distributed as dist\n",
    "# import torch._inductor.config as config\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from lib.lorentz.manifold import CustomLorentz\n",
    "from lib.geoopt import ManifoldParameter\n",
    "# from lib.geoopt.optim import RiemannianAdam\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PyTorch nn.Module definitions for the GPT-2 model\n",
    "\n",
    "class Rotary(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        if seq_len != self.seq_len_cached:\n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.outer(t, self.inv_freq).to(x.device)\n",
    "            self.cos_cached = freqs.cos().bfloat16()\n",
    "            self.sin_cached = freqs.sin().bfloat16()\n",
    "        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n",
    "\n",
    "def apply_rotary_emb(x, cos, sin):\n",
    "    assert x.ndim == 4 # multihead attention\n",
    "    d = x.shape[3]//2\n",
    "    x1 = x[..., :d]\n",
    "    x2 = x[..., d:]\n",
    "    y1 = x1 * cos + x2 * sin\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    return torch.cat([y1, y2], 3).type_as(x)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n",
    "        self.rotary = Rotary(self.head_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n",
    "        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n",
    "        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n",
    "        cos, sin = self.rotary(q)\n",
    "        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n",
    "        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n",
    "        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n",
    "        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n",
    "        return x\n",
    "\n",
    "class LorentzMLR(nn.Module):\n",
    "    \"\"\" Multinomial logistic regression (MLR) in the Lorentz model\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            manifold: CustomLorentz, \n",
    "            num_features: int, \n",
    "            num_classes: int\n",
    "        ):\n",
    "        super(LorentzMLR, self).__init__()\n",
    "\n",
    "        self.manifold = manifold\n",
    "\n",
    "        self.a = torch.nn.Parameter(torch.zeros(num_classes,))\n",
    "        self.z = torch.nn.Parameter(F.pad(torch.zeros(num_classes, num_features-2), pad=(1,0), value=1)) # z should not be (0,0)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, num_features)\n",
    "\n",
    "        # Hyperplane parameters\n",
    "        sqrt_mK = 1 / self.manifold.k.sqrt()  # scalar\n",
    "        norm_z = torch.norm(self.z, dim=-1)  # (num_classes,)\n",
    "        w_t = torch.sinh(sqrt_mK * self.a) * norm_z  # (num_classes,)\n",
    "        w_s = torch.cosh(sqrt_mK * self.a).unsqueeze(-1) * self.z  # (num_classes, num_features -1)\n",
    "\n",
    "        beta = torch.sqrt(-w_t**2 + torch.norm(w_s, dim=-1)**2)  # (num_classes,)\n",
    "\n",
    "        x0 = x.narrow(-1, 0, 1)  # (B, T, 1)\n",
    "        x_rest = x.narrow(-1, 1, x.shape[-1]-1)  # (B, T, num_features -1)\n",
    "        inner_prod = torch.matmul(x_rest, self.z.T)  # (B, T, num_classes)\n",
    "        alpha = -x0 * w_t.view(1, 1, -1) + torch.cosh(sqrt_mK * self.a).view(1, 1, -1) * inner_prod  # (B, T, num_classes)\n",
    "        sqrt_mK_alpha_over_beta = sqrt_mK * alpha / beta.view(1, 1, -1)\n",
    "        d = self.manifold.k.sqrt() * torch.abs(torch.asinh(sqrt_mK_alpha_over_beta))  # (B, T, num_classes)\n",
    "\n",
    "        logits = torch.sign(alpha) * beta.view(1, 1, -1) * d  # (B, T, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def init_weights(self):\n",
    "        stdv = 1. / math.sqrt(self.z.size(1))\n",
    "        nn.init.uniform_(self.z, -stdv, stdv)\n",
    "        nn.init.uniform_(self.a, -stdv, stdv)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# The main GPT-2 model\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size : int = 50304\n",
    "    n_layer : int = 12\n",
    "    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n",
    "    n_embd : int = 768\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "        ))\n",
    "        self.manifold = CustomLorentz(k=torch.tensor([1.0]))\n",
    "        self.lm_head = LorentzMLR(\n",
    "            manifold=self.manifold,\n",
    "            num_features=config.n_embd,\n",
    "            num_classes=config.vocab_size\n",
    "        )\n",
    "        # self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # self.lm_head.weight.data.zero_()\n",
    "\n",
    "    def forward(self, idx, targets=None, return_logits=True):\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        x = F.rms_norm(x, (x.size(-1),))\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = F.rms_norm(x, (x.size(-1),))\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            logits = logits.float() # use tf32/fp32 for logits\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            logits = logits.float() # use tf32/fp32 for logits\n",
    "            loss = None\n",
    "\n",
    "        # there are performance reasons why not returning logits is prudent, if not needed\n",
    "        if not return_logits:\n",
    "            logits = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# int main\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    # data hyperparams\n",
    "    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n",
    "    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on\n",
    "    # optimization hyperparams\n",
    "    batch_size : int = 8*64 # batch size, in sequences, across all devices\n",
    "    device_batch_size : int = 64 # batch size, in sequences, per device\n",
    "    sequence_length : int = 1024 # sequence length, in tokens\n",
    "    num_iterations : int = 4578 # number of iterations to run\n",
    "    warmup_iters : int = 0\n",
    "    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n",
    "    weight_decay : float = 0\n",
    "    # evaluation and logging hyperparams\n",
    "    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end\n",
    "    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n",
    "    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n",
    "    lr : float = 1e-2\n",
    "args = Hyperparameters()\n",
    "\n",
    "# set up DDP (distributed data parallel). torchrun sets this env variable\n",
    "assert torch.cuda.is_available()\n",
    "# dist.init_process_group(backend='nccl')\n",
    "# ddp_rank = int(os.environ['RANK'])\n",
    "# ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "# ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "# device = f'cuda:{ddp_local_rank}'\n",
    "# torch.cuda.set_device(device)\n",
    "# print(f\"using device: {device}\")\n",
    "# master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n",
    "\n",
    "# convenience variables\n",
    "# B, T = args.device_batch_size, args.sequence_length\n",
    "# # calculate the number of steps to take in the val loop.\n",
    "# assert args.val_tokens % (B * T * ddp_world_size) == 0\n",
    "# val_steps = args.val_tokens // (B * T * ddp_world_size)\n",
    "# # calculate the steps of gradient accumulation required to attain the desired global batch size.\n",
    "# assert args.batch_size % (B * ddp_world_size) == 0\n",
    "# train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n",
    "\n",
    "# load tokens\n",
    "# train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n",
    "# val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n",
    "# if master_process:\n",
    "#     print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n",
    "#     print(f\"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files\")\n",
    "# x, y = train_loader.next_batch()\n",
    "\n",
    "# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n",
    "# this originates from Karpathy's experiments.\n",
    "num_vocab = 50304\n",
    "model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n",
    "raw_model = model.cuda()\n",
    "# if hasattr(config, \"coordinate_descent_tuning\"):\n",
    "#     config.coordinate_descent_tuning = True # suggested by @Chillee\n",
    "# model = torch.compile(model)\n",
    "# # here we wrap model into DDP container\n",
    "# model = DDP(model, device_ids=[ddp_local_rank])\n",
    "# raw_model = model.module # always contains the \"raw\" unwrapped model\n",
    "# ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All parameters in the model\n",
    "all_params = list(raw_model.parameters())\n",
    "\n",
    "# Parameters \n",
    "manifold_params = [p for p in raw_model.parameters() if isinstance(p, ManifoldParameter)]\n",
    "transformer_params = list(raw_model.transformer.h.parameters())\n",
    "matrix_params = [p for p in transformer_params if p.ndim == 2 and not isinstance(p, ManifoldParameter)]\n",
    "scalar_params = [p for p in transformer_params if p.ndim < 2 and not isinstance(p, ManifoldParameter)]\n",
    "embedding_params = [raw_model.transformer.wte.weight]\n",
    "# other_params = [p for p in all_params if p not in manifold_params + matrix_params + scalar_params + embedding_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a torch.Size([50304]) <class 'torch.nn.parameter.Parameter'>\n",
      "z torch.Size([50304, 767]) <class 'torch.nn.parameter.Parameter'>\n",
      "manifold.k torch.Size([1]) <class 'torch.nn.parameter.Parameter'>\n",
      "weight torch.Size([50304, 768])\n"
     ]
    }
   ],
   "source": [
    "for n, p in raw_model.lm_head.named_parameters():\n",
    "    print(n, p.size(), type(p))\n",
    "for n, p in raw_model.transformer.wte.named_parameters():\n",
    "    print(n, p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['torch.Size([1])', 'torch.Size([3072, 768])',\n",
       "       'torch.Size([50304, 767])', 'torch.Size([50304, 768])',\n",
       "       'torch.Size([50304])', 'torch.Size([768, 3072])',\n",
       "       'torch.Size([768, 768])'], dtype='<U24')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ass = []\n",
    "for n,p in raw_model.named_parameters():\n",
    "    ass.append(str(p.size()))\n",
    "np.unique(ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in Model: 162201601\n"
     ]
    }
   ],
   "source": [
    "# Function to count and print parameter shapes\n",
    "def print_param_info(param_list, group_name):\n",
    "    count = len(param_list)\n",
    "    total_params = sum(p.numel() for p in param_list)\n",
    "    print(f\"{group_name}:\")\n",
    "    print(f\"  Number of Parameters: {count}\")\n",
    "    print(f\"  Total Elements: {total_params}\")\n",
    "    print(f\"  Shapes:\")\n",
    "    for i, param in enumerate(param_list):\n",
    "        print(f\"    Param {i + 1}: {tuple(param.shape)}\")\n",
    "    print()\n",
    "\n",
    "# Extra: Total Parameters in the model\n",
    "total_params = sum(p.numel() for p in all_params)\n",
    "print(f\"Total Parameters in Model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_groups(model, lr_manifold, weight_decay_manifold):\n",
    "    no_decay = [\"scale\"]\n",
    "    k_params = [\"manifold.k\"]\n",
    "\n",
    "    muon_params = []\n",
    "    muon_no_decay_params = []\n",
    "    riemannian_params = []\n",
    "    riemannian_no_decay_params = []\n",
    "    manifold_params = []\n",
    "    k_parameters = []\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if any(nd in n for nd in k_params):\n",
    "            k_parameters.append(p)\n",
    "            continue\n",
    "        if isinstance(p, ManifoldParameter):\n",
    "            manifold_params.append(p)\n",
    "            continue\n",
    "        if p.ndim == 2:\n",
    "            if any(nd in n for nd in no_decay):\n",
    "                muon_no_decay_params.append(p)\n",
    "            else:\n",
    "                muon_params.append(p)\n",
    "        else:\n",
    "            if any(nd in n for nd in no_decay):\n",
    "                riemannian_no_decay_params.append(p)\n",
    "            else:\n",
    "                riemannian_params.append(p)\n",
    "\n",
    "    parameters = []\n",
    "\n",
    "    # Muon optimizer parameter groups\n",
    "    if muon_params:\n",
    "        parameters.append(\n",
    "            {\n",
    "                \"params\": muon_params,\n",
    "                # Add any Muon optimizer-specific settings here\n",
    "            }\n",
    "        )\n",
    "    if muon_no_decay_params:\n",
    "        parameters.append(\n",
    "            {\n",
    "                \"params\": muon_no_decay_params,\n",
    "                \"weight_decay\": 0,\n",
    "                # Add any Muon optimizer-specific settings here\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Riemannian SGD optimizer parameter groups\n",
    "    if riemannian_params:\n",
    "        parameters.append(\n",
    "            {\n",
    "                \"params\": riemannian_params,\n",
    "                # Add any RiemannianSGD optimizer-specific settings here\n",
    "            }\n",
    "        )\n",
    "    if riemannian_no_decay_params:\n",
    "        parameters.append(\n",
    "            {\n",
    "                \"params\": riemannian_no_decay_params,\n",
    "                \"weight_decay\": 0,\n",
    "                # Add any RiemannianSGD optimizer-specific settings here\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Manifold parameters\n",
    "    if manifold_params:\n",
    "        parameters.append(\n",
    "            {\n",
    "                \"params\": manifold_params,\n",
    "                \"lr\": lr_manifold,\n",
    "                \"weight_decay\": weight_decay_manifold,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # k parameters\n",
    "    if k_parameters:\n",
    "        parameters.append(\n",
    "            {\n",
    "                \"params\": k_parameters,\n",
    "                \"weight_decay\": 0,\n",
    "                \"lr\": 1e-4,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return parameters\n",
    "\n",
    "param_groups = get_param_groups(model, lr_manifold=0.01, weight_decay_manifold=0.001)\n",
    "\n",
    "# # Create the Muon optimizer for matrix parameters\n",
    "# muon_params = [group for group in param_groups if 'muon' in group]\n",
    "# muon_optimizer = Muon(muon_params, lr=0.02, momentum=0.95, nesterov=True)\n",
    "\n",
    "# # Create the Riemannian SGD optimizer for non-matrix parameters\n",
    "# riemannian_params = [group for group in param_groups if 'riemannian' in group]\n",
    "# riemannian_optimizer = RiemannianSGD(riemannian_params, lr=0.01, weight_decay=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162151296"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = param_groups[0]\n",
    "c = 0\n",
    "for p in g['params']:\n",
    "    c += p.numel()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fokin_HCNN)",
   "language": "python",
   "name": "fokin_hcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
