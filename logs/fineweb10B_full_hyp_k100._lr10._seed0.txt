W0216 00:49:18.441057 140365029259072 torch/distributed/run.py:779] 
W0216 00:49:18.441057 140365029259072 torch/distributed/run.py:779] *****************************************
W0216 00:49:18.441057 140365029259072 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0216 00:49:18.441057 140365029259072 torch/distributed/run.py:779] *****************************************
[Rank 0] Using device: cuda:0
[Rank 0] This is the master process.
[Rank 0] Training DataLoader: 2700000000 tokens across 27 files.
[Rank 0] Validation DataLoader: 100000000 tokens across 1 files.
[Rank 1] Using device: cuda:1
[Rank 0] Model wrapped in DDP.
k params lengths: head = 1, attn = 12
Tokenizer vocab size: 50304
attn.k is learned

=== Report ===
Model Size:    162.20M

Data Path:            data/fineweb10B
Sequence Length:      1024
Batch Size (global):  64
Batch Size (device):  8
n_layers:              12
n_heads:               6
head_dim:             128
n_embd:               768
Seed:                 0
==============================

Logs for this run will be stored in: runs/02.16/2971_fw_hh_k100.0_lr1e+01_s0/
Writing logs to: runs/02.16/2971_fw_hh_k100.0_lr1e+01_s0/tensorboard_logs
step:0/10, tokens seen: 0.00M, val_loss:10.9806 train_time:0.22s step_avg:nanms
step:10/10, tokens seen:0.59M, avg_train_loss:9.2583 time:114/nans step_avg:nanms
step:10/10, tokens seen: 0.66M, val_loss:8.3694 train_time:0.00s step_avg:nanms
Total training time: 164.12s
peak memory consumption: 32283 MiB
