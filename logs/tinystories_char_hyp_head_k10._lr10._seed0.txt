[Rank 0] Using device: cuda:0
[Rank 0] This is the master process.
[Rank 0] Training DataLoader: 187975984 tokens across 1 files.
[Rank 0] Validation DataLoader: 1909027 tokens across 1 files.
[Rank 0] Model wrapped in DDP.
k params lengths: head = 1, attn = 0
Tokenizer vocab size: 122
head.k is learned with 10.0 lr

=== Report ===
Model Size:    310.53K

Data Path:            data/tinystories_char
Sequence Length:      1024
Batch Size (global):  128
Batch Size (device):  64
n_layers:              6
n_heads:               4
head_dim:             16
n_embd:               64
Seed:                 0
==============================

Logs for this run will be stored in: runs/02.16/1768_tsc_he_k10.0_lr1e+01_s0/
Writing logs to: runs/02.16/1768_tsc_he_k10.0_lr1e+01_s0/tensorboard_logs
step:0/10, tokens seen: 0.00M, val_loss:4.9214 train_time:0.19s step_avg:nanms
step:10/10, tokens seen:1.18M, avg_train_loss:3.2711 time:38/nans step_avg:nanms
step:10/10, tokens seen: 1.31M, val_loss:2.6110 train_time:0.00s step_avg:nanms
Total training time: 38.39s
peak memory consumption: 1525 MiB
