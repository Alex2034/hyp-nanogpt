====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# Copyright (c) Meta Platforms, Inc. and affiliates.

import torch

# avoid division by zero when calculating scale
EPS = 1e-12


def scale(t, amax_t, dtype_t):
    min_v, max_v = torch.finfo(dtype_t).min, torch.finfo(dtype_t).max
    scale_t = torch.clamp(amax_t.float(), min=EPS) / max_v
    t_fp8 = (t / scale_t).clamp(min=min_v, max=max_v).to(dtype_t)
    return t_fp8, scale_t


def matmul(first, amax_first, dtype_first, second_t, amax_second_t, dtype_second_t):
    first_fp8, scale_first = scale(first, amax_first, dtype_first)
    second_t_fp8, scale_second_t = scale(second_t, amax_second_t, dtype_second_t)
    output = torch._scaled_mm(
        first_fp8,
        second_t_fp8.t(),
        scale_a=scale_first,
        scale_b=scale_second_t.t(),
        bias=None,
        out_dtype=torch.bfloat16,
        use_fast_accum=True,
    )
    return output


@torch._dynamo.allow_in_graph
class Fp8LinearFn(torch.autograd.Function):
    @staticmethod
    def forward(ctx, a, b_t):
        amax_a = a.abs().amax(dim=-1, keepdim=True)
        amax_b_t = b_t.abs().amax(dim=-1, keepdim=True)
        out = matmul(a, amax_a, torch.float8_e4m3fn, b_t, amax_b_t, torch.float8_e4m3fn)

        ctx.a_requires_grad = a.requires_grad
        ctx.b_requires_grad = b_t.requires_grad

        ctx.save_for_backward(a, b_t, amax_b_t.max())

        return out

    @staticmethod
    def backward(ctx, grad_out):
        a, b_t, amax_b = ctx.saved_tensors

        if ctx.a_requires_grad:
            b = b_t.t().contiguous()
            amax_grad_out = grad_out.abs().amax(dim=-1, keepdim=True)
            amax_b = amax_b.repeat(b.shape[0], 1)
            grad_a = matmul(grad_out, amax_grad_out, torch.float8_e5m2, b, amax_b, torch.float8_e4m3fn)
        else:
            grad_a = None
        if ctx.b_requires_grad:
            grad_b = grad_out.t() @ a
        else:
            grad_b = None

        return grad_a, grad_b


class Fp8Linear(torch.nn.Linear):
    def forward(self, input: torch.Tensor) -> torch.Tensor:
        out = Fp8LinearFn.apply(input.flatten(end_dim=-2), self.weight)
        out = out.unflatten(0, input.shape[:-1])
        return out


def convert_linear_to_fp8(module):
    new_module = Fp8Linear(
        in_features=module.in_features,
        out_features=module.out_features,
        bias=False,
        dtype=module.weight.dtype,
        device=module.weight.device,
    )
    new_module.weight = module.weight
    return new_module

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5,
                 rank=0, world_size=1):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)
        self.rank = rank
        self.world_size = world_size

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % self.world_size == self.rank:
                    g = p.grad
                    if g is None:
                        continue
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.c_fc = convert_linear_to_fp8(self.c_fc)
        self.c_proj = convert_linear_to_fp8(self.c_proj)

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95,
                  rank=ddp_rank, world_size=ddp_world_size)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.0+cu124 compiled for CUDA 12.4
nvidia-smi:
Sun Oct 20 15:46:25 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          Off |   00000000:18:00.0 Off |                   0* |
| N/A   32C    P0            150W /  700W |    4860MiB /  81559MiB |      5%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          Off |   00000000:2A:00.0 Off |                   0* |
| N/A   33C    P0            135W /  700W |    4908MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          Off |   00000000:3A:00.0 Off |                   0* |
| N/A   33C    P0            127W /  700W |    4908MiB /  81559MiB |     11%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          Off |   00000000:5D:00.0 Off |                   0* |
| N/A   31C    P0            131W /  700W |    4908MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          Off |   00000000:84:00.0 Off |                   0* |
| N/A   33C    P0            144W /  700W |    4908MiB /  81559MiB |     15%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          Off |   00000000:8C:00.0 Off |                   0* |
| N/A   35C    P0            144W /  700W |    4908MiB /  81559MiB |      8%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          Off |   00000000:91:00.0 Off |                   0* |
| N/A   34C    P0            143W /  700W |    4908MiB /  81559MiB |     14%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          Off |   00000000:E5:00.0 Off |                   0* |
| N/A   32C    P0            137W /  700W |    4668MiB /  81559MiB |      5%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/5100 val_loss:15.9968 train_time:469ms step_avg:nanms
step:1/5100 train_loss:15.9927 train_time:41726ms step_avg:nanms
step:2/5100 train_loss:9.4728 train_time:41846ms step_avg:nanms
step:3/5100 train_loss:8.6639 train_time:41980ms step_avg:nanms
step:4/5100 train_loss:7.9944 train_time:42116ms step_avg:nanms
step:5/5100 train_loss:7.6140 train_time:42251ms step_avg:nanms
step:6/5100 train_loss:7.6285 train_time:42385ms step_avg:nanms
step:7/5100 train_loss:7.2293 train_time:42520ms step_avg:nanms
step:8/5100 train_loss:7.5599 train_time:42655ms step_avg:nanms
step:9/5100 train_loss:7.3659 train_time:42793ms step_avg:nanms
step:10/5100 train_loss:7.1143 train_time:42932ms step_avg:nanms
step:11/5100 train_loss:7.1054 train_time:82ms step_avg:nanms
step:12/5100 train_loss:6.9314 train_time:218ms step_avg:nanms
step:13/5100 train_loss:6.7288 train_time:358ms step_avg:119.27ms
step:14/5100 train_loss:6.6954 train_time:491ms step_avg:122.65ms
step:15/5100 train_loss:6.6358 train_time:626ms step_avg:125.28ms
step:16/5100 train_loss:6.5716 train_time:764ms step_avg:127.35ms
step:17/5100 train_loss:6.5849 train_time:902ms step_avg:128.81ms
step:18/5100 train_loss:6.6092 train_time:1047ms step_avg:130.91ms
step:19/5100 train_loss:6.4282 train_time:1179ms step_avg:131.05ms
step:20/5100 train_loss:6.4549 train_time:1315ms step_avg:131.52ms
step:21/5100 train_loss:6.1108 train_time:1452ms step_avg:131.96ms
step:22/5100 train_loss:6.4762 train_time:1587ms step_avg:132.27ms
step:23/5100 train_loss:6.6752 train_time:1725ms step_avg:132.70ms
step:24/5100 train_loss:6.3587 train_time:1863ms step_avg:133.08ms
step:25/5100 train_loss:6.4755 train_time:2001ms step_avg:133.40ms
step:26/5100 train_loss:6.1811 train_time:2139ms step_avg:133.68ms
step:27/5100 train_loss:6.1120 train_time:2277ms step_avg:133.93ms
step:28/5100 train_loss:6.2536 train_time:2415ms step_avg:134.16ms
step:29/5100 train_loss:5.9344 train_time:2556ms step_avg:134.54ms
step:30/5100 train_loss:6.2240 train_time:2691ms step_avg:134.57ms
step:31/5100 train_loss:6.0531 train_time:2832ms step_avg:134.84ms
step:32/5100 train_loss:6.0243 train_time:2969ms step_avg:134.94ms
step:33/5100 train_loss:5.8475 train_time:3108ms step_avg:135.15ms
step:34/5100 train_loss:6.1324 train_time:3247ms step_avg:135.31ms
step:35/5100 train_loss:6.0660 train_time:3384ms step_avg:135.36ms
step:36/5100 train_loss:6.1998 train_time:3522ms step_avg:135.45ms
step:37/5100 train_loss:6.1492 train_time:3660ms step_avg:135.56ms
step:38/5100 train_loss:6.0483 train_time:3802ms step_avg:135.77ms
step:39/5100 train_loss:5.9361 train_time:3937ms step_avg:135.75ms
step:40/5100 train_loss:5.9505 train_time:4076ms step_avg:135.85ms
step:41/5100 train_loss:5.8696 train_time:4214ms step_avg:135.94ms
step:42/5100 train_loss:5.8996 train_time:4351ms step_avg:135.98ms
step:43/5100 train_loss:5.7643 train_time:4487ms step_avg:135.97ms
step:44/5100 train_loss:5.8679 train_time:4629ms step_avg:136.15ms
step:45/5100 train_loss:5.8389 train_time:4764ms step_avg:136.11ms
step:46/5100 train_loss:5.9940 train_time:4901ms step_avg:136.15ms
step:47/5100 train_loss:5.7834 train_time:5040ms step_avg:136.22ms
step:48/5100 train_loss:5.6607 train_time:5180ms step_avg:136.31ms
step:49/5100 train_loss:5.8734 train_time:5317ms step_avg:136.34ms
step:50/5100 train_loss:5.7525 train_time:5456ms step_avg:136.41ms
step:51/5100 train_loss:5.8986 train_time:5597ms step_avg:136.52ms
step:52/5100 train_loss:5.7511 train_time:5735ms step_avg:136.56ms
step:53/5100 train_loss:5.6102 train_time:5873ms step_avg:136.59ms
step:54/5100 train_loss:5.7481 train_time:6011ms step_avg:136.62ms
step:55/5100 train_loss:5.6249 train_time:6148ms step_avg:136.63ms
step:56/5100 train_loss:5.9670 train_time:6285ms step_avg:136.63ms
step:57/5100 train_loss:5.6270 train_time:6423ms step_avg:136.66ms
step:58/5100 train_loss:5.4918 train_time:6560ms step_avg:136.68ms
step:59/5100 train_loss:5.6292 train_time:6698ms step_avg:136.70ms
step:60/5100 train_loss:5.6034 train_time:6837ms step_avg:136.74ms
step:61/5100 train_loss:5.7002 train_time:6976ms step_avg:136.79ms
step:62/5100 train_loss:5.4708 train_time:7115ms step_avg:136.84ms
step:63/5100 train_loss:5.5722 train_time:7254ms step_avg:136.86ms
step:64/5100 train_loss:5.5564 train_time:7389ms step_avg:136.83ms
step:65/5100 train_loss:5.2228 train_time:7528ms step_avg:136.87ms
step:66/5100 train_loss:5.3701 train_time:7666ms step_avg:136.90ms
step:67/5100 train_loss:5.5208 train_time:7805ms step_avg:136.92ms
step:68/5100 train_loss:5.3998 train_time:7943ms step_avg:136.95ms
step:69/5100 train_loss:5.6615 train_time:8082ms step_avg:136.98ms
step:70/5100 train_loss:5.3020 train_time:8221ms step_avg:137.02ms
step:71/5100 train_loss:5.3195 train_time:8359ms step_avg:137.03ms
step:72/5100 train_loss:5.5242 train_time:8502ms step_avg:137.12ms
step:73/5100 train_loss:5.4567 train_time:8637ms step_avg:137.10ms
step:74/5100 train_loss:5.3402 train_time:8775ms step_avg:137.11ms
step:75/5100 train_loss:5.4658 train_time:8913ms step_avg:137.13ms
step:76/5100 train_loss:5.4324 train_time:9050ms step_avg:137.12ms
step:77/5100 train_loss:5.3978 train_time:9188ms step_avg:137.13ms
step:78/5100 train_loss:5.4739 train_time:9327ms step_avg:137.16ms
step:79/5100 train_loss:5.5380 train_time:9464ms step_avg:137.16ms
step:80/5100 train_loss:5.3260 train_time:9603ms step_avg:137.19ms
step:81/5100 train_loss:5.4549 train_time:9741ms step_avg:137.20ms
step:82/5100 train_loss:5.2075 train_time:9880ms step_avg:137.22ms
step:83/5100 train_loss:5.3914 train_time:10018ms step_avg:137.24ms
step:84/5100 train_loss:5.3324 train_time:10158ms step_avg:137.27ms
step:85/5100 train_loss:5.3105 train_time:10297ms step_avg:137.29ms
step:86/5100 train_loss:5.1747 train_time:10435ms step_avg:137.31ms
step:87/5100 train_loss:5.3921 train_time:10578ms step_avg:137.38ms
step:88/5100 train_loss:5.2905 train_time:10711ms step_avg:137.32ms
step:89/5100 train_loss:5.3594 train_time:10849ms step_avg:137.33ms
step:90/5100 train_loss:5.3023 train_time:10986ms step_avg:137.32ms
step:91/5100 train_loss:5.2372 train_time:11125ms step_avg:137.34ms
step:92/5100 train_loss:5.2117 train_time:11264ms step_avg:137.36ms
step:93/5100 train_loss:5.3625 train_time:11403ms step_avg:137.38ms
step:94/5100 train_loss:5.1653 train_time:11541ms step_avg:137.39ms
step:95/5100 train_loss:5.1782 train_time:11680ms step_avg:137.41ms
step:96/5100 train_loss:5.2137 train_time:11817ms step_avg:137.41ms
step:97/5100 train_loss:5.1316 train_time:11954ms step_avg:137.41ms
step:98/5100 train_loss:5.2080 train_time:12095ms step_avg:137.45ms
step:99/5100 train_loss:5.1293 train_time:12229ms step_avg:137.41ms
step:100/5100 train_loss:5.2491 train_time:12367ms step_avg:137.42ms
step:101/5100 train_loss:5.2236 train_time:12506ms step_avg:137.42ms
step:102/5100 train_loss:5.1246 train_time:12645ms step_avg:137.44ms
step:103/5100 train_loss:5.2148 train_time:12784ms step_avg:137.46ms
step:104/5100 train_loss:5.1687 train_time:12922ms step_avg:137.47ms
step:105/5100 train_loss:5.0306 train_time:13060ms step_avg:137.48ms
step:106/5100 train_loss:5.1311 train_time:13199ms step_avg:137.49ms
step:107/5100 train_loss:5.3350 train_time:13339ms step_avg:137.51ms
step:108/5100 train_loss:5.1029 train_time:13478ms step_avg:137.53ms
step:109/5100 train_loss:4.8897 train_time:13617ms step_avg:137.54ms
step:110/5100 train_loss:5.0761 train_time:13755ms step_avg:137.55ms
step:111/5100 train_loss:5.0558 train_time:13892ms step_avg:137.55ms
step:112/5100 train_loss:5.0177 train_time:14030ms step_avg:137.55ms
step:113/5100 train_loss:5.1295 train_time:14167ms step_avg:137.54ms
step:114/5100 train_loss:5.0585 train_time:14306ms step_avg:137.56ms
step:115/5100 train_loss:4.9049 train_time:14445ms step_avg:137.57ms
step:116/5100 train_loss:5.0627 train_time:14583ms step_avg:137.57ms
step:117/5100 train_loss:4.9753 train_time:14721ms step_avg:137.58ms
step:118/5100 train_loss:4.9375 train_time:14859ms step_avg:137.58ms
step:119/5100 train_loss:5.0823 train_time:14998ms step_avg:137.60ms
step:120/5100 train_loss:5.0384 train_time:15136ms step_avg:137.60ms
step:121/5100 train_loss:4.9705 train_time:15275ms step_avg:137.61ms
step:122/5100 train_loss:4.8661 train_time:15414ms step_avg:137.62ms
step:123/5100 train_loss:4.9886 train_time:15552ms step_avg:137.63ms
step:124/5100 train_loss:4.8261 train_time:15688ms step_avg:137.62ms
step:125/5100 train_loss:5.1402 train_time:15826ms step_avg:137.62ms
step:125/5100 val_loss:4.9715 train_time:15884ms step_avg:138.12ms
step:126/5100 train_loss:5.0194 train_time:15975ms step_avg:137.71ms
step:127/5100 train_loss:4.9594 train_time:16120ms step_avg:137.78ms
step:128/5100 train_loss:5.0188 train_time:16263ms step_avg:137.82ms
step:129/5100 train_loss:4.8988 train_time:16395ms step_avg:137.77ms
step:130/5100 train_loss:5.2059 train_time:16531ms step_avg:137.76ms
step:131/5100 train_loss:4.9493 train_time:16667ms step_avg:137.75ms
step:132/5100 train_loss:4.9723 train_time:16804ms step_avg:137.73ms
step:133/5100 train_loss:4.9144 train_time:16941ms step_avg:137.73ms
step:134/5100 train_loss:4.9483 train_time:17080ms step_avg:137.74ms
step:135/5100 train_loss:4.8427 train_time:17222ms step_avg:137.78ms
step:136/5100 train_loss:4.9773 train_time:17361ms step_avg:137.79ms
step:137/5100 train_loss:4.7414 train_time:17496ms step_avg:137.76ms
step:138/5100 train_loss:4.9075 train_time:17634ms step_avg:137.76ms
step:139/5100 train_loss:4.8541 train_time:17770ms step_avg:137.75ms
step:140/5100 train_loss:4.8887 train_time:17908ms step_avg:137.75ms
step:141/5100 train_loss:4.9455 train_time:18046ms step_avg:137.76ms
step:142/5100 train_loss:4.8245 train_time:18185ms step_avg:137.76ms
step:143/5100 train_loss:4.8831 train_time:18325ms step_avg:137.78ms
step:144/5100 train_loss:4.7444 train_time:18464ms step_avg:137.79ms
step:145/5100 train_loss:4.8783 train_time:18602ms step_avg:137.80ms
step:146/5100 train_loss:4.8334 train_time:18741ms step_avg:137.80ms
step:147/5100 train_loss:4.7069 train_time:18878ms step_avg:137.80ms
step:148/5100 train_loss:4.8620 train_time:19016ms step_avg:137.80ms
step:149/5100 train_loss:4.8478 train_time:19153ms step_avg:137.79ms
step:150/5100 train_loss:4.8784 train_time:19293ms step_avg:137.80ms
step:151/5100 train_loss:4.9139 train_time:19435ms step_avg:137.83ms
step:152/5100 train_loss:4.8023 train_time:19571ms step_avg:137.82ms
step:153/5100 train_loss:4.8138 train_time:19709ms step_avg:137.82ms
step:154/5100 train_loss:4.8875 train_time:19848ms step_avg:137.83ms
step:155/5100 train_loss:4.8539 train_time:19986ms step_avg:137.84ms
step:156/5100 train_loss:4.7990 train_time:20124ms step_avg:137.84ms
step:157/5100 train_loss:4.8235 train_time:20263ms step_avg:137.84ms
step:158/5100 train_loss:4.9332 train_time:20401ms step_avg:137.84ms
step:159/5100 train_loss:4.7284 train_time:20538ms step_avg:137.84ms
step:160/5100 train_loss:4.8112 train_time:20675ms step_avg:137.84ms
step:161/5100 train_loss:4.6318 train_time:20816ms step_avg:137.86ms
step:162/5100 train_loss:4.8163 train_time:20954ms step_avg:137.86ms
step:163/5100 train_loss:4.8447 train_time:21092ms step_avg:137.86ms
step:164/5100 train_loss:4.8362 train_time:21231ms step_avg:137.87ms
step:165/5100 train_loss:4.6550 train_time:21369ms step_avg:137.86ms
step:166/5100 train_loss:4.7706 train_time:21507ms step_avg:137.86ms
step:167/5100 train_loss:4.9047 train_time:21645ms step_avg:137.86ms
step:168/5100 train_loss:4.6855 train_time:21784ms step_avg:137.87ms
step:169/5100 train_loss:4.7896 train_time:21924ms step_avg:137.89ms
step:170/5100 train_loss:4.6372 train_time:22062ms step_avg:137.89ms
step:171/5100 train_loss:4.5429 train_time:22201ms step_avg:137.89ms
step:172/5100 train_loss:4.6984 train_time:22342ms step_avg:137.92ms
step:173/5100 train_loss:4.6847 train_time:22475ms step_avg:137.89ms
step:174/5100 train_loss:4.7374 train_time:22613ms step_avg:137.88ms
step:175/5100 train_loss:4.8923 train_time:22752ms step_avg:137.89ms
step:176/5100 train_loss:4.7357 train_time:22889ms step_avg:137.89ms
step:177/5100 train_loss:4.5948 train_time:23029ms step_avg:137.90ms
step:178/5100 train_loss:4.5555 train_time:23167ms step_avg:137.90ms
step:179/5100 train_loss:4.6306 train_time:23305ms step_avg:137.90ms
step:180/5100 train_loss:4.6373 train_time:23444ms step_avg:137.91ms
step:181/5100 train_loss:4.6274 train_time:23583ms step_avg:137.91ms
step:182/5100 train_loss:4.7718 train_time:23722ms step_avg:137.92ms
step:183/5100 train_loss:4.6298 train_time:23860ms step_avg:137.92ms
step:184/5100 train_loss:4.5836 train_time:23997ms step_avg:137.92ms
step:185/5100 train_loss:4.5926 train_time:24135ms step_avg:137.91ms
step:186/5100 train_loss:4.7126 train_time:24273ms step_avg:137.91ms
step:187/5100 train_loss:4.6314 train_time:24411ms step_avg:137.92ms
step:188/5100 train_loss:4.8067 train_time:24552ms step_avg:137.93ms
step:189/5100 train_loss:4.6449 train_time:24831ms step_avg:138.72ms
step:190/5100 train_loss:4.5683 train_time:25127ms step_avg:139.59ms
step:191/5100 train_loss:4.6952 train_time:25260ms step_avg:139.56ms
step:192/5100 train_loss:4.5394 train_time:25396ms step_avg:139.54ms
step:193/5100 train_loss:4.4710 train_time:25531ms step_avg:139.51ms
step:194/5100 train_loss:4.6983 train_time:25666ms step_avg:139.49ms
step:195/5100 train_loss:4.6333 train_time:25802ms step_avg:139.47ms
step:196/5100 train_loss:4.8205 train_time:25938ms step_avg:139.45ms
step:197/5100 train_loss:4.6845 train_time:26077ms step_avg:139.45ms
step:198/5100 train_loss:4.5299 train_time:26219ms step_avg:139.46ms
step:199/5100 train_loss:4.5978 train_time:26356ms step_avg:139.45ms
step:200/5100 train_loss:4.4669 train_time:26494ms step_avg:139.44ms
step:201/5100 train_loss:4.5532 train_time:26635ms step_avg:139.45ms
step:202/5100 train_loss:4.4564 train_time:26769ms step_avg:139.42ms
step:203/5100 train_loss:4.7026 train_time:26906ms step_avg:139.41ms
step:204/5100 train_loss:4.5541 train_time:27046ms step_avg:139.41ms
step:205/5100 train_loss:4.6036 train_time:27184ms step_avg:139.40ms
step:206/5100 train_loss:4.7196 train_time:27325ms step_avg:139.42ms
step:207/5100 train_loss:4.3761 train_time:27467ms step_avg:139.43ms
step:208/5100 train_loss:4.5316 train_time:27603ms step_avg:139.41ms
step:209/5100 train_loss:4.5052 train_time:27740ms step_avg:139.40ms
step:210/5100 train_loss:4.6662 train_time:27884ms step_avg:139.42ms
step:211/5100 train_loss:4.5832 train_time:28014ms step_avg:139.38ms
step:212/5100 train_loss:4.4717 train_time:28151ms step_avg:139.36ms
step:213/5100 train_loss:4.5860 train_time:28294ms step_avg:139.38ms
step:214/5100 train_loss:4.4380 train_time:28428ms step_avg:139.35ms
step:215/5100 train_loss:4.5069 train_time:28567ms step_avg:139.35ms
step:216/5100 train_loss:4.3696 train_time:28705ms step_avg:139.35ms
step:217/5100 train_loss:4.4671 train_time:28844ms step_avg:139.34ms
step:218/5100 train_loss:4.4412 train_time:28982ms step_avg:139.33ms
step:219/5100 train_loss:4.4603 train_time:29120ms step_avg:139.33ms
step:220/5100 train_loss:4.4553 train_time:29258ms step_avg:139.32ms
step:221/5100 train_loss:4.4864 train_time:29398ms step_avg:139.33ms
step:222/5100 train_loss:4.5024 train_time:29541ms step_avg:139.35ms
step:223/5100 train_loss:4.4345 train_time:29672ms step_avg:139.31ms
step:224/5100 train_loss:4.4325 train_time:29811ms step_avg:139.30ms
step:225/5100 train_loss:4.6292 train_time:29949ms step_avg:139.30ms
step:226/5100 train_loss:4.3027 train_time:30087ms step_avg:139.29ms
step:227/5100 train_loss:4.3522 train_time:30227ms step_avg:139.30ms
step:228/5100 train_loss:4.3596 train_time:30367ms step_avg:139.30ms
step:229/5100 train_loss:4.5122 train_time:30504ms step_avg:139.29ms
step:230/5100 train_loss:4.2954 train_time:30641ms step_avg:139.28ms
step:231/5100 train_loss:4.4420 train_time:30780ms step_avg:139.28ms
step:232/5100 train_loss:4.2977 train_time:30918ms step_avg:139.27ms
step:233/5100 train_loss:4.3233 train_time:31054ms step_avg:139.26ms
step:234/5100 train_loss:4.4850 train_time:31192ms step_avg:139.25ms
step:235/5100 train_loss:4.3714 train_time:31330ms step_avg:139.24ms
step:236/5100 train_loss:4.2603 train_time:31468ms step_avg:139.24ms
step:237/5100 train_loss:4.4657 train_time:31606ms step_avg:139.23ms
step:238/5100 train_loss:4.4375 train_time:31745ms step_avg:139.23ms
step:239/5100 train_loss:4.3012 train_time:31885ms step_avg:139.23ms
step:240/5100 train_loss:4.4479 train_time:32024ms step_avg:139.23ms
step:241/5100 train_loss:4.4567 train_time:32162ms step_avg:139.23ms
step:242/5100 train_loss:4.3298 train_time:32300ms step_avg:139.23ms
step:243/5100 train_loss:4.5061 train_time:32437ms step_avg:139.22ms
step:244/5100 train_loss:4.3527 train_time:32573ms step_avg:139.20ms
step:245/5100 train_loss:4.3933 train_time:32713ms step_avg:139.20ms
step:246/5100 train_loss:4.4630 train_time:32850ms step_avg:139.20ms
step:247/5100 train_loss:4.3943 train_time:32988ms step_avg:139.19ms
step:248/5100 train_loss:4.3371 train_time:33134ms step_avg:139.22ms
step:249/5100 train_loss:4.4670 train_time:33265ms step_avg:139.19ms
step:250/5100 train_loss:4.2357 train_time:33404ms step_avg:139.18ms
step:250/5100 val_loss:4.3364 train_time:33460ms step_avg:139.42ms
step:251/5100 train_loss:4.2911 train_time:33551ms step_avg:139.22ms
step:252/5100 train_loss:4.3977 train_time:33697ms step_avg:139.25ms
step:253/5100 train_loss:4.4405 train_time:33835ms step_avg:139.24ms
step:254/5100 train_loss:4.2620 train_time:33971ms step_avg:139.23ms
step:255/5100 train_loss:4.2108 train_time:34107ms step_avg:139.21ms
step:256/5100 train_loss:4.3849 train_time:34246ms step_avg:139.21ms
step:257/5100 train_loss:4.3051 train_time:34379ms step_avg:139.19ms
step:258/5100 train_loss:4.3086 train_time:34518ms step_avg:139.19ms
step:259/5100 train_loss:4.2776 train_time:34658ms step_avg:139.19ms
step:260/5100 train_loss:4.3216 train_time:34798ms step_avg:139.19ms
step:261/5100 train_loss:4.3664 train_time:34936ms step_avg:139.19ms
step:262/5100 train_loss:4.3216 train_time:35073ms step_avg:139.18ms
step:263/5100 train_loss:4.2941 train_time:35210ms step_avg:139.17ms
step:264/5100 train_loss:4.2078 train_time:35345ms step_avg:139.16ms
step:265/5100 train_loss:4.2897 train_time:35483ms step_avg:139.15ms
step:266/5100 train_loss:4.1430 train_time:35621ms step_avg:139.14ms
step:267/5100 train_loss:4.2157 train_time:35760ms step_avg:139.14ms
step:268/5100 train_loss:4.2235 train_time:35899ms step_avg:139.14ms
step:269/5100 train_loss:4.2367 train_time:36039ms step_avg:139.15ms
step:270/5100 train_loss:4.1567 train_time:36179ms step_avg:139.15ms
step:271/5100 train_loss:4.3881 train_time:36317ms step_avg:139.15ms
step:272/5100 train_loss:4.2828 train_time:36451ms step_avg:139.13ms
step:273/5100 train_loss:4.1968 train_time:36589ms step_avg:139.12ms
step:274/5100 train_loss:4.2414 train_time:36727ms step_avg:139.12ms
step:275/5100 train_loss:4.3219 train_time:36866ms step_avg:139.12ms
step:276/5100 train_loss:4.3453 train_time:37006ms step_avg:139.12ms
step:277/5100 train_loss:4.5152 train_time:37145ms step_avg:139.12ms
step:278/5100 train_loss:4.3086 train_time:37283ms step_avg:139.12ms
step:279/5100 train_loss:4.3803 train_time:37425ms step_avg:139.13ms
step:280/5100 train_loss:4.2759 train_time:37561ms step_avg:139.11ms
step:281/5100 train_loss:4.3941 train_time:37700ms step_avg:139.11ms
step:282/5100 train_loss:4.2273 train_time:37838ms step_avg:139.11ms
step:283/5100 train_loss:4.2540 train_time:37977ms step_avg:139.11ms
step:284/5100 train_loss:4.1828 train_time:38115ms step_avg:139.11ms
step:285/5100 train_loss:4.3362 train_time:38254ms step_avg:139.10ms
step:286/5100 train_loss:4.3379 train_time:38398ms step_avg:139.12ms
step:287/5100 train_loss:4.3648 train_time:38527ms step_avg:139.09ms
step:288/5100 train_loss:4.1941 train_time:38665ms step_avg:139.08ms
step:289/5100 train_loss:4.2861 train_time:38804ms step_avg:139.08ms
step:290/5100 train_loss:4.1509 train_time:38942ms step_avg:139.08ms
step:291/5100 train_loss:4.1389 train_time:39080ms step_avg:139.07ms
step:292/5100 train_loss:4.2207 train_time:39218ms step_avg:139.07ms
step:293/5100 train_loss:4.1340 train_time:39357ms step_avg:139.07ms
step:294/5100 train_loss:4.1839 train_time:39496ms step_avg:139.07ms
step:295/5100 train_loss:4.2207 train_time:39633ms step_avg:139.06ms
step:296/5100 train_loss:4.0996 train_time:39770ms step_avg:139.06ms
step:297/5100 train_loss:4.1161 train_time:39908ms step_avg:139.05ms
step:298/5100 train_loss:4.1176 train_time:40046ms step_avg:139.05ms
step:299/5100 train_loss:4.2285 train_time:40185ms step_avg:139.05ms
step:300/5100 train_loss:4.0911 train_time:40328ms step_avg:139.06ms
step:301/5100 train_loss:4.2351 train_time:40463ms step_avg:139.05ms
step:302/5100 train_loss:4.2419 train_time:40601ms step_avg:139.05ms
step:303/5100 train_loss:4.1798 train_time:40740ms step_avg:139.04ms
step:304/5100 train_loss:4.2485 train_time:40879ms step_avg:139.04ms
step:305/5100 train_loss:4.2287 train_time:41017ms step_avg:139.04ms
step:306/5100 train_loss:4.7022 train_time:41156ms step_avg:139.04ms
step:307/5100 train_loss:4.1980 train_time:41295ms step_avg:139.04ms
step:308/5100 train_loss:4.0954 train_time:41432ms step_avg:139.03ms
step:309/5100 train_loss:4.2555 train_time:41570ms step_avg:139.03ms
step:310/5100 train_loss:4.1111 train_time:41714ms step_avg:139.05ms
step:311/5100 train_loss:4.3361 train_time:41847ms step_avg:139.03ms
step:312/5100 train_loss:4.1919 train_time:41984ms step_avg:139.02ms
step:313/5100 train_loss:4.1161 train_time:42122ms step_avg:139.02ms
step:314/5100 train_loss:4.2276 train_time:42260ms step_avg:139.01ms
step:315/5100 train_loss:4.3399 train_time:42399ms step_avg:139.01ms
step:316/5100 train_loss:4.2129 train_time:42537ms step_avg:139.01ms
step:317/5100 train_loss:4.0488 train_time:42676ms step_avg:139.01ms
step:318/5100 train_loss:4.1238 train_time:42813ms step_avg:139.00ms
step:319/5100 train_loss:4.1608 train_time:42951ms step_avg:139.00ms
step:320/5100 train_loss:4.1355 train_time:43087ms step_avg:138.99ms
step:321/5100 train_loss:4.2494 train_time:43225ms step_avg:138.99ms
step:322/5100 train_loss:4.1938 train_time:43363ms step_avg:138.98ms
step:323/5100 train_loss:4.1673 train_time:43502ms step_avg:138.98ms
step:324/5100 train_loss:4.2512 train_time:43642ms step_avg:138.99ms
step:325/5100 train_loss:4.2152 train_time:43780ms step_avg:138.98ms
step:326/5100 train_loss:4.2753 train_time:43920ms step_avg:138.99ms
step:327/5100 train_loss:4.1282 train_time:44057ms step_avg:138.98ms
step:328/5100 train_loss:4.6234 train_time:44195ms step_avg:138.98ms
step:329/5100 train_loss:4.3128 train_time:44332ms step_avg:138.97ms
step:330/5100 train_loss:4.0581 train_time:44469ms step_avg:138.96ms
step:331/5100 train_loss:3.9999 train_time:44612ms step_avg:138.98ms
step:332/5100 train_loss:4.2156 train_time:44745ms step_avg:138.96ms
step:333/5100 train_loss:4.1362 train_time:44883ms step_avg:138.96ms
step:334/5100 train_loss:4.1224 train_time:45021ms step_avg:138.95ms
step:335/5100 train_loss:4.0841 train_time:45162ms step_avg:138.96ms
step:336/5100 train_loss:4.2553 train_time:45299ms step_avg:138.95ms
step:337/5100 train_loss:4.1928 train_time:45437ms step_avg:138.95ms
step:338/5100 train_loss:4.6605 train_time:45575ms step_avg:138.95ms
step:339/5100 train_loss:4.1766 train_time:45713ms step_avg:138.94ms
step:340/5100 train_loss:4.1250 train_time:45850ms step_avg:138.94ms
step:341/5100 train_loss:4.1617 train_time:45988ms step_avg:138.94ms
step:342/5100 train_loss:4.0786 train_time:46125ms step_avg:138.93ms
step:343/5100 train_loss:4.0473 train_time:46263ms step_avg:138.93ms
step:344/5100 train_loss:4.0983 train_time:46405ms step_avg:138.94ms
step:345/5100 train_loss:4.2280 train_time:46540ms step_avg:138.92ms
step:346/5100 train_loss:4.0747 train_time:46679ms step_avg:138.93ms
step:347/5100 train_loss:4.0105 train_time:46818ms step_avg:138.92ms
step:348/5100 train_loss:4.0538 train_time:46956ms step_avg:138.92ms
step:349/5100 train_loss:4.1001 train_time:47093ms step_avg:138.92ms
step:350/5100 train_loss:4.0485 train_time:47238ms step_avg:138.93ms
step:351/5100 train_loss:3.7731 train_time:47370ms step_avg:138.91ms
step:352/5100 train_loss:4.0443 train_time:47504ms step_avg:138.90ms
step:353/5100 train_loss:4.3864 train_time:47642ms step_avg:138.90ms
step:354/5100 train_loss:3.8980 train_time:47843ms step_avg:139.08ms
step:355/5100 train_loss:4.1568 train_time:47942ms step_avg:138.96ms
step:356/5100 train_loss:4.0271 train_time:48079ms step_avg:138.96ms
step:357/5100 train_loss:4.1206 train_time:48214ms step_avg:138.95ms
step:358/5100 train_loss:4.0716 train_time:48351ms step_avg:138.94ms
step:359/5100 train_loss:4.0755 train_time:48486ms step_avg:138.93ms
step:360/5100 train_loss:4.1025 train_time:48622ms step_avg:138.92ms
step:361/5100 train_loss:3.6895 train_time:48758ms step_avg:138.91ms
step:362/5100 train_loss:4.2441 train_time:48901ms step_avg:138.92ms
step:363/5100 train_loss:4.1495 train_time:49038ms step_avg:138.92ms
step:364/5100 train_loss:4.0641 train_time:49175ms step_avg:138.91ms
step:365/5100 train_loss:3.9757 train_time:49317ms step_avg:138.92ms
step:366/5100 train_loss:4.1400 train_time:49448ms step_avg:138.90ms
step:367/5100 train_loss:4.0983 train_time:49586ms step_avg:138.90ms
step:368/5100 train_loss:4.0792 train_time:49722ms step_avg:138.89ms
step:369/5100 train_loss:4.0708 train_time:49858ms step_avg:138.88ms
step:370/5100 train_loss:3.9572 train_time:49996ms step_avg:138.88ms
step:371/5100 train_loss:4.1130 train_time:50133ms step_avg:138.87ms
step:372/5100 train_loss:3.9959 train_time:50270ms step_avg:138.87ms
step:373/5100 train_loss:3.9199 train_time:50408ms step_avg:138.87ms
step:374/5100 train_loss:4.1376 train_time:50544ms step_avg:138.86ms
step:375/5100 train_loss:4.0665 train_time:50682ms step_avg:138.86ms
step:375/5100 val_loss:4.0593 train_time:50737ms step_avg:139.01ms
step:376/5100 train_loss:4.0304 train_time:50825ms step_avg:138.87ms
step:377/5100 train_loss:4.0928 train_time:50972ms step_avg:138.89ms
step:378/5100 train_loss:4.0107 train_time:51256ms step_avg:139.28ms
step:379/5100 train_loss:4.0720 train_time:51388ms step_avg:139.26ms
step:380/5100 train_loss:4.1031 train_time:51693ms step_avg:139.71ms
step:381/5100 train_loss:4.1757 train_time:51825ms step_avg:139.69ms
step:382/5100 train_loss:4.0769 train_time:51961ms step_avg:139.68ms
step:383/5100 train_loss:4.0520 train_time:52097ms step_avg:139.67ms
step:384/5100 train_loss:4.0119 train_time:52232ms step_avg:139.66ms
step:385/5100 train_loss:4.0913 train_time:52368ms step_avg:139.65ms
step:386/5100 train_loss:4.0064 train_time:52504ms step_avg:139.64ms
step:387/5100 train_loss:4.1129 train_time:52644ms step_avg:139.64ms
step:388/5100 train_loss:4.3056 train_time:52785ms step_avg:139.64ms
step:389/5100 train_loss:4.0224 train_time:52922ms step_avg:139.64ms
step:390/5100 train_loss:4.0109 train_time:53059ms step_avg:139.63ms
step:391/5100 train_loss:4.1062 train_time:53196ms step_avg:139.62ms
step:392/5100 train_loss:4.0350 train_time:53332ms step_avg:139.61ms
step:393/5100 train_loss:4.1402 train_time:53467ms step_avg:139.60ms
step:394/5100 train_loss:3.9744 train_time:53605ms step_avg:139.60ms
step:395/5100 train_loss:4.1094 train_time:53743ms step_avg:139.59ms
step:396/5100 train_loss:3.8518 train_time:53881ms step_avg:139.59ms
step:397/5100 train_loss:4.0520 train_time:54018ms step_avg:139.58ms
step:398/5100 train_loss:4.1111 train_time:54155ms step_avg:139.57ms
step:399/5100 train_loss:4.1103 train_time:54291ms step_avg:139.57ms
step:400/5100 train_loss:4.0045 train_time:54427ms step_avg:139.56ms
step:401/5100 train_loss:4.0558 train_time:54566ms step_avg:139.55ms
step:402/5100 train_loss:4.1244 train_time:54705ms step_avg:139.55ms
step:403/5100 train_loss:4.0628 train_time:54841ms step_avg:139.55ms
step:404/5100 train_loss:4.1714 train_time:54979ms step_avg:139.54ms
step:405/5100 train_loss:3.9325 train_time:55117ms step_avg:139.54ms
step:406/5100 train_loss:4.0141 train_time:55253ms step_avg:139.53ms
step:407/5100 train_loss:4.3050 train_time:55391ms step_avg:139.52ms
step:408/5100 train_loss:4.0094 train_time:55529ms step_avg:139.52ms
step:409/5100 train_loss:4.0397 train_time:55667ms step_avg:139.52ms
step:410/5100 train_loss:4.0831 train_time:55805ms step_avg:139.51ms
step:411/5100 train_loss:3.9606 train_time:55942ms step_avg:139.51ms
step:412/5100 train_loss:3.9819 train_time:56081ms step_avg:139.51ms
step:413/5100 train_loss:4.4214 train_time:56221ms step_avg:139.51ms
step:414/5100 train_loss:3.8810 train_time:56359ms step_avg:139.50ms
step:415/5100 train_loss:4.2354 train_time:56497ms step_avg:139.50ms
step:416/5100 train_loss:3.9800 train_time:56633ms step_avg:139.49ms
step:417/5100 train_loss:3.9856 train_time:56771ms step_avg:139.49ms
step:418/5100 train_loss:4.1753 train_time:56909ms step_avg:139.48ms
step:419/5100 train_loss:3.9038 train_time:57047ms step_avg:139.48ms
step:420/5100 train_loss:4.0199 train_time:57187ms step_avg:139.48ms
step:421/5100 train_loss:3.9496 train_time:57325ms step_avg:139.48ms
step:422/5100 train_loss:3.8611 train_time:57463ms step_avg:139.47ms
step:423/5100 train_loss:3.9939 train_time:57601ms step_avg:139.47ms
step:424/5100 train_loss:4.0883 train_time:57739ms step_avg:139.47ms
step:425/5100 train_loss:3.8465 train_time:57876ms step_avg:139.46ms
step:426/5100 train_loss:4.0298 train_time:58013ms step_avg:139.46ms
step:427/5100 train_loss:3.9139 train_time:58154ms step_avg:139.46ms
step:428/5100 train_loss:4.1199 train_time:58291ms step_avg:139.45ms
step:429/5100 train_loss:4.0347 train_time:58433ms step_avg:139.46ms
step:430/5100 train_loss:3.9718 train_time:58565ms step_avg:139.44ms
step:431/5100 train_loss:3.9415 train_time:58703ms step_avg:139.44ms
step:432/5100 train_loss:3.8433 train_time:58840ms step_avg:139.43ms
step:433/5100 train_loss:3.9799 train_time:58979ms step_avg:139.43ms
step:434/5100 train_loss:4.0500 train_time:59117ms step_avg:139.43ms
step:435/5100 train_loss:3.9839 train_time:59254ms step_avg:139.42ms
step:436/5100 train_loss:4.0332 train_time:59393ms step_avg:139.42ms
step:437/5100 train_loss:4.0428 train_time:59529ms step_avg:139.41ms
step:438/5100 train_loss:3.9308 train_time:59667ms step_avg:139.41ms
step:439/5100 train_loss:3.9383 train_time:59805ms step_avg:139.41ms
step:440/5100 train_loss:3.9214 train_time:59942ms step_avg:139.40ms
step:441/5100 train_loss:4.0970 train_time:60082ms step_avg:139.40ms
step:442/5100 train_loss:3.9771 train_time:60220ms step_avg:139.40ms
step:443/5100 train_loss:3.9684 train_time:60359ms step_avg:139.40ms
step:444/5100 train_loss:3.8548 train_time:60497ms step_avg:139.39ms
step:445/5100 train_loss:4.1279 train_time:60633ms step_avg:139.39ms
step:446/5100 train_loss:4.0566 train_time:60773ms step_avg:139.39ms
step:447/5100 train_loss:4.0513 train_time:60911ms step_avg:139.38ms
step:448/5100 train_loss:3.9690 train_time:61048ms step_avg:139.38ms
step:449/5100 train_loss:4.0651 train_time:61186ms step_avg:139.38ms
step:450/5100 train_loss:3.8890 train_time:61324ms step_avg:139.37ms
step:451/5100 train_loss:3.9380 train_time:61463ms step_avg:139.37ms
step:452/5100 train_loss:3.7946 train_time:61602ms step_avg:139.37ms
step:453/5100 train_loss:3.9224 train_time:61741ms step_avg:139.37ms
step:454/5100 train_loss:3.8910 train_time:61879ms step_avg:139.37ms
step:455/5100 train_loss:3.8478 train_time:62016ms step_avg:139.36ms
step:456/5100 train_loss:4.0617 train_time:62153ms step_avg:139.36ms
step:457/5100 train_loss:3.9368 train_time:62296ms step_avg:139.37ms
step:458/5100 train_loss:4.0115 train_time:62429ms step_avg:139.35ms
step:459/5100 train_loss:4.0444 train_time:62568ms step_avg:139.35ms
step:460/5100 train_loss:3.8506 train_time:62706ms step_avg:139.35ms
step:461/5100 train_loss:4.0192 train_time:62843ms step_avg:139.34ms
step:462/5100 train_loss:3.9156 train_time:62983ms step_avg:139.34ms
step:463/5100 train_loss:3.9327 train_time:63122ms step_avg:139.34ms
step:464/5100 train_loss:3.9939 train_time:63260ms step_avg:139.34ms
step:465/5100 train_loss:3.9323 train_time:63404ms step_avg:139.35ms
step:466/5100 train_loss:3.9314 train_time:63533ms step_avg:139.33ms
step:467/5100 train_loss:4.0319 train_time:63672ms step_avg:139.33ms
step:468/5100 train_loss:4.0438 train_time:63809ms step_avg:139.32ms
step:469/5100 train_loss:4.0166 train_time:63947ms step_avg:139.32ms
step:470/5100 train_loss:3.8999 train_time:64086ms step_avg:139.32ms
step:471/5100 train_loss:3.9901 train_time:64224ms step_avg:139.32ms
step:472/5100 train_loss:4.0467 train_time:64362ms step_avg:139.31ms
step:473/5100 train_loss:3.9842 train_time:64499ms step_avg:139.31ms
step:474/5100 train_loss:3.9373 train_time:64637ms step_avg:139.30ms
step:475/5100 train_loss:3.7961 train_time:64774ms step_avg:139.30ms
step:476/5100 train_loss:4.2287 train_time:64911ms step_avg:139.29ms
step:477/5100 train_loss:3.9830 train_time:65050ms step_avg:139.29ms
step:478/5100 train_loss:3.7976 train_time:65188ms step_avg:139.29ms
step:479/5100 train_loss:4.0300 train_time:65331ms step_avg:139.30ms
step:480/5100 train_loss:3.9790 train_time:65464ms step_avg:139.29ms
step:481/5100 train_loss:4.1209 train_time:65603ms step_avg:139.29ms
step:482/5100 train_loss:3.9366 train_time:65742ms step_avg:139.28ms
step:483/5100 train_loss:3.7426 train_time:65880ms step_avg:139.28ms
step:484/5100 train_loss:4.0219 train_time:66018ms step_avg:139.28ms
step:485/5100 train_loss:3.8815 train_time:66155ms step_avg:139.27ms
step:486/5100 train_loss:3.8839 train_time:66293ms step_avg:139.27ms
step:487/5100 train_loss:3.8123 train_time:66430ms step_avg:139.27ms
step:488/5100 train_loss:3.8821 train_time:66568ms step_avg:139.26ms
step:489/5100 train_loss:4.0860 train_time:66706ms step_avg:139.26ms
step:490/5100 train_loss:3.9237 train_time:66844ms step_avg:139.26ms
step:491/5100 train_loss:3.8110 train_time:66981ms step_avg:139.25ms
step:492/5100 train_loss:3.8297 train_time:67120ms step_avg:139.25ms
step:493/5100 train_loss:3.9461 train_time:67259ms step_avg:139.25ms
step:494/5100 train_loss:3.7903 train_time:67396ms step_avg:139.25ms
step:495/5100 train_loss:3.9254 train_time:67532ms step_avg:139.24ms
step:496/5100 train_loss:3.8607 train_time:67670ms step_avg:139.24ms
step:497/5100 train_loss:3.7469 train_time:67808ms step_avg:139.24ms
step:498/5100 train_loss:3.9460 train_time:67947ms step_avg:139.24ms
step:499/5100 train_loss:4.0172 train_time:68086ms step_avg:139.24ms
step:500/5100 train_loss:4.0465 train_time:68224ms step_avg:139.23ms
step:500/5100 val_loss:3.9217 train_time:68281ms step_avg:139.35ms
step:501/5100 train_loss:3.9561 train_time:68372ms step_avg:139.25ms
step:502/5100 train_loss:4.0111 train_time:68520ms step_avg:139.27ms
step:503/5100 train_loss:3.9539 train_time:68661ms step_avg:139.27ms
step:504/5100 train_loss:3.9957 train_time:68800ms step_avg:139.27ms
step:505/5100 train_loss:3.9461 train_time:68935ms step_avg:139.26ms
step:506/5100 train_loss:4.0330 train_time:69068ms step_avg:139.25ms
step:507/5100 train_loss:3.8532 train_time:69204ms step_avg:139.24ms
step:508/5100 train_loss:3.9737 train_time:69341ms step_avg:139.24ms
step:509/5100 train_loss:4.0421 train_time:69480ms step_avg:139.24ms
step:510/5100 train_loss:3.9899 train_time:69621ms step_avg:139.24ms
step:511/5100 train_loss:3.7958 train_time:69759ms step_avg:139.24ms
step:512/5100 train_loss:3.9957 train_time:69896ms step_avg:139.24ms
step:513/5100 train_loss:3.9339 train_time:70032ms step_avg:139.23ms
step:514/5100 train_loss:3.8916 train_time:70169ms step_avg:139.22ms
step:515/5100 train_loss:3.9654 train_time:70306ms step_avg:139.22ms
step:516/5100 train_loss:3.9554 train_time:70445ms step_avg:139.22ms
step:517/5100 train_loss:4.2958 train_time:70584ms step_avg:139.22ms
step:518/5100 train_loss:3.8930 train_time:70722ms step_avg:139.22ms
step:519/5100 train_loss:4.0009 train_time:70861ms step_avg:139.22ms
step:520/5100 train_loss:3.8956 train_time:70999ms step_avg:139.21ms
step:521/5100 train_loss:3.8953 train_time:71136ms step_avg:139.21ms
step:522/5100 train_loss:3.8476 train_time:71274ms step_avg:139.21ms
step:523/5100 train_loss:3.8683 train_time:71411ms step_avg:139.20ms
step:524/5100 train_loss:4.4850 train_time:71550ms step_avg:139.20ms
step:525/5100 train_loss:3.9602 train_time:71689ms step_avg:139.20ms
step:526/5100 train_loss:3.8928 train_time:71828ms step_avg:139.20ms
step:527/5100 train_loss:3.9019 train_time:71967ms step_avg:139.20ms
step:528/5100 train_loss:3.8572 train_time:72106ms step_avg:139.20ms
step:529/5100 train_loss:3.8337 train_time:72243ms step_avg:139.20ms
step:530/5100 train_loss:4.0592 train_time:72382ms step_avg:139.20ms
step:531/5100 train_loss:3.8568 train_time:72520ms step_avg:139.19ms
step:532/5100 train_loss:4.1314 train_time:72658ms step_avg:139.19ms
step:533/5100 train_loss:3.9425 train_time:72794ms step_avg:139.19ms
step:534/5100 train_loss:3.8750 train_time:72932ms step_avg:139.18ms
step:535/5100 train_loss:3.8911 train_time:73071ms step_avg:139.18ms
step:536/5100 train_loss:3.8255 train_time:73212ms step_avg:139.19ms
step:537/5100 train_loss:3.9547 train_time:73347ms step_avg:139.18ms
step:538/5100 train_loss:3.9478 train_time:73486ms step_avg:139.18ms
step:539/5100 train_loss:3.8460 train_time:73623ms step_avg:139.17ms
step:540/5100 train_loss:4.3399 train_time:73761ms step_avg:139.17ms
step:541/5100 train_loss:3.8758 train_time:73900ms step_avg:139.17ms
step:542/5100 train_loss:3.9975 train_time:74038ms step_avg:139.17ms
step:543/5100 train_loss:3.8203 train_time:74176ms step_avg:139.17ms
step:544/5100 train_loss:3.7893 train_time:74314ms step_avg:139.16ms
step:545/5100 train_loss:3.8795 train_time:74453ms step_avg:139.16ms
step:546/5100 train_loss:3.8012 train_time:74590ms step_avg:139.16ms
step:547/5100 train_loss:3.8510 train_time:74728ms step_avg:139.16ms
step:548/5100 train_loss:3.8619 train_time:74866ms step_avg:139.16ms
step:549/5100 train_loss:3.8400 train_time:75004ms step_avg:139.15ms
step:550/5100 train_loss:3.9310 train_time:75143ms step_avg:139.15ms
step:551/5100 train_loss:3.8177 train_time:75287ms step_avg:139.16ms
step:552/5100 train_loss:3.8333 train_time:75422ms step_avg:139.15ms
step:553/5100 train_loss:4.1591 train_time:75560ms step_avg:139.15ms
step:554/5100 train_loss:3.9571 train_time:75697ms step_avg:139.15ms
step:555/5100 train_loss:3.9176 train_time:75837ms step_avg:139.15ms
step:556/5100 train_loss:3.8659 train_time:75974ms step_avg:139.15ms
step:557/5100 train_loss:3.9007 train_time:76113ms step_avg:139.15ms
step:558/5100 train_loss:3.5519 train_time:76247ms step_avg:139.14ms
step:559/5100 train_loss:3.8156 train_time:76391ms step_avg:139.15ms
step:560/5100 train_loss:3.8589 train_time:76523ms step_avg:139.13ms
step:561/5100 train_loss:3.9072 train_time:76660ms step_avg:139.13ms
step:562/5100 train_loss:3.8184 train_time:76798ms step_avg:139.13ms
step:563/5100 train_loss:3.7589 train_time:76936ms step_avg:139.13ms
step:564/5100 train_loss:3.9690 train_time:77074ms step_avg:139.12ms
step:565/5100 train_loss:3.7782 train_time:77211ms step_avg:139.12ms
step:566/5100 train_loss:3.8941 train_time:77350ms step_avg:139.12ms
step:567/5100 train_loss:3.8532 train_time:77637ms step_avg:139.38ms
step:568/5100 train_loss:3.7919 train_time:77768ms step_avg:139.37ms
step:569/5100 train_loss:3.8933 train_time:77905ms step_avg:139.37ms
step:570/5100 train_loss:3.8644 train_time:78210ms step_avg:139.66ms
step:571/5100 train_loss:3.8959 train_time:78343ms step_avg:139.65ms
step:572/5100 train_loss:3.9722 train_time:78481ms step_avg:139.65ms
step:573/5100 train_loss:3.9290 train_time:78616ms step_avg:139.64ms
step:574/5100 train_loss:3.9312 train_time:78751ms step_avg:139.63ms
step:575/5100 train_loss:3.9838 train_time:78886ms step_avg:139.62ms
step:576/5100 train_loss:3.9353 train_time:79022ms step_avg:139.61ms
step:577/5100 train_loss:3.9525 train_time:79164ms step_avg:139.62ms
step:578/5100 train_loss:3.8954 train_time:79303ms step_avg:139.62ms
step:579/5100 train_loss:3.8816 train_time:79442ms step_avg:139.62ms
step:580/5100 train_loss:3.8733 train_time:79576ms step_avg:139.61ms
step:581/5100 train_loss:3.8095 train_time:79712ms step_avg:139.60ms
step:582/5100 train_loss:3.8385 train_time:79849ms step_avg:139.60ms
step:583/5100 train_loss:4.0644 train_time:79985ms step_avg:139.59ms
step:584/5100 train_loss:3.8356 train_time:80124ms step_avg:139.59ms
step:585/5100 train_loss:3.7973 train_time:80262ms step_avg:139.59ms
step:586/5100 train_loss:3.9919 train_time:80401ms step_avg:139.59ms
step:587/5100 train_loss:3.7427 train_time:80538ms step_avg:139.58ms
step:588/5100 train_loss:3.8762 train_time:80679ms step_avg:139.58ms
step:589/5100 train_loss:3.8547 train_time:80814ms step_avg:139.57ms
step:590/5100 train_loss:4.2115 train_time:80950ms step_avg:139.57ms
step:591/5100 train_loss:3.9845 train_time:81087ms step_avg:139.56ms
step:592/5100 train_loss:3.7295 train_time:81225ms step_avg:139.56ms
step:593/5100 train_loss:3.7422 train_time:81363ms step_avg:139.56ms
step:594/5100 train_loss:3.7379 train_time:81501ms step_avg:139.56ms
step:595/5100 train_loss:3.7702 train_time:81639ms step_avg:139.55ms
step:596/5100 train_loss:4.1331 train_time:81776ms step_avg:139.55ms
step:597/5100 train_loss:3.8531 train_time:81912ms step_avg:139.54ms
step:598/5100 train_loss:3.7900 train_time:82051ms step_avg:139.54ms
step:599/5100 train_loss:3.8641 train_time:82190ms step_avg:139.54ms
step:600/5100 train_loss:3.6823 train_time:82328ms step_avg:139.54ms
step:601/5100 train_loss:3.8057 train_time:82466ms step_avg:139.54ms
step:602/5100 train_loss:3.8430 train_time:82605ms step_avg:139.54ms
step:603/5100 train_loss:3.8635 train_time:82746ms step_avg:139.54ms
step:604/5100 train_loss:3.9939 train_time:82882ms step_avg:139.53ms
step:605/5100 train_loss:3.8432 train_time:83024ms step_avg:139.54ms
step:606/5100 train_loss:3.8275 train_time:83157ms step_avg:139.53ms
step:607/5100 train_loss:3.7745 train_time:83294ms step_avg:139.52ms
step:608/5100 train_loss:4.0221 train_time:83431ms step_avg:139.52ms
step:609/5100 train_loss:3.8570 train_time:83569ms step_avg:139.51ms
step:610/5100 train_loss:3.8224 train_time:83709ms step_avg:139.52ms
step:611/5100 train_loss:3.9298 train_time:83846ms step_avg:139.51ms
step:612/5100 train_loss:3.8260 train_time:83985ms step_avg:139.51ms
step:613/5100 train_loss:3.8078 train_time:84122ms step_avg:139.51ms
step:614/5100 train_loss:3.9698 train_time:84261ms step_avg:139.50ms
step:615/5100 train_loss:3.9367 train_time:84401ms step_avg:139.51ms
step:616/5100 train_loss:3.8944 train_time:84536ms step_avg:139.50ms
step:617/5100 train_loss:3.8270 train_time:84673ms step_avg:139.49ms
step:618/5100 train_loss:3.7832 train_time:84815ms step_avg:139.50ms
step:619/5100 train_loss:3.8856 train_time:84949ms step_avg:139.49ms
step:620/5100 train_loss:3.7870 train_time:85086ms step_avg:139.48ms
step:621/5100 train_loss:3.7945 train_time:85223ms step_avg:139.48ms
step:622/5100 train_loss:4.1097 train_time:85362ms step_avg:139.48ms
step:623/5100 train_loss:3.7936 train_time:85499ms step_avg:139.48ms
step:624/5100 train_loss:3.8243 train_time:85637ms step_avg:139.47ms
step:625/5100 train_loss:3.9073 train_time:85777ms step_avg:139.47ms
step:625/5100 val_loss:3.8332 train_time:85831ms step_avg:139.56ms
step:626/5100 train_loss:3.9333 train_time:85920ms step_avg:139.48ms
step:627/5100 train_loss:3.9558 train_time:86068ms step_avg:139.49ms
step:628/5100 train_loss:3.9378 train_time:86207ms step_avg:139.49ms
step:629/5100 train_loss:3.9773 train_time:86344ms step_avg:139.49ms
step:630/5100 train_loss:3.8043 train_time:86479ms step_avg:139.48ms
step:631/5100 train_loss:3.9262 train_time:86614ms step_avg:139.48ms
step:632/5100 train_loss:3.9632 train_time:86750ms step_avg:139.47ms
step:633/5100 train_loss:3.8619 train_time:86886ms step_avg:139.46ms
step:634/5100 train_loss:3.7929 train_time:87024ms step_avg:139.46ms
step:635/5100 train_loss:3.8933 train_time:87165ms step_avg:139.46ms
step:636/5100 train_loss:4.1470 train_time:87307ms step_avg:139.47ms
step:637/5100 train_loss:3.7376 train_time:87439ms step_avg:139.46ms
step:638/5100 train_loss:3.5671 train_time:87576ms step_avg:139.45ms
step:639/5100 train_loss:3.7860 train_time:87712ms step_avg:139.45ms
step:640/5100 train_loss:3.8244 train_time:87848ms step_avg:139.44ms
step:641/5100 train_loss:3.7843 train_time:87984ms step_avg:139.44ms
step:642/5100 train_loss:3.7825 train_time:88127ms step_avg:139.44ms
step:643/5100 train_loss:3.8220 train_time:88259ms step_avg:139.43ms
step:644/5100 train_loss:3.8376 train_time:88397ms step_avg:139.43ms
step:645/5100 train_loss:3.7663 train_time:88534ms step_avg:139.42ms
step:646/5100 train_loss:3.9817 train_time:88673ms step_avg:139.42ms
step:647/5100 train_loss:3.8750 train_time:88812ms step_avg:139.42ms
step:648/5100 train_loss:3.8745 train_time:88945ms step_avg:139.41ms
step:649/5100 train_loss:3.9016 train_time:89093ms step_avg:139.43ms
step:650/5100 train_loss:3.9669 train_time:89221ms step_avg:139.41ms
step:651/5100 train_loss:3.8239 train_time:89357ms step_avg:139.40ms
step:652/5100 train_loss:3.9689 train_time:89494ms step_avg:139.40ms
step:653/5100 train_loss:3.7951 train_time:89631ms step_avg:139.39ms
step:654/5100 train_loss:3.8631 train_time:89769ms step_avg:139.39ms
step:655/5100 train_loss:3.6348 train_time:89908ms step_avg:139.39ms
step:656/5100 train_loss:3.7827 train_time:90045ms step_avg:139.39ms
step:657/5100 train_loss:3.7857 train_time:90185ms step_avg:139.39ms
step:658/5100 train_loss:3.7164 train_time:90327ms step_avg:139.39ms
step:659/5100 train_loss:3.8976 train_time:90458ms step_avg:139.38ms
step:660/5100 train_loss:3.7946 train_time:90596ms step_avg:139.38ms
step:661/5100 train_loss:3.8893 train_time:90732ms step_avg:139.37ms
step:662/5100 train_loss:3.9632 train_time:90870ms step_avg:139.37ms
step:663/5100 train_loss:3.8751 train_time:91006ms step_avg:139.37ms
step:664/5100 train_loss:3.7515 train_time:91143ms step_avg:139.36ms
step:665/5100 train_loss:3.8376 train_time:91281ms step_avg:139.36ms
step:666/5100 train_loss:3.7050 train_time:91424ms step_avg:139.37ms
step:667/5100 train_loss:3.9947 train_time:91557ms step_avg:139.36ms
step:668/5100 train_loss:3.8320 train_time:91696ms step_avg:139.36ms
step:669/5100 train_loss:3.8365 train_time:91833ms step_avg:139.35ms
step:670/5100 train_loss:3.6846 train_time:91973ms step_avg:139.35ms
step:671/5100 train_loss:3.8012 train_time:92112ms step_avg:139.35ms
step:672/5100 train_loss:3.7586 train_time:92250ms step_avg:139.35ms
step:673/5100 train_loss:3.7776 train_time:92388ms step_avg:139.35ms
step:674/5100 train_loss:4.0546 train_time:92525ms step_avg:139.34ms
step:675/5100 train_loss:3.8453 train_time:92663ms step_avg:139.34ms
step:676/5100 train_loss:3.9180 train_time:92800ms step_avg:139.34ms
step:677/5100 train_loss:3.6920 train_time:92938ms step_avg:139.34ms
step:678/5100 train_loss:3.7989 train_time:93075ms step_avg:139.33ms
step:679/5100 train_loss:3.7475 train_time:93213ms step_avg:139.33ms
step:680/5100 train_loss:3.8920 train_time:93351ms step_avg:139.33ms
step:681/5100 train_loss:3.7895 train_time:93488ms step_avg:139.33ms
step:682/5100 train_loss:3.8236 train_time:93626ms step_avg:139.32ms
step:683/5100 train_loss:3.8918 train_time:93762ms step_avg:139.32ms
step:684/5100 train_loss:3.9347 train_time:93901ms step_avg:139.32ms
step:685/5100 train_loss:3.8397 train_time:94039ms step_avg:139.32ms
step:686/5100 train_loss:3.9155 train_time:94178ms step_avg:139.32ms
step:687/5100 train_loss:3.8373 train_time:94317ms step_avg:139.32ms
step:688/5100 train_loss:3.8846 train_time:94454ms step_avg:139.31ms
step:689/5100 train_loss:3.5038 train_time:94593ms step_avg:139.31ms
step:690/5100 train_loss:3.6219 train_time:94731ms step_avg:139.31ms
step:691/5100 train_loss:3.7622 train_time:94870ms step_avg:139.31ms
step:692/5100 train_loss:3.6431 train_time:95006ms step_avg:139.30ms
step:693/5100 train_loss:3.8403 train_time:95152ms step_avg:139.31ms
step:694/5100 train_loss:3.8703 train_time:95291ms step_avg:139.31ms
step:695/5100 train_loss:3.7578 train_time:95425ms step_avg:139.31ms
step:696/5100 train_loss:3.7413 train_time:95562ms step_avg:139.30ms
step:697/5100 train_loss:4.0637 train_time:95699ms step_avg:139.30ms
step:698/5100 train_loss:3.8053 train_time:95841ms step_avg:139.30ms
step:699/5100 train_loss:3.8453 train_time:95974ms step_avg:139.29ms
step:700/5100 train_loss:4.0081 train_time:96112ms step_avg:139.29ms
step:701/5100 train_loss:3.7722 train_time:96254ms step_avg:139.30ms
step:702/5100 train_loss:3.7387 train_time:96389ms step_avg:139.29ms
step:703/5100 train_loss:3.7250 train_time:96526ms step_avg:139.29ms
step:704/5100 train_loss:3.6861 train_time:96664ms step_avg:139.29ms
step:705/5100 train_loss:3.7726 train_time:96801ms step_avg:139.28ms
step:706/5100 train_loss:3.7636 train_time:96942ms step_avg:139.28ms
step:707/5100 train_loss:3.7817 train_time:97078ms step_avg:139.28ms
step:708/5100 train_loss:3.8518 train_time:97215ms step_avg:139.28ms
step:709/5100 train_loss:3.7979 train_time:97351ms step_avg:139.27ms
step:710/5100 train_loss:3.7847 train_time:97490ms step_avg:139.27ms
step:711/5100 train_loss:3.7529 train_time:97630ms step_avg:139.27ms
step:712/5100 train_loss:3.7886 train_time:97767ms step_avg:139.27ms
step:713/5100 train_loss:3.8541 train_time:97904ms step_avg:139.27ms
step:714/5100 train_loss:3.8567 train_time:98042ms step_avg:139.26ms
step:715/5100 train_loss:3.7737 train_time:98180ms step_avg:139.26ms
step:716/5100 train_loss:3.7789 train_time:98317ms step_avg:139.26ms
step:717/5100 train_loss:3.7897 train_time:98456ms step_avg:139.26ms
step:718/5100 train_loss:3.9336 train_time:98593ms step_avg:139.26ms
step:719/5100 train_loss:3.7990 train_time:98731ms step_avg:139.25ms
step:720/5100 train_loss:3.8743 train_time:98869ms step_avg:139.25ms
step:721/5100 train_loss:4.0400 train_time:99012ms step_avg:139.26ms
step:722/5100 train_loss:3.6634 train_time:99143ms step_avg:139.25ms
step:723/5100 train_loss:3.9287 train_time:99283ms step_avg:139.25ms
step:724/5100 train_loss:3.9863 train_time:99424ms step_avg:139.25ms
step:725/5100 train_loss:3.7745 train_time:99558ms step_avg:139.24ms
step:726/5100 train_loss:3.8520 train_time:99696ms step_avg:139.24ms
step:727/5100 train_loss:3.7435 train_time:99833ms step_avg:139.24ms
step:728/5100 train_loss:3.7602 train_time:99973ms step_avg:139.24ms
step:729/5100 train_loss:3.9393 train_time:100113ms step_avg:139.24ms
step:730/5100 train_loss:3.8841 train_time:100251ms step_avg:139.24ms
step:731/5100 train_loss:3.8852 train_time:100389ms step_avg:139.24ms
step:732/5100 train_loss:3.7646 train_time:100526ms step_avg:139.23ms
step:733/5100 train_loss:3.7873 train_time:100664ms step_avg:139.23ms
step:734/5100 train_loss:4.0333 train_time:100801ms step_avg:139.23ms
step:735/5100 train_loss:3.7638 train_time:100939ms step_avg:139.23ms
step:736/5100 train_loss:3.8249 train_time:101077ms step_avg:139.22ms
step:737/5100 train_loss:3.9454 train_time:101216ms step_avg:139.22ms
step:738/5100 train_loss:3.8586 train_time:101354ms step_avg:139.22ms
step:739/5100 train_loss:3.8052 train_time:101493ms step_avg:139.22ms
step:740/5100 train_loss:3.7071 train_time:101631ms step_avg:139.22ms
step:741/5100 train_loss:4.3420 train_time:101768ms step_avg:139.22ms
step:742/5100 train_loss:3.7017 train_time:101906ms step_avg:139.22ms
step:743/5100 train_loss:3.7909 train_time:102048ms step_avg:139.22ms
step:744/5100 train_loss:3.7871 train_time:102181ms step_avg:139.21ms
step:745/5100 train_loss:3.8521 train_time:102319ms step_avg:139.21ms
step:746/5100 train_loss:3.8252 train_time:102462ms step_avg:139.22ms
step:747/5100 train_loss:3.8049 train_time:102598ms step_avg:139.21ms
step:748/5100 train_loss:3.8289 train_time:102733ms step_avg:139.20ms
step:749/5100 train_loss:3.7632 train_time:102871ms step_avg:139.20ms
step:750/5100 train_loss:3.7668 train_time:103010ms step_avg:139.20ms
step:750/5100 val_loss:3.7762 train_time:103064ms step_avg:139.28ms
step:751/5100 train_loss:3.8046 train_time:103153ms step_avg:139.21ms
step:752/5100 train_loss:3.7659 train_time:103302ms step_avg:139.22ms
step:753/5100 train_loss:3.8058 train_time:103441ms step_avg:139.22ms
step:754/5100 train_loss:3.8185 train_time:103577ms step_avg:139.22ms
step:755/5100 train_loss:3.7925 train_time:103712ms step_avg:139.21ms
step:756/5100 train_loss:3.8697 train_time:103993ms step_avg:139.40ms
step:757/5100 train_loss:3.6961 train_time:104125ms step_avg:139.39ms
step:758/5100 train_loss:3.9335 train_time:104263ms step_avg:139.39ms
step:759/5100 train_loss:3.8551 train_time:104429ms step_avg:139.42ms
step:760/5100 train_loss:3.7836 train_time:104737ms step_avg:139.65ms
step:761/5100 train_loss:3.8929 train_time:104882ms step_avg:139.66ms
step:762/5100 train_loss:3.6023 train_time:105018ms step_avg:139.65ms
step:763/5100 train_loss:3.7522 train_time:105154ms step_avg:139.65ms
step:764/5100 train_loss:3.8731 train_time:105288ms step_avg:139.64ms
step:765/5100 train_loss:3.5191 train_time:105423ms step_avg:139.63ms
step:766/5100 train_loss:3.9497 train_time:105560ms step_avg:139.63ms
step:767/5100 train_loss:3.7978 train_time:105698ms step_avg:139.63ms
step:768/5100 train_loss:3.7615 train_time:105838ms step_avg:139.63ms
step:769/5100 train_loss:3.7772 train_time:105976ms step_avg:139.63ms
step:770/5100 train_loss:3.8025 train_time:106112ms step_avg:139.62ms
step:771/5100 train_loss:3.8567 train_time:106250ms step_avg:139.62ms
step:772/5100 train_loss:4.0845 train_time:106387ms step_avg:139.62ms
step:773/5100 train_loss:3.6649 train_time:106524ms step_avg:139.61ms
step:774/5100 train_loss:3.8573 train_time:106659ms step_avg:139.61ms
step:775/5100 train_loss:3.8442 train_time:106797ms step_avg:139.60ms
step:776/5100 train_loss:3.8098 train_time:106936ms step_avg:139.60ms
step:777/5100 train_loss:3.6113 train_time:107074ms step_avg:139.60ms
step:778/5100 train_loss:3.6157 train_time:107211ms step_avg:139.60ms
step:779/5100 train_loss:3.6795 train_time:107348ms step_avg:139.59ms
step:780/5100 train_loss:3.7741 train_time:107484ms step_avg:139.59ms
step:781/5100 train_loss:3.8067 train_time:107620ms step_avg:139.59ms
step:782/5100 train_loss:3.8655 train_time:107758ms step_avg:139.58ms
step:783/5100 train_loss:3.7734 train_time:107895ms step_avg:139.58ms
step:784/5100 train_loss:3.7810 train_time:108033ms step_avg:139.58ms
step:785/5100 train_loss:3.7808 train_time:108171ms step_avg:139.57ms
step:786/5100 train_loss:3.7598 train_time:108313ms step_avg:139.58ms
step:787/5100 train_loss:3.6616 train_time:108445ms step_avg:139.57ms
step:788/5100 train_loss:3.9206 train_time:108581ms step_avg:139.56ms
step:789/5100 train_loss:3.7031 train_time:108719ms step_avg:139.56ms
step:790/5100 train_loss:3.7688 train_time:108856ms step_avg:139.56ms
step:791/5100 train_loss:3.8265 train_time:108995ms step_avg:139.56ms
step:792/5100 train_loss:3.9557 train_time:109133ms step_avg:139.56ms
step:793/5100 train_loss:3.9669 train_time:109271ms step_avg:139.55ms
step:794/5100 train_loss:3.6791 train_time:109408ms step_avg:139.55ms
step:795/5100 train_loss:3.7991 train_time:109545ms step_avg:139.55ms
step:796/5100 train_loss:3.8606 train_time:109687ms step_avg:139.55ms
step:797/5100 train_loss:3.9588 train_time:109819ms step_avg:139.54ms
step:798/5100 train_loss:3.7177 train_time:109957ms step_avg:139.54ms
step:799/5100 train_loss:3.8598 train_time:110094ms step_avg:139.54ms
step:800/5100 train_loss:3.7615 train_time:110232ms step_avg:139.53ms
step:801/5100 train_loss:3.7454 train_time:110369ms step_avg:139.53ms
step:802/5100 train_loss:3.8360 train_time:110508ms step_avg:139.53ms
step:803/5100 train_loss:3.6961 train_time:110643ms step_avg:139.52ms
step:804/5100 train_loss:3.7195 train_time:110780ms step_avg:139.52ms
step:805/5100 train_loss:3.8376 train_time:110918ms step_avg:139.52ms
step:806/5100 train_loss:3.7351 train_time:111056ms step_avg:139.52ms
step:807/5100 train_loss:3.7443 train_time:111194ms step_avg:139.52ms
step:808/5100 train_loss:3.8411 train_time:111333ms step_avg:139.51ms
step:809/5100 train_loss:3.7631 train_time:111471ms step_avg:139.51ms
step:810/5100 train_loss:3.6898 train_time:111608ms step_avg:139.51ms
step:811/5100 train_loss:3.7699 train_time:111746ms step_avg:139.51ms
step:812/5100 train_loss:3.8006 train_time:111885ms step_avg:139.51ms
step:813/5100 train_loss:3.7980 train_time:112023ms step_avg:139.51ms
step:814/5100 train_loss:3.8376 train_time:112161ms step_avg:139.50ms
step:815/5100 train_loss:3.7793 train_time:112299ms step_avg:139.50ms
step:816/5100 train_loss:3.7662 train_time:112437ms step_avg:139.50ms
step:817/5100 train_loss:3.8636 train_time:112578ms step_avg:139.50ms
step:818/5100 train_loss:3.9629 train_time:112715ms step_avg:139.50ms
step:819/5100 train_loss:3.7258 train_time:112851ms step_avg:139.49ms
step:820/5100 train_loss:3.9247 train_time:112988ms step_avg:139.49ms
step:821/5100 train_loss:3.7140 train_time:113125ms step_avg:139.49ms
step:822/5100 train_loss:3.7467 train_time:113264ms step_avg:139.49ms
step:823/5100 train_loss:3.8659 train_time:113402ms step_avg:139.49ms
step:824/5100 train_loss:3.7897 train_time:113541ms step_avg:139.48ms
step:825/5100 train_loss:3.7100 train_time:113678ms step_avg:139.48ms
step:826/5100 train_loss:3.8193 train_time:113815ms step_avg:139.48ms
step:827/5100 train_loss:3.7041 train_time:113953ms step_avg:139.48ms
step:828/5100 train_loss:3.9311 train_time:114090ms step_avg:139.47ms
step:829/5100 train_loss:3.8221 train_time:114229ms step_avg:139.47ms
step:830/5100 train_loss:3.8811 train_time:114374ms step_avg:139.48ms
step:831/5100 train_loss:3.7370 train_time:114507ms step_avg:139.47ms
step:832/5100 train_loss:3.7842 train_time:114643ms step_avg:139.47ms
step:833/5100 train_loss:3.7224 train_time:114783ms step_avg:139.47ms
step:834/5100 train_loss:3.8465 train_time:114922ms step_avg:139.47ms
step:835/5100 train_loss:3.6851 train_time:115061ms step_avg:139.47ms
step:836/5100 train_loss:3.6602 train_time:115197ms step_avg:139.46ms
step:837/5100 train_loss:3.9299 train_time:115337ms step_avg:139.46ms
step:838/5100 train_loss:3.6223 train_time:115475ms step_avg:139.46ms
step:839/5100 train_loss:3.7946 train_time:115611ms step_avg:139.46ms
step:840/5100 train_loss:3.6272 train_time:115751ms step_avg:139.46ms
step:841/5100 train_loss:3.6800 train_time:115886ms step_avg:139.45ms
step:842/5100 train_loss:3.7635 train_time:116026ms step_avg:139.45ms
step:843/5100 train_loss:3.7904 train_time:116163ms step_avg:139.45ms
step:844/5100 train_loss:3.7859 train_time:116301ms step_avg:139.45ms
step:845/5100 train_loss:3.6307 train_time:116439ms step_avg:139.45ms
step:846/5100 train_loss:3.8685 train_time:116576ms step_avg:139.44ms
step:847/5100 train_loss:3.7288 train_time:116713ms step_avg:139.44ms
step:848/5100 train_loss:3.6876 train_time:116852ms step_avg:139.44ms
step:849/5100 train_loss:3.8307 train_time:116989ms step_avg:139.44ms
step:850/5100 train_loss:3.6983 train_time:117126ms step_avg:139.44ms
step:851/5100 train_loss:3.6514 train_time:117268ms step_avg:139.44ms
step:852/5100 train_loss:3.9430 train_time:117402ms step_avg:139.43ms
step:853/5100 train_loss:3.6516 train_time:117541ms step_avg:139.43ms
step:854/5100 train_loss:3.7659 train_time:117679ms step_avg:139.43ms
step:855/5100 train_loss:3.8508 train_time:117820ms step_avg:139.43ms
step:856/5100 train_loss:3.7360 train_time:117955ms step_avg:139.43ms
step:857/5100 train_loss:3.7490 train_time:118092ms step_avg:139.42ms
step:858/5100 train_loss:3.8014 train_time:118231ms step_avg:139.42ms
step:859/5100 train_loss:3.6897 train_time:118368ms step_avg:139.42ms
step:860/5100 train_loss:3.7627 train_time:118506ms step_avg:139.42ms
step:861/5100 train_loss:3.7988 train_time:118644ms step_avg:139.42ms
step:862/5100 train_loss:3.8416 train_time:118781ms step_avg:139.41ms
step:863/5100 train_loss:3.7928 train_time:118919ms step_avg:139.41ms
step:864/5100 train_loss:3.7774 train_time:119057ms step_avg:139.41ms
step:865/5100 train_loss:3.6072 train_time:119198ms step_avg:139.41ms
step:866/5100 train_loss:3.7888 train_time:119332ms step_avg:139.41ms
step:867/5100 train_loss:4.0722 train_time:119470ms step_avg:139.41ms
step:868/5100 train_loss:3.6484 train_time:119608ms step_avg:139.40ms
step:869/5100 train_loss:3.8335 train_time:119747ms step_avg:139.40ms
step:870/5100 train_loss:3.8166 train_time:119886ms step_avg:139.40ms
step:871/5100 train_loss:3.6549 train_time:120024ms step_avg:139.40ms
step:872/5100 train_loss:3.6262 train_time:120162ms step_avg:139.40ms
step:873/5100 train_loss:3.8683 train_time:120303ms step_avg:139.40ms
step:874/5100 train_loss:3.6525 train_time:120438ms step_avg:139.40ms
step:875/5100 train_loss:3.3738 train_time:120575ms step_avg:139.39ms
