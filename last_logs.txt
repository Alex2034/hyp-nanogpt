W0126 17:58:17.221647 139998356772672 torch/distributed/run.py:779] 
W0126 17:58:17.221647 139998356772672 torch/distributed/run.py:779] *****************************************
W0126 17:58:17.221647 139998356772672 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0126 17:58:17.221647 139998356772672 torch/distributed/run.py:779] *****************************************
[Rank 0] Using device: cuda:0
[Rank 0] This is the master process.
[Rank 0] Training DataLoader: 2700000000 tokens across 27 files.
[Rank 0] Validation DataLoader: 100000000 tokens across 1 files.
[Rank 1] Using device: cuda:1
W0126 17:58:27.845376 139998356772672 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGINT death signal, shutting down workers
W0126 17:58:27.845766 139998356772672 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 119467 closing signal SIGINT
W0126 17:58:27.846165 139998356772672 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 119468 closing signal SIGINT
[rank1]: Traceback (most recent call last):
[rank1]:   File "train_gpt2_refactored.py", line 277, in <module>
[rank1]:     model = GPT(config, tokenizer)  # Step 1: Create the model on CPU
[rank1]:   File "train_gpt2_refactored.py", line 99, in __init__
[rank1]:     h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
[rank1]:   File "train_gpt2_refactored.py", line 99, in <listcomp>
[rank1]:     h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
[rank1]:   File "/home/jovyan/fokin/modded-nanogpt/modules/layers.py", line 186, in __init__
[rank1]:     self.mlp = MLP(config)
[rank1]:   File "/home/jovyan/fokin/modded-nanogpt/modules/layers.py", line 40, in __init__
[rank1]:     self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 104, in __init__
[rank1]:     self.reset_parameters()
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 110, in reset_parameters
[rank1]:     init.kaiming_uniform_(self.weight, a=math.sqrt(5))
[rank1]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/nn/init.py", line 460, in kaiming_uniform_
[rank1]:     return tensor.uniform_(-bound, bound, generator=generator)
[rank1]: KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "train_gpt2_refactored.py", line 277, in <module>
[rank0]:     model = GPT(config, tokenizer)  # Step 1: Create the model on CPU
[rank0]:   File "train_gpt2_refactored.py", line 109, in __init__
[rank0]:     self.lm_head = LorentzMLR(
[rank0]:   File "/home/jovyan/fokin/modded-nanogpt/modules/layers.py", line 144, in __init__
[rank0]:     self.z = torch.nn.Parameter(F.pad(torch.zeros(num_classes, num_features-2), pad=(1,0), value=1)) # z should not be (0,0)
[rank0]:   File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/nn/functional.py", line 4552, in pad
[rank0]:     return torch._C._nn.pad(input, pad, mode, value)
[rank0]: KeyboardInterrupt
Traceback (most recent call last):
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/jovyan/miniconda3/envs/fokin_HCNN/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 119283 got signal: 2
