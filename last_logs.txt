W0215 19:25:07.551933 140131444729664 torch/distributed/run.py:779] 
W0215 19:25:07.551933 140131444729664 torch/distributed/run.py:779] *****************************************
W0215 19:25:07.551933 140131444729664 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0215 19:25:07.551933 140131444729664 torch/distributed/run.py:779] *****************************************
Tokenizer vocab size: 122
Tokenizer vocab size: 122
[Rank 0] Using device: cuda:0
[Rank 0] This is the master process.
[Rank 0] Training DataLoader: 187975984 tokens across 1 files.
[Rank 0] Validation DataLoader: 1909027 tokens across 1 files.
[Rank 1] Using device: cuda:1
[Rank 0] Model wrapped in DDP.
k params lengths: head = 0, attn = 0
k is not learned

=== Report ===
Model Size:    1.21M parameters

Data Path:            data/tinystories_char
Sequence Length:      1024
Batch Size (global):  512
Batch Size (device):  256
n_layers:              6
n_heads:               4
head_dim:             32
n_embd:               128
Seed:                 0
==============================

Logs for this run will be stored in: runs/15.02_69923_euc_head_0/
Writing logs to: runs/15.02_69923_euc_head_0/tensorboard_logs
step:0/300, tokens seen: 0.00M, val_loss:4.9898 train_time:0.20s step_avg:nanms
step:10/300, tokens seen:4.72M, avg_train_loss:2.9974 time:47/nans step_avg:nanms
step:10/300, tokens seen: 5.24M, val_loss:2.4196 train_time:0.00s step_avg:nanms
step:20/300, tokens seen:9.96M, avg_train_loss:2.3809 time:48/10s step_avg:34ms
step:20/300, tokens seen: 10.49M, val_loss:2.3759 train_time:0.34s step_avg:34ms
step:30/300, tokens seen:15.20M, avg_train_loss:2.2833 time:48/10s step_avg:34ms
step:30/300, tokens seen: 15.73M, val_loss:2.2279 train_time:0.69s step_avg:34ms
step:40/300, tokens seen:20.45M, avg_train_loss:2.1304 time:49/10s step_avg:34ms
step:40/300, tokens seen: 20.97M, val_loss:2.0084 train_time:1.07s step_avg:36ms
step:50/300, tokens seen:25.69M, avg_train_loss:1.9310 time:49/11s step_avg:35ms
step:50/300, tokens seen: 26.21M, val_loss:1.8650 train_time:1.41s step_avg:35ms
step:60/300, tokens seen:30.93M, avg_train_loss:1.7604 time:50/11s step_avg:35ms
step:60/300, tokens seen: 31.46M, val_loss:1.6469 train_time:1.75s step_avg:35ms
step:70/300, tokens seen:36.18M, avg_train_loss:1.5986 time:50/10s step_avg:35ms
step:70/300, tokens seen: 36.70M, val_loss:1.5084 train_time:2.09s step_avg:35ms
step:80/300, tokens seen:41.42M, avg_train_loss:1.4673 time:51/10s step_avg:35ms
step:80/300, tokens seen: 41.94M, val_loss:1.4112 train_time:2.44s step_avg:35ms
step:90/300, tokens seen:46.66M, avg_train_loss:1.3879 time:52/10s step_avg:35ms
step:90/300, tokens seen: 47.19M, val_loss:1.3372 train_time:2.78s step_avg:35ms
[Step 100] Generated Text: Once upon a time in a little girl nal bit. She dad. Lily said her day, 
step:100/300, tokens seen:51.90M, avg_train_loss:1.2951 time:63/47s step_avg:158ms
step:100/300, tokens seen: 52.43M, val_loss:1.2575 train_time:14.19s step_avg:158ms
step:110/300, tokens seen:57.15M, avg_train_loss:1.2337 time:81/76s step_avg:254ms
step:110/300, tokens seen: 57.67M, val_loss:1.2023 train_time:25.40s step_avg:254ms
step:120/300, tokens seen:62.39M, avg_train_loss:1.1757 time:82/70s step_avg:234ms
step:120/300, tokens seen: 62.91M, val_loss:1.1458 train_time:25.74s step_avg:234ms
step:130/300, tokens seen:67.63M, avg_train_loss:1.1250 time:83/65s step_avg:217ms
step:130/300, tokens seen: 68.16M, val_loss:1.1020 train_time:26.08s step_avg:217ms
step:140/300, tokens seen:72.88M, avg_train_loss:1.0880 time:83/61s step_avg:203ms
step:140/300, tokens seen: 73.40M, val_loss:1.0680 train_time:26.43s step_avg:203ms
step:150/300, tokens seen:78.12M, avg_train_loss:1.0523 time:84/57s step_avg:191ms
step:150/300, tokens seen: 78.64M, val_loss:1.0338 train_time:26.77s step_avg:191ms
step:160/300, tokens seen:83.36M, avg_train_loss:1.0167 time:84/54s step_avg:181ms
step:160/300, tokens seen: 83.89M, val_loss:0.9967 train_time:27.11s step_avg:181ms
step:170/300, tokens seen:88.60M, avg_train_loss:0.9905 time:85/51s step_avg:172ms
step:170/300, tokens seen: 89.13M, val_loss:0.9763 train_time:27.46s step_avg:172ms
step:180/300, tokens seen:93.85M, avg_train_loss:0.9667 time:85/49s step_avg:164ms
step:180/300, tokens seen: 94.37M, val_loss:0.9567 train_time:27.80s step_avg:164ms
step:190/300, tokens seen:99.09M, avg_train_loss:0.9435 time:86/47s step_avg:156ms
step:190/300, tokens seen: 99.61M, val_loss:0.9325 train_time:28.14s step_avg:156ms
[Step 200] Generated Text: Once upon a time in a time. He wanted to reach, judger on the story and
step:200/300, tokens seen:104.33M, avg_train_loss:0.9204 time:87/45s step_avg:151ms
step:200/300, tokens seen: 104.86M, val_loss:0.9079 train_time:28.66s step_avg:151ms
step:210/300, tokens seen:109.58M, avg_train_loss:0.8935 time:104/59s step_avg:197ms
step:210/300, tokens seen: 110.10M, val_loss:0.8832 train_time:39.39s step_avg:197ms
step:220/300, tokens seen:114.82M, avg_train_loss:0.8744 time:104/57s step_avg:189ms
step:220/300, tokens seen: 115.34M, val_loss:0.8708 train_time:39.74s step_avg:189ms
step:230/300, tokens seen:120.06M, avg_train_loss:0.8643 time:105/55s step_avg:182ms
step:230/300, tokens seen: 120.59M, val_loss:0.8486 train_time:40.08s step_avg:182ms
step:240/300, tokens seen:125.30M, avg_train_loss:0.8411 time:105/53s step_avg:176ms
step:240/300, tokens seen: 125.83M, val_loss:0.8356 train_time:40.42s step_avg:176ms
step:250/300, tokens seen:130.55M, avg_train_loss:0.8301 time:106/51s step_avg:170ms
step:250/300, tokens seen: 131.07M, val_loss:0.8215 train_time:40.77s step_avg:170ms
step:260/300, tokens seen:135.79M, avg_train_loss:0.8160 time:106/49s step_avg:164ms
step:260/300, tokens seen: 136.31M, val_loss:0.8104 train_time:41.11s step_avg:164ms
step:270/300, tokens seen:141.03M, avg_train_loss:0.8039 time:107/48s step_avg:159ms
step:270/300, tokens seen: 141.56M, val_loss:0.8005 train_time:41.46s step_avg:159ms
step:280/300, tokens seen:146.28M, avg_train_loss:0.7940 time:108/46s step_avg:155ms
step:280/300, tokens seen: 146.80M, val_loss:0.7917 train_time:41.80s step_avg:155ms
step:290/300, tokens seen:151.52M, avg_train_loss:0.7890 time:108/45s step_avg:151ms
step:290/300, tokens seen: 152.04M, val_loss:0.7854 train_time:42.14s step_avg:151ms
[Step 300] Generated Text: Once upon a time in a comtret from all the others named Tom's cawl!

"W
step:300/300, tokens seen:156.76M, avg_train_loss:0.7843 time:109/44s step_avg:147ms
[rank0]:W0215 19:27:13.339193 140674875000640 torch/_dynamo/convert_frame.py:762] [0/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W0215 19:27:13.339193 140674875000640 torch/_dynamo/convert_frame.py:762] [0/8]    function: 'forward' (/home/jovyan/fokin/modded-nanogpt/model/model.py:222)
[rank0]:W0215 19:27:13.339193 140674875000640 torch/_dynamo/convert_frame.py:762] [0/8]    last reason: ___check_obj_id(L['self'].transformer.h[0].attn.rotary.seq_len_cached, 94058399354464)
[rank0]:W0215 19:27:13.339193 140674875000640 torch/_dynamo/convert_frame.py:762] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W0215 19:27:13.339193 140674875000640 torch/_dynamo/convert_frame.py:762] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
step:300/300, tokens seen: 157.29M, val_loss:0.7810 train_time:42.66s step_avg:147ms
Total training time: 116.91s
peak memory consumption: 8769 MiB
