====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging

import random
import datetime
import time
from torch.utils.tensorboard import SummaryWriter
import json

import glob
from dataclasses import dataclass

import numpy as np
import math
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

from lib.lorentz.manifold import CustomLorentz
from lib.geoopt import ManifoldParameter
from lib.geoopt.optim import RiemannianSGD

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

class LorentzMLR(nn.Module):
    """ Multinomial logistic regression (MLR) in the Lorentz model
    """
    def __init__(
            self, 
            manifold: CustomLorentz, 
            num_features: int, 
            num_classes: int
        ):
        super(LorentzMLR, self).__init__()

        self.manifold = manifold

        self.a = torch.nn.Parameter(torch.zeros(num_classes,))
        self.z = torch.nn.Parameter(F.pad(torch.zeros(num_classes, num_features-2), pad=(1,0), value=1)) # z should not be (0,0)

        self.init_weights()

    def forward(self, x):
        # x: (B, T, num_features)

        # Hyperplane parameters
        sqrt_mK = 1 / self.manifold.k.sqrt()  # scalar
        norm_z = torch.norm(self.z, dim=-1)  # (num_classes,)
        w_t = torch.sinh(sqrt_mK * self.a) * norm_z  # (num_classes,)
        w_s = torch.cosh(sqrt_mK * self.a).unsqueeze(-1) * self.z  # (num_classes, num_features -1)

        beta = torch.sqrt(-w_t**2 + torch.norm(w_s, dim=-1)**2)  # (num_classes,)

        x0 = x.narrow(-1, 0, 1)  # (B, T, 1)
        x_rest = x.narrow(-1, 1, x.shape[-1]-1)  # (B, T, num_features -1)
        inner_prod = torch.matmul(x_rest, self.z.T)  # (B, T, num_classes)
        alpha = -x0 * w_t.view(1, 1, -1) + torch.cosh(sqrt_mK * self.a).view(1, 1, -1) * inner_prod  # (B, T, num_classes)
        sqrt_mK_alpha_over_beta = sqrt_mK * alpha / beta.view(1, 1, -1)
        d = self.manifold.k.sqrt() * torch.abs(torch.asinh(sqrt_mK_alpha_over_beta))  # (B, T, num_classes)

        logits = torch.sign(alpha) * beta.view(1, 1, -1) * d  # (B, T, num_classes)

        return logits

    def init_weights(self):
        stdv = 1. / math.sqrt(self.z.size(1))
        nn.init.uniform_(self.z, -stdv, stdv)
        nn.init.uniform_(self.a, -stdv, stdv)

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 784
    lm_head : str = 'euc'

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))

        if config.lm_head == 'euc':
            self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
            self.lm_head.weight.data.zero_()

        elif config.lm_head == 'hyp':
            self.manifold = CustomLorentz(k=torch.tensor([1.0]))
            self.lm_head = LorentzMLR(
                manifold=self.manifold,
                num_features=config.n_embd,
                num_classes=config.vocab_size
            )
        else:
            raise ValueError("Invalid lm_head, choose 'euc'/'hyp'.")

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),))
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    # input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    # input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    input_bin : str = 'data/tinystories/train.bin' # input .bin to train on
    input_val_bin : str = 'data/tinystories/val.bin' # input .bin to eval validation loss on
    num_vocab : int = 1000
    # optimization hyperparams
    batch_size : int = 2*32 # batch size, in sequences, across all devices
    device_batch_size : int = 32 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 100_000 # number of iterations to run (for FW 2.7B was 4578)
    warmup_iters : int = 100
    warmdown_iters : int = 100 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    train_loss_every : int = 100 
    val_loss_every : int = 500 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 6160384 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # model
    vocab_size : int = 1000
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 384
    lm_head : str = 'euc'
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
# num_vocab = 50304
model = GPT(GPTConfig(vocab_size=args.num_vocab, 
                      n_layer=args.n_layer, 
                      n_head=args.n_head,
                      n_embd=args.n_embd,
                      lm_head=args.lm_head))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
# optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)
# optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.003, betas=(0.9, 0.95), fused=True)
# optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)
# optimizers = [optimizer1, optimizer2, optimizer3]
# Fix the `k` parameter 
for name, param in raw_model.named_parameters():
    if "manifold.k" in name:
        param.requires_grad = False
        
lm_head_params = [p for p in raw_model.lm_head.parameters() if p.requires_grad]

params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
wte_params = [raw_model.transformer.wte.weight]
# scalar_params = [p for p in params if p.ndim < 2]

optimizer_lm_head = RiemannianSGD(
    [{'params': lm_head_params}], lr=0.005, weight_decay=5e-4, momentum=0.9, nesterov=True, stabilize=1
)

optimizer_muon = Muon(matrix_params, lr=0.05, momentum=0.95)
# optimizer_scalar = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizer_wte = torch.optim.Adam(wte_params, lr=0.6, betas=(0.8, 0.95), fused=True)

optimizers = [optimizer_lm_head, optimizer_muon, optimizer_wte]


# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it >= args.num_iterations - args.warmdown_iters:
        return (args.num_iterations - it) / args.warmdown_iters
    # 3) 
    else:
        decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)
        assert 0 <= decay_ratio <= 1
        return 0.1**decay_ratio
    
    # def get_lr(it, schedule='cos'):
    # # 1) linear warmup for warmup_iters steps
    #     if it < warmup_iters:
    #         return max_lr * it / warmup_iters
    #     # 2) if it > lr_decay_iters, return min learning rate
    #     if it > lr_decay_iters:
    #         return min_lr
    #     # 3) in between, use cosine decay down to min learning rate
    #     decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    #     assert 0 <= decay_ratio <= 1
        
    #     if schedule=='cos':
    #         coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1
    #         return min_lr + coeff * (max_lr - min_lr)

    #     elif schedule=='exp':
    #         return max_lr * (min_lr / max_lr) ** decay_ratio


schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]


# begin logging
if master_process:

    funny_animals = [
        "kinkajou", "platypus", "wombat", "parrot",
        "axolotl", "lemur", "blobfish", "dingo",
        "whale", "quokka", "sloth", "capybara", "emu",
        "koala", "marmoset", "meerkat", "panda"
    ]
    prefix = random.choice(funny_animals)
    now = datetime.datetime.now()
    date_part = now.strftime('%d.%m')  
    seconds_since_midnight = int(time.time() % 86400)
    run_id = f"{prefix}_{date_part}_{seconds_since_midnight}"

    # Create log directory and file
    logdir = f'runs/{run_id}/'
    os.makedirs(logdir, exist_ok=True)
    os.makedirs(os.path.join(logdir, "tensorboard_logs"), exist_ok=True)

    print(f"Logs for this run will be stored in: {logdir}")

    print("Writing logs to: " + os.path.join(logdir, "tensorboard_logs"))
    writer = SummaryWriter(log_dir=os.path.join(logdir, "tensorboard_logs"))

    config_path = os.path.join(logdir, "config.json")
    with open(config_path, "w") as f:
        json.dump(vars(args), f, indent=4)

    def pretty_json(hp):
        json_hp = json.dumps(hp, indent=2)
        return "".join("\t" + line for line in json_hp.splitlines(True))
    
    writer.add_text("run_params", pretty_json(vars(args)))
    logfile = os.path.join(logdir, 'log.txt')
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_s = 0.0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
total_t0 = time.time()
train_loss_accum = 0.0
train_loss_count = 0
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_s = 0.0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_s += time.time() - t0
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_s:.2f}s step_avg:{1000*training_time_s/(timed_steps-1):.0f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_s:.2f}s step_avg:{1000*training_time_s/(timed_steps-1):.0f}ms\n')
            writer.add_scalar('Loss/Validation', val_loss.item(), step)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_s += time.time() - t0
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'ckpts/%s_state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for name, p in model.named_parameters():
        if p.grad is None:
            # print(f"WARNING: Parameter {name} has no gradient. Skipping.")
            continue
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    train_loss_accum += train_loss.item()
    train_loss_count += 1
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process and (step+1) % args.train_loss_every == 0:
        avg_train_loss = train_loss_accum / train_loss_count
        elapsed_time = time.time() - total_t0
        approx_time = training_time_s + (time.time() - t0)
        avg_time_per_step = approx_time/timed_steps
        estimated_total_time = avg_time_per_step * args.num_iterations
        print(f"step:{step+1}/{args.num_iterations} avg_train_loss:{avg_train_loss:.4f} train_time:{approx_time:.0f}ms step_avg:{1000*avg_time_per_step:.0f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} avg_train_loss:{avg_train_loss:.4f} train_time:{approx_time:.0f}ms step_avg:{1000*avg_time_per_step:.0f}ms\n")
        writer.add_scalar('Loss/Train', avg_train_loss, step)
        train_loss_accum = 0.0
        train_loss_count = 0
if master_process:
    total_training_time = time.time() - total_t0
    print(f"Total training time: {total_training_time:.2f}s")
    with open(logfile, "a") as f:
        f.write(f"Total training time: {total_training_time:.2f}s\n")
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
if master_process:
    writer.close()
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.4.1+cu121 compiled for CUDA 12.1
nvidia-smi:
Thu Dec 12 15:00:29 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   26C    P0              71W / 700W |      0MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2D:00.0 Off |                    0 |
| N/A   28C    P0              71W / 700W |      0MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3F:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |  11075MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:66:00.0 Off |                    0 |
| N/A   26C    P0              72W / 700W |      0MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   26C    P0              72W / 700W |      0MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:AE:00.0 Off |                    0 |
| N/A   28C    P0              72W / 700W |      0MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:BF:00.0 Off |                    0 |
| N/A   29C    P0             118W / 700W |   1659MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E4:00.0 Off |                    0 |
| N/A   28C    P0             117W / 700W |   1659MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    2   N/A  N/A   2984539      C   python                                    11066MiB |
|    6   N/A  N/A   3001743      C   ...niconda3/envs/fokin_HCNN/bin/python     1650MiB |
|    7   N/A  N/A   3001744      C   ...niconda3/envs/fokin_HCNN/bin/python     1650MiB |
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/100000 val_loss:6.9078 train_time:1.22s step_avg:nanms
step:100/100000 avg_train_loss:6.0122 train_time:3ms step_avg:33ms
step:200/100000 avg_train_loss:4.5407 train_time:6ms step_avg:33ms
step:300/100000 avg_train_loss:3.9412 train_time:10ms step_avg:33ms
step:400/100000 avg_train_loss:3.5703 train_time:13ms step_avg:33ms
step:500/100000 avg_train_loss:3.2395 train_time:16ms step_avg:33ms
step:500/100000 val_loss:3.1343 train_time:16.13s step_avg:33ms
step:600/100000 avg_train_loss:3.0165 train_time:19ms step_avg:33ms
step:700/100000 avg_train_loss:2.8303 train_time:23ms step_avg:33ms
step:800/100000 avg_train_loss:2.6616 train_time:26ms step_avg:33ms
step:900/100000 avg_train_loss:2.5481 train_time:29ms step_avg:33ms
step:1000/100000 avg_train_loss:2.4513 train_time:33ms step_avg:33ms
step:1000/100000 val_loss:2.4099 train_time:32.57s step_avg:33ms
step:1100/100000 avg_train_loss:2.3564 train_time:36ms step_avg:33ms
step:1200/100000 avg_train_loss:2.2831 train_time:39ms step_avg:33ms
step:1300/100000 avg_train_loss:2.2168 train_time:42ms step_avg:33ms
step:1400/100000 avg_train_loss:2.1191 train_time:46ms step_avg:33ms
step:1500/100000 avg_train_loss:2.1311 train_time:49ms step_avg:33ms
step:1500/100000 val_loss:2.0599 train_time:49.06s step_avg:33ms
step:1600/100000 avg_train_loss:2.0349 train_time:52ms step_avg:33ms
step:1700/100000 avg_train_loss:2.0017 train_time:56ms step_avg:33ms
step:1800/100000 avg_train_loss:1.9504 train_time:59ms step_avg:33ms
step:1900/100000 avg_train_loss:1.9070 train_time:62ms step_avg:33ms
step:2000/100000 avg_train_loss:1.8786 train_time:66ms step_avg:33ms
step:2000/100000 val_loss:1.8482 train_time:65.51s step_avg:33ms
step:2100/100000 avg_train_loss:1.8284 train_time:69ms step_avg:33ms
step:2200/100000 avg_train_loss:1.7994 train_time:72ms step_avg:33ms
step:2300/100000 avg_train_loss:1.7825 train_time:75ms step_avg:33ms
step:2400/100000 avg_train_loss:1.7257 train_time:79ms step_avg:33ms
step:2500/100000 avg_train_loss:1.7287 train_time:82ms step_avg:33ms
step:2500/100000 val_loss:1.7113 train_time:81.98s step_avg:33ms
step:2600/100000 avg_train_loss:1.6885 train_time:85ms step_avg:33ms
step:2700/100000 avg_train_loss:1.6671 train_time:89ms step_avg:33ms
step:2800/100000 avg_train_loss:1.6438 train_time:92ms step_avg:33ms
step:2900/100000 avg_train_loss:1.6383 train_time:95ms step_avg:33ms
step:3000/100000 avg_train_loss:1.6266 train_time:98ms step_avg:33ms
step:3000/100000 val_loss:1.6141 train_time:98.44s step_avg:33ms
step:3100/100000 avg_train_loss:1.5639 train_time:102ms step_avg:33ms
step:3200/100000 avg_train_loss:1.5991 train_time:105ms step_avg:33ms
step:3300/100000 avg_train_loss:1.5648 train_time:108ms step_avg:33ms
step:3400/100000 avg_train_loss:1.5523 train_time:112ms step_avg:33ms
step:3500/100000 avg_train_loss:1.5251 train_time:115ms step_avg:33ms
step:3500/100000 val_loss:1.5396 train_time:114.92s step_avg:33ms
step:3600/100000 avg_train_loss:1.5401 train_time:118ms step_avg:33ms
step:3700/100000 avg_train_loss:1.5635 train_time:121ms step_avg:33ms
step:3800/100000 avg_train_loss:1.5423 train_time:125ms step_avg:33ms
step:3900/100000 avg_train_loss:1.5242 train_time:128ms step_avg:33ms
step:4000/100000 avg_train_loss:1.4729 train_time:131ms step_avg:33ms
step:4000/100000 val_loss:1.4854 train_time:131.36s step_avg:33ms
step:4100/100000 avg_train_loss:1.4523 train_time:135ms step_avg:33ms
step:4200/100000 avg_train_loss:1.4836 train_time:138ms step_avg:33ms
step:4300/100000 avg_train_loss:1.4506 train_time:141ms step_avg:33ms
step:4400/100000 avg_train_loss:1.4330 train_time:145ms step_avg:33ms
step:4500/100000 avg_train_loss:1.4658 train_time:148ms step_avg:33ms
step:4500/100000 val_loss:1.4383 train_time:147.84s step_avg:33ms
step:4600/100000 avg_train_loss:1.4563 train_time:151ms step_avg:33ms
step:4700/100000 avg_train_loss:1.4304 train_time:154ms step_avg:33ms
step:4800/100000 avg_train_loss:1.4161 train_time:158ms step_avg:33ms
step:4900/100000 avg_train_loss:1.4295 train_time:161ms step_avg:33ms
step:5000/100000 avg_train_loss:1.4215 train_time:164ms step_avg:33ms
step:5000/100000 val_loss:1.4026 train_time:164.28s step_avg:33ms
step:5100/100000 avg_train_loss:1.3952 train_time:168ms step_avg:33ms
step:5200/100000 avg_train_loss:1.4096 train_time:171ms step_avg:33ms
step:5300/100000 avg_train_loss:1.3727 train_time:174ms step_avg:33ms
step:5400/100000 avg_train_loss:1.4226 train_time:177ms step_avg:33ms
step:5500/100000 avg_train_loss:1.3684 train_time:181ms step_avg:33ms
step:5500/100000 val_loss:1.3741 train_time:180.77s step_avg:33ms
step:5600/100000 avg_train_loss:1.3546 train_time:184ms step_avg:33ms
step:5700/100000 avg_train_loss:1.3217 train_time:187ms step_avg:33ms
step:5800/100000 avg_train_loss:1.3497 train_time:191ms step_avg:33ms
step:5900/100000 avg_train_loss:1.3710 train_time:194ms step_avg:33ms
step:6000/100000 avg_train_loss:1.3368 train_time:197ms step_avg:33ms
step:6000/100000 val_loss:1.3472 train_time:197.24s step_avg:33ms
step:6100/100000 avg_train_loss:1.3389 train_time:201ms step_avg:33ms
step:6200/100000 avg_train_loss:1.3268 train_time:204ms step_avg:33ms
step:6300/100000 avg_train_loss:1.3232 train_time:207ms step_avg:33ms
step:6400/100000 avg_train_loss:1.3447 train_time:210ms step_avg:33ms
step:6500/100000 avg_train_loss:1.3235 train_time:214ms step_avg:33ms
step:6500/100000 val_loss:1.3246 train_time:213.68s step_avg:33ms
step:6600/100000 avg_train_loss:1.3420 train_time:217ms step_avg:33ms
step:6700/100000 avg_train_loss:1.3140 train_time:220ms step_avg:33ms
step:6800/100000 avg_train_loss:1.3375 train_time:224ms step_avg:33ms
step:6900/100000 avg_train_loss:1.3057 train_time:227ms step_avg:33ms
step:7000/100000 avg_train_loss:1.2836 train_time:230ms step_avg:33ms
step:7000/100000 val_loss:1.3067 train_time:230.19s step_avg:33ms
step:7100/100000 avg_train_loss:1.3009 train_time:233ms step_avg:33ms
step:7200/100000 avg_train_loss:1.3031 train_time:237ms step_avg:33ms
step:7300/100000 avg_train_loss:1.2969 train_time:240ms step_avg:33ms
step:7400/100000 avg_train_loss:1.3015 train_time:243ms step_avg:33ms
step:7500/100000 avg_train_loss:1.3038 train_time:247ms step_avg:33ms
step:7500/100000 val_loss:1.2895 train_time:246.63s step_avg:33ms
step:7600/100000 avg_train_loss:1.2986 train_time:250ms step_avg:33ms
step:7700/100000 avg_train_loss:1.3084 train_time:253ms step_avg:33ms
step:7800/100000 avg_train_loss:1.3065 train_time:256ms step_avg:33ms
step:7900/100000 avg_train_loss:1.2905 train_time:260ms step_avg:33ms
step:8000/100000 avg_train_loss:1.3342 train_time:263ms step_avg:33ms
step:8000/100000 val_loss:1.2777 train_time:263.08s step_avg:33ms
step:8100/100000 avg_train_loss:1.2722 train_time:266ms step_avg:33ms
step:8200/100000 avg_train_loss:1.2904 train_time:270ms step_avg:33ms
step:8300/100000 avg_train_loss:1.2551 train_time:273ms step_avg:33ms
step:8400/100000 avg_train_loss:1.2745 train_time:276ms step_avg:33ms
step:8500/100000 avg_train_loss:1.2744 train_time:280ms step_avg:33ms
step:8500/100000 val_loss:1.2625 train_time:279.53s step_avg:33ms
step:8600/100000 avg_train_loss:1.2469 train_time:283ms step_avg:33ms
step:8700/100000 avg_train_loss:1.2612 train_time:286ms step_avg:33ms
step:8800/100000 avg_train_loss:1.2528 train_time:289ms step_avg:33ms
step:8900/100000 avg_train_loss:1.2597 train_time:293ms step_avg:33ms
step:9000/100000 avg_train_loss:1.2454 train_time:296ms step_avg:33ms
step:9000/100000 val_loss:1.2509 train_time:296.00s step_avg:33ms
step:9100/100000 avg_train_loss:1.2818 train_time:299ms step_avg:33ms
step:9200/100000 avg_train_loss:1.2320 train_time:303ms step_avg:33ms
step:9300/100000 avg_train_loss:1.2255 train_time:306ms step_avg:33ms
step:9400/100000 avg_train_loss:1.2296 train_time:310ms step_avg:33ms
step:9500/100000 avg_train_loss:1.2134 train_time:314ms step_avg:33ms
step:9500/100000 val_loss:1.2372 train_time:313.70s step_avg:33ms
step:9600/100000 avg_train_loss:1.2484 train_time:317ms step_avg:33ms
step:9700/100000 avg_train_loss:1.2803 train_time:320ms step_avg:33ms
step:9800/100000 avg_train_loss:1.2243 train_time:324ms step_avg:33ms
step:9900/100000 avg_train_loss:1.2292 train_time:327ms step_avg:33ms
step:10000/100000 avg_train_loss:1.2046 train_time:330ms step_avg:33ms
step:10000/100000 val_loss:1.2287 train_time:330.20s step_avg:33ms
step:10100/100000 avg_train_loss:1.1906 train_time:333ms step_avg:33ms
step:10200/100000 avg_train_loss:1.1979 train_time:337ms step_avg:33ms
step:10300/100000 avg_train_loss:1.2026 train_time:340ms step_avg:33ms
step:10400/100000 avg_train_loss:1.2057 train_time:343ms step_avg:33ms
step:10500/100000 avg_train_loss:1.1934 train_time:347ms step_avg:33ms
step:10500/100000 val_loss:1.2187 train_time:346.63s step_avg:33ms
step:10600/100000 avg_train_loss:1.2539 train_time:350ms step_avg:33ms
step:10700/100000 avg_train_loss:1.1694 train_time:353ms step_avg:33ms
step:10800/100000 avg_train_loss:1.2372 train_time:356ms step_avg:33ms
step:10900/100000 avg_train_loss:1.2287 train_time:360ms step_avg:33ms
step:11000/100000 avg_train_loss:1.2123 train_time:363ms step_avg:33ms
step:11000/100000 val_loss:1.2104 train_time:363.08s step_avg:33ms
step:11100/100000 avg_train_loss:1.2080 train_time:366ms step_avg:33ms
step:11200/100000 avg_train_loss:1.2296 train_time:370ms step_avg:33ms
step:11300/100000 avg_train_loss:1.2119 train_time:373ms step_avg:33ms
step:11400/100000 avg_train_loss:1.2129 train_time:376ms step_avg:33ms
step:11500/100000 avg_train_loss:1.1720 train_time:380ms step_avg:33ms
step:11500/100000 val_loss:1.2017 train_time:379.52s step_avg:33ms
step:11600/100000 avg_train_loss:1.2231 train_time:383ms step_avg:33ms
step:11700/100000 avg_train_loss:1.1775 train_time:386ms step_avg:33ms
step:11800/100000 avg_train_loss:1.1801 train_time:389ms step_avg:33ms
step:11900/100000 avg_train_loss:1.1831 train_time:393ms step_avg:33ms
step:12000/100000 avg_train_loss:1.1881 train_time:396ms step_avg:33ms
step:12000/100000 val_loss:1.1974 train_time:396.01s step_avg:33ms
step:12100/100000 avg_train_loss:1.1692 train_time:399ms step_avg:33ms
step:12200/100000 avg_train_loss:1.1929 train_time:403ms step_avg:33ms
step:12300/100000 avg_train_loss:1.2056 train_time:406ms step_avg:33ms
step:12400/100000 avg_train_loss:1.1472 train_time:409ms step_avg:33ms
step:12500/100000 avg_train_loss:1.1623 train_time:412ms step_avg:33ms
step:12500/100000 val_loss:1.1881 train_time:412.46s step_avg:33ms
step:12600/100000 avg_train_loss:1.1854 train_time:416ms step_avg:33ms
step:12700/100000 avg_train_loss:1.1633 train_time:419ms step_avg:33ms
step:12800/100000 avg_train_loss:1.1702 train_time:422ms step_avg:33ms
step:12900/100000 avg_train_loss:1.1697 train_time:426ms step_avg:33ms
step:13000/100000 avg_train_loss:1.2211 train_time:429ms step_avg:33ms
step:13000/100000 val_loss:1.1811 train_time:428.90s step_avg:33ms
step:13100/100000 avg_train_loss:1.2024 train_time:432ms step_avg:33ms
step:13200/100000 avg_train_loss:1.2219 train_time:435ms step_avg:33ms
step:13300/100000 avg_train_loss:1.1495 train_time:439ms step_avg:33ms
step:13400/100000 avg_train_loss:1.1624 train_time:442ms step_avg:33ms
step:13500/100000 avg_train_loss:1.1630 train_time:445ms step_avg:33ms
step:13500/100000 val_loss:1.1753 train_time:445.36s step_avg:33ms
step:13600/100000 avg_train_loss:1.1681 train_time:449ms step_avg:33ms
step:13700/100000 avg_train_loss:1.1586 train_time:452ms step_avg:33ms
step:13800/100000 avg_train_loss:1.1701 train_time:455ms step_avg:33ms
step:13900/100000 avg_train_loss:1.1932 train_time:459ms step_avg:33ms
step:14000/100000 avg_train_loss:1.1543 train_time:462ms step_avg:33ms
step:14000/100000 val_loss:1.1691 train_time:461.79s step_avg:33ms
step:14100/100000 avg_train_loss:1.1705 train_time:465ms step_avg:33ms
step:14200/100000 avg_train_loss:1.1663 train_time:468ms step_avg:33ms
step:14300/100000 avg_train_loss:1.1863 train_time:472ms step_avg:33ms
step:14400/100000 avg_train_loss:1.1587 train_time:475ms step_avg:33ms
step:14500/100000 avg_train_loss:1.1651 train_time:478ms step_avg:33ms
step:14500/100000 val_loss:1.1640 train_time:478.30s step_avg:33ms
step:14600/100000 avg_train_loss:1.1531 train_time:482ms step_avg:33ms
step:14700/100000 avg_train_loss:1.1925 train_time:485ms step_avg:33ms
step:14800/100000 avg_train_loss:1.1570 train_time:488ms step_avg:33ms
step:14900/100000 avg_train_loss:1.1270 train_time:491ms step_avg:33ms
step:15000/100000 avg_train_loss:1.1334 train_time:495ms step_avg:33ms
step:15000/100000 val_loss:1.1599 train_time:494.74s step_avg:33ms
step:15100/100000 avg_train_loss:1.1190 train_time:498ms step_avg:33ms
step:15200/100000 avg_train_loss:1.1559 train_time:501ms step_avg:33ms
step:15300/100000 avg_train_loss:1.1479 train_time:505ms step_avg:33ms
step:15400/100000 avg_train_loss:1.1440 train_time:508ms step_avg:33ms
step:15500/100000 avg_train_loss:1.1381 train_time:511ms step_avg:33ms
step:15500/100000 val_loss:1.1553 train_time:511.21s step_avg:33ms
step:15600/100000 avg_train_loss:1.1514 train_time:514ms step_avg:33ms
step:15700/100000 avg_train_loss:1.1438 train_time:518ms step_avg:33ms
step:15800/100000 avg_train_loss:1.1446 train_time:521ms step_avg:33ms
step:15900/100000 avg_train_loss:1.1687 train_time:524ms step_avg:33ms
step:16000/100000 avg_train_loss:1.1436 train_time:528ms step_avg:33ms
step:16000/100000 val_loss:1.1507 train_time:527.67s step_avg:33ms
step:16100/100000 avg_train_loss:1.1580 train_time:531ms step_avg:33ms
step:16200/100000 avg_train_loss:1.1487 train_time:534ms step_avg:33ms
step:16300/100000 avg_train_loss:1.1424 train_time:538ms step_avg:33ms
step:16400/100000 avg_train_loss:1.1194 train_time:541ms step_avg:33ms
step:16500/100000 avg_train_loss:1.1326 train_time:544ms step_avg:33ms
step:16500/100000 val_loss:1.1467 train_time:544.14s step_avg:33ms
step:16600/100000 avg_train_loss:1.1408 train_time:547ms step_avg:33ms
step:16700/100000 avg_train_loss:1.1353 train_time:551ms step_avg:33ms
step:16800/100000 avg_train_loss:1.1577 train_time:554ms step_avg:33ms
step:16900/100000 avg_train_loss:1.1477 train_time:557ms step_avg:33ms
step:17000/100000 avg_train_loss:1.1640 train_time:561ms step_avg:33ms
step:17000/100000 val_loss:1.1436 train_time:560.58s step_avg:33ms
step:17100/100000 avg_train_loss:1.1668 train_time:564ms step_avg:33ms
step:17200/100000 avg_train_loss:1.1467 train_time:567ms step_avg:33ms
step:17300/100000 avg_train_loss:1.1666 train_time:570ms step_avg:33ms
step:17400/100000 avg_train_loss:1.1733 train_time:574ms step_avg:33ms
step:17500/100000 avg_train_loss:1.1369 train_time:577ms step_avg:33ms
step:17500/100000 val_loss:1.1402 train_time:577.02s step_avg:33ms
step:17600/100000 avg_train_loss:1.1111 train_time:580ms step_avg:33ms
step:17700/100000 avg_train_loss:1.1613 train_time:584ms step_avg:33ms
step:17800/100000 avg_train_loss:1.1404 train_time:587ms step_avg:33ms
step:17900/100000 avg_train_loss:1.1160 train_time:590ms step_avg:33ms
step:18000/100000 avg_train_loss:1.1268 train_time:593ms step_avg:33ms
step:18000/100000 val_loss:1.1373 train_time:593.46s step_avg:33ms
step:18100/100000 avg_train_loss:1.1395 train_time:597ms step_avg:33ms
step:18200/100000 avg_train_loss:1.1133 train_time:600ms step_avg:33ms
step:18300/100000 avg_train_loss:1.1222 train_time:603ms step_avg:33ms
step:18400/100000 avg_train_loss:1.1482 train_time:607ms step_avg:33ms
step:18500/100000 avg_train_loss:1.1427 train_time:610ms step_avg:33ms
step:18500/100000 val_loss:1.1358 train_time:609.91s step_avg:33ms
step:18600/100000 avg_train_loss:1.0987 train_time:613ms step_avg:33ms
step:18700/100000 avg_train_loss:1.1202 train_time:618ms step_avg:33ms
step:18800/100000 avg_train_loss:1.0883 train_time:621ms step_avg:33ms
step:18900/100000 avg_train_loss:1.1371 train_time:624ms step_avg:33ms
step:19000/100000 avg_train_loss:1.1538 train_time:628ms step_avg:33ms
step:19000/100000 val_loss:1.1305 train_time:627.58s step_avg:33ms
step:19100/100000 avg_train_loss:1.1482 train_time:631ms step_avg:33ms
step:19200/100000 avg_train_loss:1.0910 train_time:634ms step_avg:33ms
step:19300/100000 avg_train_loss:1.1164 train_time:637ms step_avg:33ms
step:19400/100000 avg_train_loss:1.0812 train_time:641ms step_avg:33ms
step:19500/100000 avg_train_loss:1.1021 train_time:644ms step_avg:33ms
step:19500/100000 val_loss:1.1262 train_time:644.02s step_avg:33ms
step:19600/100000 avg_train_loss:1.0754 train_time:647ms step_avg:33ms
step:19700/100000 avg_train_loss:1.1035 train_time:651ms step_avg:33ms
step:19800/100000 avg_train_loss:1.0996 train_time:654ms step_avg:33ms
step:19900/100000 avg_train_loss:1.1285 train_time:657ms step_avg:33ms
step:20000/100000 avg_train_loss:1.0999 train_time:660ms step_avg:33ms
step:20000/100000 val_loss:1.1239 train_time:660.45s step_avg:33ms
step:20100/100000 avg_train_loss:1.1204 train_time:664ms step_avg:33ms
step:20200/100000 avg_train_loss:1.1391 train_time:667ms step_avg:33ms
step:20300/100000 avg_train_loss:1.1150 train_time:670ms step_avg:33ms
step:20400/100000 avg_train_loss:1.1103 train_time:674ms step_avg:33ms
step:20500/100000 avg_train_loss:1.1270 train_time:677ms step_avg:33ms
step:20500/100000 val_loss:1.1201 train_time:676.91s step_avg:33ms
step:20600/100000 avg_train_loss:1.1240 train_time:680ms step_avg:33ms
step:20700/100000 avg_train_loss:1.1174 train_time:683ms step_avg:33ms
step:20800/100000 avg_train_loss:1.0939 train_time:687ms step_avg:33ms
step:20900/100000 avg_train_loss:1.1275 train_time:690ms step_avg:33ms
step:21000/100000 avg_train_loss:1.0944 train_time:693ms step_avg:33ms
step:21000/100000 val_loss:1.1189 train_time:693.35s step_avg:33ms
step:21100/100000 avg_train_loss:1.0958 train_time:697ms step_avg:33ms
step:21200/100000 avg_train_loss:1.0787 train_time:700ms step_avg:33ms
step:21300/100000 avg_train_loss:1.1084 train_time:703ms step_avg:33ms
step:21400/100000 avg_train_loss:1.0958 train_time:707ms step_avg:33ms
step:21500/100000 avg_train_loss:1.1079 train_time:710ms step_avg:33ms
step:21500/100000 val_loss:1.1161 train_time:709.80s step_avg:33ms
step:21600/100000 avg_train_loss:1.1151 train_time:713ms step_avg:33ms
step:21700/100000 avg_train_loss:1.0816 train_time:716ms step_avg:33ms
step:21800/100000 avg_train_loss:1.0702 train_time:720ms step_avg:33ms
step:21900/100000 avg_train_loss:1.0969 train_time:723ms step_avg:33ms
step:22000/100000 avg_train_loss:1.0894 train_time:726ms step_avg:33ms
step:22000/100000 val_loss:1.1118 train_time:726.30s step_avg:33ms
step:22100/100000 avg_train_loss:1.1016 train_time:730ms step_avg:33ms
step:22200/100000 avg_train_loss:1.0763 train_time:733ms step_avg:33ms
step:22300/100000 avg_train_loss:1.1302 train_time:736ms step_avg:33ms
step:22400/100000 avg_train_loss:1.1304 train_time:739ms step_avg:33ms
step:22500/100000 avg_train_loss:1.1514 train_time:743ms step_avg:33ms
step:22500/100000 val_loss:1.1107 train_time:742.75s step_avg:33ms
step:22600/100000 avg_train_loss:1.1009 train_time:746ms step_avg:33ms
step:22700/100000 avg_train_loss:1.0641 train_time:749ms step_avg:33ms
step:22800/100000 avg_train_loss:1.0801 train_time:753ms step_avg:33ms
step:22900/100000 avg_train_loss:1.1069 train_time:756ms step_avg:33ms
step:23000/100000 avg_train_loss:1.0868 train_time:759ms step_avg:33ms
step:23000/100000 val_loss:1.1069 train_time:759.17s step_avg:33ms
step:23100/100000 avg_train_loss:1.0898 train_time:762ms step_avg:33ms
step:23200/100000 avg_train_loss:1.1247 train_time:766ms step_avg:33ms
step:23300/100000 avg_train_loss:1.0918 train_time:769ms step_avg:33ms
step:23400/100000 avg_train_loss:1.0942 train_time:772ms step_avg:33ms
step:23500/100000 avg_train_loss:1.0860 train_time:776ms step_avg:33ms
step:23500/100000 val_loss:1.1043 train_time:775.62s step_avg:33ms
step:23600/100000 avg_train_loss:1.1422 train_time:779ms step_avg:33ms
step:23700/100000 avg_train_loss:1.0911 train_time:782ms step_avg:33ms
step:23800/100000 avg_train_loss:1.1021 train_time:785ms step_avg:33ms
step:23900/100000 avg_train_loss:1.0810 train_time:789ms step_avg:33ms
step:24000/100000 avg_train_loss:1.1062 train_time:792ms step_avg:33ms
step:24000/100000 val_loss:1.1040 train_time:792.06s step_avg:33ms
step:24100/100000 avg_train_loss:1.1010 train_time:795ms step_avg:33ms
step:24200/100000 avg_train_loss:1.0797 train_time:799ms step_avg:33ms
step:24300/100000 avg_train_loss:1.0686 train_time:802ms step_avg:33ms
step:24400/100000 avg_train_loss:1.0384 train_time:805ms step_avg:33ms
step:24500/100000 avg_train_loss:1.1006 train_time:808ms step_avg:33ms
step:24500/100000 val_loss:1.1022 train_time:808.49s step_avg:33ms
step:24600/100000 avg_train_loss:1.0860 train_time:812ms step_avg:33ms
step:24700/100000 avg_train_loss:1.0822 train_time:815ms step_avg:33ms
step:24800/100000 avg_train_loss:1.0749 train_time:818ms step_avg:33ms
step:24900/100000 avg_train_loss:1.0784 train_time:822ms step_avg:33ms
step:25000/100000 avg_train_loss:1.0896 train_time:825ms step_avg:33ms
step:25000/100000 val_loss:1.0997 train_time:824.92s step_avg:33ms
step:25100/100000 avg_train_loss:1.0851 train_time:828ms step_avg:33ms
step:25200/100000 avg_train_loss:1.0977 train_time:832ms step_avg:33ms
step:25300/100000 avg_train_loss:1.1004 train_time:835ms step_avg:33ms
step:25400/100000 avg_train_loss:1.0951 train_time:838ms step_avg:33ms
step:25500/100000 avg_train_loss:1.0779 train_time:841ms step_avg:33ms
step:25500/100000 val_loss:1.0972 train_time:841.37s step_avg:33ms
step:25600/100000 avg_train_loss:1.0894 train_time:845ms step_avg:33ms
step:25700/100000 avg_train_loss:1.0794 train_time:848ms step_avg:33ms
step:25800/100000 avg_train_loss:1.0625 train_time:851ms step_avg:33ms
step:25900/100000 avg_train_loss:1.0809 train_time:855ms step_avg:33ms
step:26000/100000 avg_train_loss:1.0833 train_time:858ms step_avg:33ms
step:26000/100000 val_loss:1.0968 train_time:857.82s step_avg:33ms
step:26100/100000 avg_train_loss:1.0894 train_time:861ms step_avg:33ms
step:26200/100000 avg_train_loss:1.1048 train_time:864ms step_avg:33ms
step:26300/100000 avg_train_loss:1.1108 train_time:868ms step_avg:33ms
step:26400/100000 avg_train_loss:1.0956 train_time:871ms step_avg:33ms
step:26500/100000 avg_train_loss:1.1043 train_time:874ms step_avg:33ms
step:26500/100000 val_loss:1.0961 train_time:874.24s step_avg:33ms
step:26600/100000 avg_train_loss:1.1057 train_time:878ms step_avg:33ms
step:26700/100000 avg_train_loss:1.1402 train_time:881ms step_avg:33ms
step:26800/100000 avg_train_loss:1.0624 train_time:884ms step_avg:33ms
step:26900/100000 avg_train_loss:1.0877 train_time:887ms step_avg:33ms
step:27000/100000 avg_train_loss:1.0813 train_time:891ms step_avg:33ms
step:27000/100000 val_loss:1.0933 train_time:890.77s step_avg:33ms
step:27100/100000 avg_train_loss:1.0929 train_time:894ms step_avg:33ms
step:27200/100000 avg_train_loss:1.0766 train_time:897ms step_avg:33ms
step:27300/100000 avg_train_loss:1.0716 train_time:901ms step_avg:33ms
step:27400/100000 avg_train_loss:1.0805 train_time:904ms step_avg:33ms
step:27500/100000 avg_train_loss:1.0760 train_time:907ms step_avg:33ms
step:27500/100000 val_loss:1.0920 train_time:907.22s step_avg:33ms
step:27600/100000 avg_train_loss:1.0723 train_time:911ms step_avg:33ms
step:27700/100000 avg_train_loss:1.0878 train_time:914ms step_avg:33ms
step:27800/100000 avg_train_loss:1.0875 train_time:917ms step_avg:33ms
step:27900/100000 avg_train_loss:1.0681 train_time:920ms step_avg:33ms
step:28000/100000 avg_train_loss:1.0718 train_time:925ms step_avg:33ms
step:28000/100000 val_loss:1.0899 train_time:924.95s step_avg:33ms
step:28100/100000 avg_train_loss:1.0506 train_time:928ms step_avg:33ms
step:28200/100000 avg_train_loss:1.0726 train_time:932ms step_avg:33ms
step:28300/100000 avg_train_loss:1.0976 train_time:935ms step_avg:33ms
step:28400/100000 avg_train_loss:1.0914 train_time:938ms step_avg:33ms
step:28500/100000 avg_train_loss:1.0644 train_time:941ms step_avg:33ms
step:28500/100000 val_loss:1.0886 train_time:941.44s step_avg:33ms
step:28600/100000 avg_train_loss:1.0794 train_time:945ms step_avg:33ms
step:28700/100000 avg_train_loss:1.0457 train_time:948ms step_avg:33ms
step:28800/100000 avg_train_loss:1.0351 train_time:951ms step_avg:33ms
step:28900/100000 avg_train_loss:1.0378 train_time:955ms step_avg:33ms
step:29000/100000 avg_train_loss:1.0580 train_time:958ms step_avg:33ms
step:29000/100000 val_loss:1.0858 train_time:957.93s step_avg:33ms
step:29100/100000 avg_train_loss:1.0475 train_time:961ms step_avg:33ms
step:29200/100000 avg_train_loss:1.0702 train_time:965ms step_avg:33ms
step:29300/100000 avg_train_loss:1.0763 train_time:968ms step_avg:33ms
step:29400/100000 avg_train_loss:1.0595 train_time:971ms step_avg:33ms
step:29500/100000 avg_train_loss:1.0926 train_time:974ms step_avg:33ms
step:29500/100000 val_loss:1.0848 train_time:974.38s step_avg:33ms
step:29600/100000 avg_train_loss:1.0561 train_time:978ms step_avg:33ms
step:29700/100000 avg_train_loss:1.0989 train_time:981ms step_avg:33ms
step:29800/100000 avg_train_loss:1.0769 train_time:984ms step_avg:33ms
step:29900/100000 avg_train_loss:1.0823 train_time:988ms step_avg:33ms
step:30000/100000 avg_train_loss:1.0697 train_time:991ms step_avg:33ms
step:30000/100000 val_loss:1.0832 train_time:990.84s step_avg:33ms
step:30100/100000 avg_train_loss:1.0584 train_time:994ms step_avg:33ms
step:30200/100000 avg_train_loss:1.0713 train_time:997ms step_avg:33ms
step:30300/100000 avg_train_loss:1.0814 train_time:1001ms step_avg:33ms
step:30400/100000 avg_train_loss:1.0338 train_time:1004ms step_avg:33ms
step:30500/100000 avg_train_loss:1.0492 train_time:1007ms step_avg:33ms
step:30500/100000 val_loss:1.0837 train_time:1007.28s step_avg:33ms
step:30600/100000 avg_train_loss:1.0804 train_time:1011ms step_avg:33ms
step:30700/100000 avg_train_loss:1.0478 train_time:1014ms step_avg:33ms
step:30800/100000 avg_train_loss:1.0574 train_time:1017ms step_avg:33ms
step:30900/100000 avg_train_loss:1.0625 train_time:1020ms step_avg:33ms
step:31000/100000 avg_train_loss:1.0617 train_time:1024ms step_avg:33ms
step:31000/100000 val_loss:1.0805 train_time:1023.72s step_avg:33ms
step:31100/100000 avg_train_loss:1.0278 train_time:1027ms step_avg:33ms
step:31200/100000 avg_train_loss:1.0524 train_time:1030ms step_avg:33ms
step:31300/100000 avg_train_loss:1.0640 train_time:1034ms step_avg:33ms
step:31400/100000 avg_train_loss:1.0489 train_time:1037ms step_avg:33ms
step:31500/100000 avg_train_loss:1.0438 train_time:1040ms step_avg:33ms
step:31500/100000 val_loss:1.0781 train_time:1040.24s step_avg:33ms
step:31600/100000 avg_train_loss:1.0778 train_time:1044ms step_avg:33ms
step:31700/100000 avg_train_loss:1.1073 train_time:1047ms step_avg:33ms
step:31800/100000 avg_train_loss:1.0901 train_time:1050ms step_avg:33ms
step:31900/100000 avg_train_loss:1.0900 train_time:1053ms step_avg:33ms
step:32000/100000 avg_train_loss:1.0288 train_time:1057ms step_avg:33ms
step:32000/100000 val_loss:1.0781 train_time:1056.66s step_avg:33ms
step:32100/100000 avg_train_loss:1.0323 train_time:1060ms step_avg:33ms
step:32200/100000 avg_train_loss:1.0860 train_time:1063ms step_avg:33ms
step:32300/100000 avg_train_loss:1.0323 train_time:1067ms step_avg:33ms
step:32400/100000 avg_train_loss:1.0481 train_time:1070ms step_avg:33ms
step:32500/100000 avg_train_loss:1.0775 train_time:1073ms step_avg:33ms
step:32500/100000 val_loss:1.0756 train_time:1073.11s step_avg:33ms
step:32600/100000 avg_train_loss:1.0723 train_time:1076ms step_avg:33ms
step:32700/100000 avg_train_loss:1.0707 train_time:1080ms step_avg:33ms
step:32800/100000 avg_train_loss:1.0504 train_time:1083ms step_avg:33ms
step:32900/100000 avg_train_loss:1.0945 train_time:1086ms step_avg:33ms
step:33000/100000 avg_train_loss:1.0472 train_time:1090ms step_avg:33ms
step:33000/100000 val_loss:1.0741 train_time:1089.59s step_avg:33ms
step:33100/100000 avg_train_loss:1.0661 train_time:1093ms step_avg:33ms
step:33200/100000 avg_train_loss:1.0579 train_time:1096ms step_avg:33ms
step:33300/100000 avg_train_loss:1.0450 train_time:1099ms step_avg:33ms
step:33400/100000 avg_train_loss:1.0818 train_time:1103ms step_avg:33ms
step:33500/100000 avg_train_loss:1.0614 train_time:1106ms step_avg:33ms
step:33500/100000 val_loss:1.0743 train_time:1106.04s step_avg:33ms
step:33600/100000 avg_train_loss:1.0430 train_time:1109ms step_avg:33ms
step:33700/100000 avg_train_loss:0.9920 train_time:1113ms step_avg:33ms
step:33800/100000 avg_train_loss:1.0665 train_time:1116ms step_avg:33ms
step:33900/100000 avg_train_loss:1.0688 train_time:1119ms step_avg:33ms
step:34000/100000 avg_train_loss:1.0370 train_time:1122ms step_avg:33ms
step:34000/100000 val_loss:1.0731 train_time:1122.47s step_avg:33ms
step:34100/100000 avg_train_loss:1.0558 train_time:1126ms step_avg:33ms
step:34200/100000 avg_train_loss:1.0227 train_time:1129ms step_avg:33ms
step:34300/100000 avg_train_loss:1.0613 train_time:1132ms step_avg:33ms
step:34400/100000 avg_train_loss:1.0651 train_time:1136ms step_avg:33ms
step:34500/100000 avg_train_loss:1.0461 train_time:1139ms step_avg:33ms
step:34500/100000 val_loss:1.0712 train_time:1138.95s step_avg:33ms
step:34600/100000 avg_train_loss:1.0674 train_time:1142ms step_avg:33ms
step:34700/100000 avg_train_loss:1.0613 train_time:1146ms step_avg:33ms
step:34800/100000 avg_train_loss:1.0496 train_time:1149ms step_avg:33ms
step:34900/100000 avg_train_loss:1.0516 train_time:1152ms step_avg:33ms
step:35000/100000 avg_train_loss:1.0417 train_time:1155ms step_avg:33ms
step:35000/100000 val_loss:1.0714 train_time:1155.43s step_avg:33ms
step:35100/100000 avg_train_loss:1.0526 train_time:1159ms step_avg:33ms
step:35200/100000 avg_train_loss:1.0377 train_time:1162ms step_avg:33ms
step:35300/100000 avg_train_loss:1.0458 train_time:1165ms step_avg:33ms
step:35400/100000 avg_train_loss:1.0701 train_time:1169ms step_avg:33ms
step:35500/100000 avg_train_loss:1.0704 train_time:1172ms step_avg:33ms
step:35500/100000 val_loss:1.0703 train_time:1171.87s step_avg:33ms
step:35600/100000 avg_train_loss:1.0676 train_time:1175ms step_avg:33ms
step:35700/100000 avg_train_loss:1.0804 train_time:1178ms step_avg:33ms
step:35800/100000 avg_train_loss:1.0641 train_time:1182ms step_avg:33ms
step:35900/100000 avg_train_loss:1.0733 train_time:1185ms step_avg:33ms
step:36000/100000 avg_train_loss:1.1130 train_time:1188ms step_avg:33ms
step:36000/100000 val_loss:1.0708 train_time:1188.32s step_avg:33ms
step:36100/100000 avg_train_loss:1.0389 train_time:1192ms step_avg:33ms
step:36200/100000 avg_train_loss:1.0650 train_time:1195ms step_avg:33ms
step:36300/100000 avg_train_loss:1.0453 train_time:1198ms step_avg:33ms
step:36400/100000 avg_train_loss:1.0629 train_time:1202ms step_avg:33ms
step:36500/100000 avg_train_loss:1.0598 train_time:1205ms step_avg:33ms
step:36500/100000 val_loss:1.0682 train_time:1204.79s step_avg:33ms
step:36600/100000 avg_train_loss:1.0296 train_time:1208ms step_avg:33ms
step:36700/100000 avg_train_loss:1.0483 train_time:1211ms step_avg:33ms
step:36800/100000 avg_train_loss:1.0516 train_time:1215ms step_avg:33ms
step:36900/100000 avg_train_loss:1.0464 train_time:1218ms step_avg:33ms
step:37000/100000 avg_train_loss:1.0466 train_time:1221ms step_avg:33ms
step:37000/100000 val_loss:1.0676 train_time:1221.23s step_avg:33ms
step:37100/100000 avg_train_loss:1.0642 train_time:1225ms step_avg:33ms
step:37200/100000 avg_train_loss:1.0379 train_time:1228ms step_avg:33ms
step:37300/100000 avg_train_loss:1.0372 train_time:1231ms step_avg:33ms
step:37400/100000 avg_train_loss:1.0259 train_time:1236ms step_avg:33ms
step:37500/100000 avg_train_loss:1.0356 train_time:1239ms step_avg:33ms
step:37500/100000 val_loss:1.0659 train_time:1238.92s step_avg:33ms
step:37600/100000 avg_train_loss:1.0804 train_time:1242ms step_avg:33ms
step:37700/100000 avg_train_loss:1.0868 train_time:1245ms step_avg:33ms
step:37800/100000 avg_train_loss:1.0080 train_time:1249ms step_avg:33ms
step:37900/100000 avg_train_loss:1.0506 train_time:1252ms step_avg:33ms
step:38000/100000 avg_train_loss:1.0292 train_time:1255ms step_avg:33ms
step:38000/100000 val_loss:1.0662 train_time:1255.34s step_avg:33ms
step:38100/100000 avg_train_loss:1.0033 train_time:1259ms step_avg:33ms
step:38200/100000 avg_train_loss:1.0218 train_time:1262ms step_avg:33ms
step:38300/100000 avg_train_loss:1.0267 train_time:1265ms step_avg:33ms
step:38400/100000 avg_train_loss:1.0283 train_time:1268ms step_avg:33ms
step:38500/100000 avg_train_loss:1.0358 train_time:1272ms step_avg:33ms
step:38500/100000 val_loss:1.0645 train_time:1271.76s step_avg:33ms
step:38600/100000 avg_train_loss:1.0527 train_time:1275ms step_avg:33ms
step:38700/100000 avg_train_loss:1.0011 train_time:1278ms step_avg:33ms
step:38800/100000 avg_train_loss:1.0901 train_time:1282ms step_avg:33ms
step:38900/100000 avg_train_loss:1.0359 train_time:1285ms step_avg:33ms
step:39000/100000 avg_train_loss:1.0663 train_time:1288ms step_avg:33ms
step:39000/100000 val_loss:1.0635 train_time:1288.23s step_avg:33ms
step:39100/100000 avg_train_loss:1.0450 train_time:1292ms step_avg:33ms
step:39200/100000 avg_train_loss:1.0548 train_time:1295ms step_avg:33ms
step:39300/100000 avg_train_loss:1.0441 train_time:1298ms step_avg:33ms
step:39400/100000 avg_train_loss:1.0506 train_time:1301ms step_avg:33ms
step:39500/100000 avg_train_loss:1.0356 train_time:1305ms step_avg:33ms
step:39500/100000 val_loss:1.0627 train_time:1304.69s step_avg:33ms
step:39600/100000 avg_train_loss:1.0445 train_time:1308ms step_avg:33ms
step:39700/100000 avg_train_loss:1.0216 train_time:1311ms step_avg:33ms
step:39800/100000 avg_train_loss:1.0465 train_time:1315ms step_avg:33ms
step:39900/100000 avg_train_loss:1.0173 train_time:1318ms step_avg:33ms
step:40000/100000 avg_train_loss:1.0354 train_time:1321ms step_avg:33ms
step:40000/100000 val_loss:1.0620 train_time:1321.15s step_avg:33ms
step:40100/100000 avg_train_loss:1.0236 train_time:1324ms step_avg:33ms
step:40200/100000 avg_train_loss:1.0275 train_time:1328ms step_avg:33ms
step:40300/100000 avg_train_loss:1.0676 train_time:1331ms step_avg:33ms
step:40400/100000 avg_train_loss:0.9902 train_time:1334ms step_avg:33ms
step:40500/100000 avg_train_loss:1.0278 train_time:1338ms step_avg:33ms
step:40500/100000 val_loss:1.0604 train_time:1337.60s step_avg:33ms
step:40600/100000 avg_train_loss:1.0369 train_time:1341ms step_avg:33ms
step:40700/100000 avg_train_loss:1.0259 train_time:1344ms step_avg:33ms
step:40800/100000 avg_train_loss:1.0191 train_time:1347ms step_avg:33ms
step:40900/100000 avg_train_loss:1.0312 train_time:1351ms step_avg:33ms
step:41000/100000 avg_train_loss:1.0871 train_time:1354ms step_avg:33ms
step:41000/100000 val_loss:1.0588 train_time:1354.03s step_avg:33ms
step:41100/100000 avg_train_loss:1.0870 train_time:1357ms step_avg:33ms
step:41200/100000 avg_train_loss:1.0645 train_time:1361ms step_avg:33ms
step:41300/100000 avg_train_loss:0.9952 train_time:1364ms step_avg:33ms
step:41400/100000 avg_train_loss:1.0104 train_time:1367ms step_avg:33ms
step:41500/100000 avg_train_loss:1.0594 train_time:1370ms step_avg:33ms
step:41500/100000 val_loss:1.0580 train_time:1370.47s step_avg:33ms
step:41600/100000 avg_train_loss:1.0262 train_time:1374ms step_avg:33ms
step:41700/100000 avg_train_loss:1.0056 train_time:1377ms step_avg:33ms
step:41800/100000 avg_train_loss:1.0642 train_time:1380ms step_avg:33ms
step:41900/100000 avg_train_loss:1.0523 train_time:1384ms step_avg:33ms
step:42000/100000 avg_train_loss:1.0318 train_time:1387ms step_avg:33ms
step:42000/100000 val_loss:1.0575 train_time:1386.94s step_avg:33ms
step:42100/100000 avg_train_loss:1.0369 train_time:1390ms step_avg:33ms
step:42200/100000 avg_train_loss:1.0544 train_time:1394ms step_avg:33ms
step:42300/100000 avg_train_loss:1.0464 train_time:1397ms step_avg:33ms
step:42400/100000 avg_train_loss:1.0270 train_time:1400ms step_avg:33ms
step:42500/100000 avg_train_loss:1.0555 train_time:1403ms step_avg:33ms
step:42500/100000 val_loss:1.0571 train_time:1403.40s step_avg:33ms
step:42600/100000 avg_train_loss:1.0225 train_time:1407ms step_avg:33ms
step:42700/100000 avg_train_loss:1.0649 train_time:1410ms step_avg:33ms
step:42800/100000 avg_train_loss:1.0233 train_time:1413ms step_avg:33ms
step:42900/100000 avg_train_loss:1.0096 train_time:1417ms step_avg:33ms
step:43000/100000 avg_train_loss:0.9971 train_time:1420ms step_avg:33ms
step:43000/100000 val_loss:1.0565 train_time:1419.88s step_avg:33ms
step:43100/100000 avg_train_loss:1.0249 train_time:1423ms step_avg:33ms
step:43200/100000 avg_train_loss:1.0416 train_time:1426ms step_avg:33ms
step:43300/100000 avg_train_loss:1.0250 train_time:1430ms step_avg:33ms
step:43400/100000 avg_train_loss:1.0208 train_time:1433ms step_avg:33ms
step:43500/100000 avg_train_loss:1.0183 train_time:1436ms step_avg:33ms
step:43500/100000 val_loss:1.0555 train_time:1436.31s step_avg:33ms
step:43600/100000 avg_train_loss:1.0256 train_time:1440ms step_avg:33ms
step:43700/100000 avg_train_loss:1.0499 train_time:1443ms step_avg:33ms
step:43800/100000 avg_train_loss:1.0277 train_time:1446ms step_avg:33ms
step:43900/100000 avg_train_loss:1.0511 train_time:1449ms step_avg:33ms
step:44000/100000 avg_train_loss:1.0231 train_time:1453ms step_avg:33ms
step:44000/100000 val_loss:1.0548 train_time:1452.77s step_avg:33ms
step:44100/100000 avg_train_loss:1.0453 train_time:1456ms step_avg:33ms
step:44200/100000 avg_train_loss:1.0292 train_time:1459ms step_avg:33ms
step:44300/100000 avg_train_loss:1.0167 train_time:1463ms step_avg:33ms
step:44400/100000 avg_train_loss:1.0350 train_time:1466ms step_avg:33ms
step:44500/100000 avg_train_loss:1.0261 train_time:1469ms step_avg:33ms
step:44500/100000 val_loss:1.0541 train_time:1469.20s step_avg:33ms
step:44600/100000 avg_train_loss:1.0221 train_time:1472ms step_avg:33ms
step:44700/100000 avg_train_loss:1.0406 train_time:1476ms step_avg:33ms
step:44800/100000 avg_train_loss:1.0462 train_time:1479ms step_avg:33ms
step:44900/100000 avg_train_loss:1.0358 train_time:1482ms step_avg:33ms
step:45000/100000 avg_train_loss:1.0565 train_time:1486ms step_avg:33ms
step:45000/100000 val_loss:1.0539 train_time:1485.62s step_avg:33ms
step:45100/100000 avg_train_loss:1.0641 train_time:1489ms step_avg:33ms
step:45200/100000 avg_train_loss:1.0399 train_time:1492ms step_avg:33ms
step:45300/100000 avg_train_loss:1.0722 train_time:1496ms step_avg:33ms
step:45400/100000 avg_train_loss:1.0418 train_time:1499ms step_avg:33ms
step:45500/100000 avg_train_loss:1.0515 train_time:1502ms step_avg:33ms
step:45500/100000 val_loss:1.0528 train_time:1502.11s step_avg:33ms
step:45600/100000 avg_train_loss:1.0145 train_time:1505ms step_avg:33ms
step:45700/100000 avg_train_loss:1.0530 train_time:1509ms step_avg:33ms
step:45800/100000 avg_train_loss:1.0376 train_time:1512ms step_avg:33ms
step:45900/100000 avg_train_loss:1.0199 train_time:1515ms step_avg:33ms
step:46000/100000 avg_train_loss:1.0247 train_time:1519ms step_avg:33ms
step:46000/100000 val_loss:1.0522 train_time:1518.56s step_avg:33ms
step:46100/100000 avg_train_loss:1.0287 train_time:1522ms step_avg:33ms
step:46200/100000 avg_train_loss:1.0352 train_time:1525ms step_avg:33ms
step:46300/100000 avg_train_loss:1.0172 train_time:1528ms step_avg:33ms
step:46400/100000 avg_train_loss:1.0634 train_time:1532ms step_avg:33ms
step:46500/100000 avg_train_loss:1.0123 train_time:1535ms step_avg:33ms
step:46500/100000 val_loss:1.0523 train_time:1535.01s step_avg:33ms
step:46600/100000 avg_train_loss:1.0094 train_time:1538ms step_avg:33ms
step:46700/100000 avg_train_loss:1.0090 train_time:1543ms step_avg:33ms
step:46800/100000 avg_train_loss:1.0087 train_time:1546ms step_avg:33ms
step:46900/100000 avg_train_loss:1.0389 train_time:1549ms step_avg:33ms
step:47000/100000 avg_train_loss:1.0696 train_time:1553ms step_avg:33ms
step:47000/100000 val_loss:1.0518 train_time:1552.69s step_avg:33ms
step:47100/100000 avg_train_loss:1.0315 train_time:1556ms step_avg:33ms
step:47200/100000 avg_train_loss:1.0203 train_time:1559ms step_avg:33ms
step:47300/100000 avg_train_loss:1.0040 train_time:1563ms step_avg:33ms
step:47400/100000 avg_train_loss:0.9866 train_time:1566ms step_avg:33ms
step:47500/100000 avg_train_loss:1.0019 train_time:1569ms step_avg:33ms
step:47500/100000 val_loss:1.0499 train_time:1569.11s step_avg:33ms
step:47600/100000 avg_train_loss:0.9961 train_time:1572ms step_avg:33ms
step:47700/100000 avg_train_loss:1.0104 train_time:1576ms step_avg:33ms
step:47800/100000 avg_train_loss:1.0093 train_time:1579ms step_avg:33ms
step:47900/100000 avg_train_loss:1.0583 train_time:1582ms step_avg:33ms
step:48000/100000 avg_train_loss:0.9819 train_time:1586ms step_avg:33ms
step:48000/100000 val_loss:1.0494 train_time:1585.61s step_avg:33ms
step:48100/100000 avg_train_loss:1.0467 train_time:1589ms step_avg:33ms
step:48200/100000 avg_train_loss:1.0347 train_time:1592ms step_avg:33ms
step:48300/100000 avg_train_loss:1.0368 train_time:1596ms step_avg:33ms
step:48400/100000 avg_train_loss:1.0250 train_time:1599ms step_avg:33ms
step:48500/100000 avg_train_loss:1.0443 train_time:1602ms step_avg:33ms
step:48500/100000 val_loss:1.0486 train_time:1602.12s step_avg:33ms
step:48600/100000 avg_train_loss:1.0368 train_time:1605ms step_avg:33ms
step:48700/100000 avg_train_loss:1.0275 train_time:1609ms step_avg:33ms
step:48800/100000 avg_train_loss:0.9957 train_time:1612ms step_avg:33ms
step:48900/100000 avg_train_loss:1.0531 train_time:1615ms step_avg:33ms
step:49000/100000 avg_train_loss:1.0049 train_time:1619ms step_avg:33ms
step:49000/100000 val_loss:1.0486 train_time:1618.56s step_avg:33ms
step:49100/100000 avg_train_loss:1.0076 train_time:1622ms step_avg:33ms
step:49200/100000 avg_train_loss:1.0024 train_time:1625ms step_avg:33ms
step:49300/100000 avg_train_loss:1.0412 train_time:1628ms step_avg:33ms
step:49400/100000 avg_train_loss:0.9951 train_time:1632ms step_avg:33ms
step:49500/100000 avg_train_loss:1.0194 train_time:1635ms step_avg:33ms
step:49500/100000 val_loss:1.0484 train_time:1635.07s step_avg:33ms
step:49600/100000 avg_train_loss:1.0407 train_time:1638ms step_avg:33ms
step:49700/100000 avg_train_loss:0.9831 train_time:1642ms step_avg:33ms
step:49800/100000 avg_train_loss:1.0059 train_time:1645ms step_avg:33ms
step:49900/100000 avg_train_loss:1.0013 train_time:1648ms step_avg:33ms
step:50000/100000 avg_train_loss:1.0070 train_time:1651ms step_avg:33ms
step:50000/100000 val_loss:1.0466 train_time:1651.50s step_avg:33ms
step:50100/100000 avg_train_loss:1.0242 train_time:1655ms step_avg:33ms
step:50200/100000 avg_train_loss:1.0035 train_time:1658ms step_avg:33ms
step:50300/100000 avg_train_loss:1.0630 train_time:1661ms step_avg:33ms
step:50400/100000 avg_train_loss:1.0415 train_time:1665ms step_avg:33ms
step:50500/100000 avg_train_loss:1.0786 train_time:1668ms step_avg:33ms
step:50500/100000 val_loss:1.0459 train_time:1667.93s step_avg:33ms
step:50600/100000 avg_train_loss:0.9937 train_time:1671ms step_avg:33ms
step:50700/100000 avg_train_loss:1.0134 train_time:1674ms step_avg:33ms
step:50800/100000 avg_train_loss:1.0080 train_time:1678ms step_avg:33ms
step:50900/100000 avg_train_loss:1.0184 train_time:1681ms step_avg:33ms
step:51000/100000 avg_train_loss:1.0110 train_time:1684ms step_avg:33ms
step:51000/100000 val_loss:1.0455 train_time:1684.36s step_avg:33ms
step:51100/100000 avg_train_loss:1.0088 train_time:1688ms step_avg:33ms
step:51200/100000 avg_train_loss:1.0570 train_time:1691ms step_avg:33ms
step:51300/100000 avg_train_loss:1.0076 train_time:1694ms step_avg:33ms
step:51400/100000 avg_train_loss:1.0222 train_time:1698ms step_avg:33ms
step:51500/100000 avg_train_loss:1.0160 train_time:1701ms step_avg:33ms
step:51500/100000 val_loss:1.0445 train_time:1700.80s step_avg:33ms
step:51600/100000 avg_train_loss:1.0616 train_time:1704ms step_avg:33ms
step:51700/100000 avg_train_loss:1.0065 train_time:1707ms step_avg:33ms
step:51800/100000 avg_train_loss:1.0343 train_time:1711ms step_avg:33ms
step:51900/100000 avg_train_loss:1.0019 train_time:1714ms step_avg:33ms
step:52000/100000 avg_train_loss:1.0557 train_time:1717ms step_avg:33ms
step:52000/100000 val_loss:1.0452 train_time:1717.25s step_avg:33ms
step:52100/100000 avg_train_loss:1.0191 train_time:1721ms step_avg:33ms
step:52200/100000 avg_train_loss:0.9805 train_time:1724ms step_avg:33ms
step:52300/100000 avg_train_loss:0.9996 train_time:1727ms step_avg:33ms
step:52400/100000 avg_train_loss:0.9673 train_time:1730ms step_avg:33ms
step:52500/100000 avg_train_loss:1.0401 train_time:1734ms step_avg:33ms
step:52500/100000 val_loss:1.0443 train_time:1733.68s step_avg:33ms
step:52600/100000 avg_train_loss:1.0034 train_time:1737ms step_avg:33ms
step:52700/100000 avg_train_loss:1.0131 train_time:1740ms step_avg:33ms
step:52800/100000 avg_train_loss:1.0077 train_time:1744ms step_avg:33ms
step:52900/100000 avg_train_loss:1.0047 train_time:1747ms step_avg:33ms
step:53000/100000 avg_train_loss:1.0213 train_time:1750ms step_avg:33ms
step:53000/100000 val_loss:1.0440 train_time:1750.14s step_avg:33ms
step:53100/100000 avg_train_loss:1.0136 train_time:1753ms step_avg:33ms
step:53200/100000 avg_train_loss:1.0385 train_time:1757ms step_avg:33ms
step:53300/100000 avg_train_loss:1.0164 train_time:1760ms step_avg:33ms
step:53400/100000 avg_train_loss:1.0251 train_time:1763ms step_avg:33ms
step:53500/100000 avg_train_loss:1.0302 train_time:1767ms step_avg:33ms
step:53500/100000 val_loss:1.0435 train_time:1766.56s step_avg:33ms
step:53600/100000 avg_train_loss:1.0209 train_time:1770ms step_avg:33ms
step:53700/100000 avg_train_loss:0.9884 train_time:1773ms step_avg:33ms
step:53800/100000 avg_train_loss:1.0034 train_time:1776ms step_avg:33ms
step:53900/100000 avg_train_loss:1.0151 train_time:1780ms step_avg:33ms
step:54000/100000 avg_train_loss:1.0136 train_time:1783ms step_avg:33ms
step:54000/100000 val_loss:1.0431 train_time:1782.99s step_avg:33ms
step:54100/100000 avg_train_loss:1.0359 train_time:1786ms step_avg:33ms
step:54200/100000 avg_train_loss:1.0160 train_time:1790ms step_avg:33ms
step:54300/100000 avg_train_loss:1.0490 train_time:1793ms step_avg:33ms
step:54400/100000 avg_train_loss:1.0392 train_time:1796ms step_avg:33ms
step:54500/100000 avg_train_loss:1.0354 train_time:1799ms step_avg:33ms
step:54500/100000 val_loss:1.0426 train_time:1799.40s step_avg:33ms
step:54600/100000 avg_train_loss:1.0520 train_time:1803ms step_avg:33ms
step:54700/100000 avg_train_loss:1.0570 train_time:1806ms step_avg:33ms
step:54800/100000 avg_train_loss:1.0091 train_time:1809ms step_avg:33ms
step:54900/100000 avg_train_loss:1.0098 train_time:1813ms step_avg:33ms
step:55000/100000 avg_train_loss:1.0393 train_time:1816ms step_avg:33ms
step:55000/100000 val_loss:1.0428 train_time:1816.04s step_avg:33ms
step:55100/100000 avg_train_loss:1.0138 train_time:1819ms step_avg:33ms
step:55200/100000 avg_train_loss:1.0093 train_time:1823ms step_avg:33ms
step:55300/100000 avg_train_loss:1.0051 train_time:1826ms step_avg:33ms
step:55400/100000 avg_train_loss:1.0253 train_time:1829ms step_avg:33ms
step:55500/100000 avg_train_loss:0.9962 train_time:1833ms step_avg:33ms
step:55500/100000 val_loss:1.0418 train_time:1832.53s step_avg:33ms
step:55600/100000 avg_train_loss:1.0214 train_time:1836ms step_avg:33ms
step:55700/100000 avg_train_loss:1.0379 train_time:1839ms step_avg:33ms
step:55800/100000 avg_train_loss:1.0127 train_time:1842ms step_avg:33ms
step:55900/100000 avg_train_loss:0.9997 train_time:1846ms step_avg:33ms
step:56000/100000 avg_train_loss:1.0033 train_time:1850ms step_avg:33ms
step:56000/100000 val_loss:1.0409 train_time:1850.20s step_avg:33ms
step:56100/100000 avg_train_loss:0.9820 train_time:1853ms step_avg:33ms
step:56200/100000 avg_train_loss:1.0322 train_time:1857ms step_avg:33ms
step:56300/100000 avg_train_loss:1.0294 train_time:1860ms step_avg:33ms
step:56400/100000 avg_train_loss:1.0437 train_time:1863ms step_avg:33ms
step:56500/100000 avg_train_loss:0.9825 train_time:1867ms step_avg:33ms
step:56500/100000 val_loss:1.0412 train_time:1866.66s step_avg:33ms
step:56600/100000 avg_train_loss:1.0204 train_time:1870ms step_avg:33ms
step:56700/100000 avg_train_loss:0.9665 train_time:1873ms step_avg:33ms
step:56800/100000 avg_train_loss:1.0047 train_time:1877ms step_avg:33ms
step:56900/100000 avg_train_loss:0.9601 train_time:1880ms step_avg:33ms
step:57000/100000 avg_train_loss:0.9912 train_time:1883ms step_avg:33ms
step:57000/100000 val_loss:1.0398 train_time:1883.12s step_avg:33ms
step:57100/100000 avg_train_loss:1.0001 train_time:1886ms step_avg:33ms
step:57200/100000 avg_train_loss:1.0111 train_time:1890ms step_avg:33ms
step:57300/100000 avg_train_loss:1.0028 train_time:1893ms step_avg:33ms
step:57400/100000 avg_train_loss:1.0159 train_time:1896ms step_avg:33ms
step:57500/100000 avg_train_loss:1.0397 train_time:1900ms step_avg:33ms
step:57500/100000 val_loss:1.0398 train_time:1899.56s step_avg:33ms
step:57600/100000 avg_train_loss:1.0085 train_time:1903ms step_avg:33ms
step:57700/100000 avg_train_loss:1.0150 train_time:1906ms step_avg:33ms
step:57800/100000 avg_train_loss:1.0260 train_time:1909ms step_avg:33ms
step:57900/100000 avg_train_loss:1.0252 train_time:1913ms step_avg:33ms
step:58000/100000 avg_train_loss:1.0090 train_time:1916ms step_avg:33ms
step:58000/100000 val_loss:1.0391 train_time:1915.99s step_avg:33ms
step:58100/100000 avg_train_loss:0.9958 train_time:1919ms step_avg:33ms
step:58200/100000 avg_train_loss:1.0381 train_time:1923ms step_avg:33ms
step:58300/100000 avg_train_loss:0.9947 train_time:1926ms step_avg:33ms
step:58400/100000 avg_train_loss:1.0002 train_time:1929ms step_avg:33ms
step:58500/100000 avg_train_loss:0.9832 train_time:1932ms step_avg:33ms
step:58500/100000 val_loss:1.0393 train_time:1932.46s step_avg:33ms
step:58600/100000 avg_train_loss:1.0023 train_time:1936ms step_avg:33ms
step:58700/100000 avg_train_loss:1.0118 train_time:1939ms step_avg:33ms
step:58800/100000 avg_train_loss:0.9989 train_time:1942ms step_avg:33ms
step:58900/100000 avg_train_loss:1.0230 train_time:1946ms step_avg:33ms
step:59000/100000 avg_train_loss:0.9820 train_time:1949ms step_avg:33ms
step:59000/100000 val_loss:1.0379 train_time:1948.92s step_avg:33ms
step:59100/100000 avg_train_loss:0.9821 train_time:1952ms step_avg:33ms
step:59200/100000 avg_train_loss:0.9988 train_time:1956ms step_avg:33ms
step:59300/100000 avg_train_loss:0.9997 train_time:1959ms step_avg:33ms
step:59400/100000 avg_train_loss:0.9961 train_time:1962ms step_avg:33ms
step:59500/100000 avg_train_loss:0.9863 train_time:1965ms step_avg:33ms
step:59500/100000 val_loss:1.0367 train_time:1965.41s step_avg:33ms
step:59600/100000 avg_train_loss:1.0404 train_time:1969ms step_avg:33ms
step:59700/100000 avg_train_loss:1.0544 train_time:1972ms step_avg:33ms
step:59800/100000 avg_train_loss:1.0417 train_time:1975ms step_avg:33ms
step:59900/100000 avg_train_loss:1.0091 train_time:1979ms step_avg:33ms
step:60000/100000 avg_train_loss:0.9759 train_time:1982ms step_avg:33ms
step:60000/100000 val_loss:1.0367 train_time:1981.84s step_avg:33ms
step:60100/100000 avg_train_loss:0.9978 train_time:1985ms step_avg:33ms
step:60200/100000 avg_train_loss:1.0163 train_time:1988ms step_avg:33ms
step:60300/100000 avg_train_loss:0.9919 train_time:1992ms step_avg:33ms
step:60400/100000 avg_train_loss:0.9940 train_time:1995ms step_avg:33ms
step:60500/100000 avg_train_loss:1.0394 train_time:1998ms step_avg:33ms
step:60500/100000 val_loss:1.0356 train_time:1998.32s step_avg:33ms
step:60600/100000 avg_train_loss:1.0003 train_time:2002ms step_avg:33ms
step:60700/100000 avg_train_loss:1.0078 train_time:2005ms step_avg:33ms
step:60800/100000 avg_train_loss:1.0098 train_time:2008ms step_avg:33ms
step:60900/100000 avg_train_loss:1.0500 train_time:2011ms step_avg:33ms
step:61000/100000 avg_train_loss:0.9959 train_time:2015ms step_avg:33ms
step:61000/100000 val_loss:1.0356 train_time:2014.75s step_avg:33ms
step:61100/100000 avg_train_loss:1.0261 train_time:2018ms step_avg:33ms
step:61200/100000 avg_train_loss:0.9885 train_time:2021ms step_avg:33ms
step:61300/100000 avg_train_loss:1.0167 train_time:2025ms step_avg:33ms
step:61400/100000 avg_train_loss:1.0141 train_time:2028ms step_avg:33ms
step:61500/100000 avg_train_loss:1.0030 train_time:2031ms step_avg:33ms
step:61500/100000 val_loss:1.0358 train_time:2031.20s step_avg:33ms
step:61600/100000 avg_train_loss:0.9837 train_time:2034ms step_avg:33ms
step:61700/100000 avg_train_loss:0.9471 train_time:2038ms step_avg:33ms
step:61800/100000 avg_train_loss:1.0193 train_time:2041ms step_avg:33ms
step:61900/100000 avg_train_loss:1.0065 train_time:2044ms step_avg:33ms
step:62000/100000 avg_train_loss:0.9989 train_time:2048ms step_avg:33ms
step:62000/100000 val_loss:1.0357 train_time:2047.69s step_avg:33ms
step:62100/100000 avg_train_loss:0.9983 train_time:2051ms step_avg:33ms
step:62200/100000 avg_train_loss:0.9819 train_time:2054ms step_avg:33ms
step:62300/100000 avg_train_loss:1.0086 train_time:2058ms step_avg:33ms
step:62400/100000 avg_train_loss:1.0143 train_time:2061ms step_avg:33ms
step:62500/100000 avg_train_loss:1.0058 train_time:2064ms step_avg:33ms
step:62500/100000 val_loss:1.0345 train_time:2064.11s step_avg:33ms
step:62600/100000 avg_train_loss:1.0164 train_time:2067ms step_avg:33ms
step:62700/100000 avg_train_loss:1.0253 train_time:2071ms step_avg:33ms
step:62800/100000 avg_train_loss:0.9869 train_time:2074ms step_avg:33ms
step:62900/100000 avg_train_loss:1.0111 train_time:2077ms step_avg:33ms
step:63000/100000 avg_train_loss:0.9961 train_time:2081ms step_avg:33ms
step:63000/100000 val_loss:1.0345 train_time:2080.54s step_avg:33ms
step:63100/100000 avg_train_loss:0.9967 train_time:2084ms step_avg:33ms
step:63200/100000 avg_train_loss:0.9914 train_time:2087ms step_avg:33ms
step:63300/100000 avg_train_loss:0.9963 train_time:2090ms step_avg:33ms
step:63400/100000 avg_train_loss:1.0225 train_time:2094ms step_avg:33ms
step:63500/100000 avg_train_loss:1.0144 train_time:2097ms step_avg:33ms
step:63500/100000 val_loss:1.0344 train_time:2097.00s step_avg:33ms
step:63600/100000 avg_train_loss:1.0343 train_time:2100ms step_avg:33ms
step:63700/100000 avg_train_loss:1.0261 train_time:2104ms step_avg:33ms
step:63800/100000 avg_train_loss:1.0133 train_time:2107ms step_avg:33ms
step:63900/100000 avg_train_loss:1.0378 train_time:2110ms step_avg:33ms
step:64000/100000 avg_train_loss:1.0645 train_time:2113ms step_avg:33ms
step:64000/100000 val_loss:1.0351 train_time:2113.48s step_avg:33ms
step:64100/100000 avg_train_loss:0.9824 train_time:2117ms step_avg:33ms
step:64200/100000 avg_train_loss:1.0197 train_time:2120ms step_avg:33ms
step:64300/100000 avg_train_loss:1.0001 train_time:2123ms step_avg:33ms
step:64400/100000 avg_train_loss:1.0215 train_time:2127ms step_avg:33ms
step:64500/100000 avg_train_loss:1.0030 train_time:2130ms step_avg:33ms
step:64500/100000 val_loss:1.0339 train_time:2129.99s step_avg:33ms
step:64600/100000 avg_train_loss:0.9888 train_time:2133ms step_avg:33ms
step:64700/100000 avg_train_loss:1.0037 train_time:2137ms step_avg:33ms
step:64800/100000 avg_train_loss:1.0116 train_time:2140ms step_avg:33ms
step:64900/100000 avg_train_loss:0.9787 train_time:2143ms step_avg:33ms
step:65000/100000 avg_train_loss:1.0310 train_time:2146ms step_avg:33ms
step:65000/100000 val_loss:1.0341 train_time:2146.47s step_avg:33ms
step:65100/100000 avg_train_loss:1.0033 train_time:2150ms step_avg:33ms
step:65200/100000 avg_train_loss:0.9971 train_time:2153ms step_avg:33ms
step:65300/100000 avg_train_loss:0.9881 train_time:2158ms step_avg:33ms
step:65400/100000 avg_train_loss:0.9813 train_time:2161ms step_avg:33ms
step:65500/100000 avg_train_loss:1.0021 train_time:2164ms step_avg:33ms
step:65500/100000 val_loss:1.0334 train_time:2164.17s step_avg:33ms
step:65600/100000 avg_train_loss:1.0259 train_time:2167ms step_avg:33ms
step:65700/100000 avg_train_loss:1.0372 train_time:2171ms step_avg:33ms
step:65800/100000 avg_train_loss:0.9772 train_time:2174ms step_avg:33ms
step:65900/100000 avg_train_loss:1.0030 train_time:2177ms step_avg:33ms
step:66000/100000 avg_train_loss:0.9758 train_time:2181ms step_avg:33ms
step:66000/100000 val_loss:1.0334 train_time:2180.61s step_avg:33ms
step:66100/100000 avg_train_loss:0.9642 train_time:2184ms step_avg:33ms
step:66200/100000 avg_train_loss:0.9667 train_time:2187ms step_avg:33ms
step:66300/100000 avg_train_loss:0.9895 train_time:2190ms step_avg:33ms
step:66400/100000 avg_train_loss:0.9804 train_time:2194ms step_avg:33ms
step:66500/100000 avg_train_loss:0.9925 train_time:2197ms step_avg:33ms
step:66500/100000 val_loss:1.0324 train_time:2197.03s step_avg:33ms
step:66600/100000 avg_train_loss:1.0121 train_time:2200ms step_avg:33ms
step:66700/100000 avg_train_loss:0.9741 train_time:2204ms step_avg:33ms
step:66800/100000 avg_train_loss:1.0337 train_time:2207ms step_avg:33ms
step:66900/100000 avg_train_loss:0.9880 train_time:2210ms step_avg:33ms
step:67000/100000 avg_train_loss:1.0269 train_time:2213ms step_avg:33ms
step:67000/100000 val_loss:1.0320 train_time:2213.48s step_avg:33ms
step:67100/100000 avg_train_loss:1.0167 train_time:2217ms step_avg:33ms
step:67200/100000 avg_train_loss:1.0055 train_time:2220ms step_avg:33ms
step:67300/100000 avg_train_loss:1.0082 train_time:2223ms step_avg:33ms
step:67400/100000 avg_train_loss:0.9818 train_time:2227ms step_avg:33ms
step:67500/100000 avg_train_loss:1.0100 train_time:2230ms step_avg:33ms
step:67500/100000 val_loss:1.0318 train_time:2229.94s step_avg:33ms
step:67600/100000 avg_train_loss:1.0137 train_time:2233ms step_avg:33ms
step:67700/100000 avg_train_loss:0.9659 train_time:2237ms step_avg:33ms
step:67800/100000 avg_train_loss:0.9936 train_time:2240ms step_avg:33ms
step:67900/100000 avg_train_loss:0.9973 train_time:2243ms step_avg:33ms
step:68000/100000 avg_train_loss:0.9843 train_time:2246ms step_avg:33ms
step:68000/100000 val_loss:1.0320 train_time:2246.38s step_avg:33ms
step:68100/100000 avg_train_loss:0.9994 train_time:2250ms step_avg:33ms
step:68200/100000 avg_train_loss:0.9912 train_time:2253ms step_avg:33ms
step:68300/100000 avg_train_loss:0.9986 train_time:2256ms step_avg:33ms
step:68400/100000 avg_train_loss:0.9580 train_time:2260ms step_avg:33ms
step:68500/100000 avg_train_loss:0.9907 train_time:2263ms step_avg:33ms
step:68500/100000 val_loss:1.0311 train_time:2262.88s step_avg:33ms
step:68600/100000 avg_train_loss:0.9984 train_time:2266ms step_avg:33ms
step:68700/100000 avg_train_loss:0.9878 train_time:2269ms step_avg:33ms
step:68800/100000 avg_train_loss:0.9775 train_time:2273ms step_avg:33ms
step:68900/100000 avg_train_loss:1.0051 train_time:2276ms step_avg:33ms
step:69000/100000 avg_train_loss:1.0425 train_time:2279ms step_avg:33ms
step:69000/100000 val_loss:1.0305 train_time:2279.34s step_avg:33ms
step:69100/100000 avg_train_loss:1.0216 train_time:2283ms step_avg:33ms
step:69200/100000 avg_train_loss:1.0483 train_time:2286ms step_avg:33ms
step:69300/100000 avg_train_loss:0.9550 train_time:2289ms step_avg:33ms
step:69400/100000 avg_train_loss:0.9602 train_time:2293ms step_avg:33ms
step:69500/100000 avg_train_loss:1.0295 train_time:2296ms step_avg:33ms
step:69500/100000 val_loss:1.0297 train_time:2295.80s step_avg:33ms
step:69600/100000 avg_train_loss:0.9721 train_time:2299ms step_avg:33ms
step:69700/100000 avg_train_loss:0.9806 train_time:2302ms step_avg:33ms
step:69800/100000 avg_train_loss:1.0206 train_time:2306ms step_avg:33ms
step:69900/100000 avg_train_loss:1.0089 train_time:2309ms step_avg:33ms
step:70000/100000 avg_train_loss:1.0127 train_time:2312ms step_avg:33ms
step:70000/100000 val_loss:1.0296 train_time:2312.27s step_avg:33ms
step:70100/100000 avg_train_loss:0.9881 train_time:2316ms step_avg:33ms
step:70200/100000 avg_train_loss:1.0183 train_time:2319ms step_avg:33ms
step:70300/100000 avg_train_loss:1.0009 train_time:2322ms step_avg:33ms
step:70400/100000 avg_train_loss:0.9893 train_time:2325ms step_avg:33ms
step:70500/100000 avg_train_loss:1.0127 train_time:2329ms step_avg:33ms
step:70500/100000 val_loss:1.0296 train_time:2328.73s step_avg:33ms
step:70600/100000 avg_train_loss:0.9731 train_time:2332ms step_avg:33ms
step:70700/100000 avg_train_loss:1.0388 train_time:2335ms step_avg:33ms
step:70800/100000 avg_train_loss:0.9907 train_time:2339ms step_avg:33ms
step:70900/100000 avg_train_loss:0.9810 train_time:2342ms step_avg:33ms
step:71000/100000 avg_train_loss:0.9365 train_time:2345ms step_avg:33ms
step:71000/100000 val_loss:1.0293 train_time:2345.20s step_avg:33ms
step:71100/100000 avg_train_loss:0.9971 train_time:2348ms step_avg:33ms
step:71200/100000 avg_train_loss:1.0078 train_time:2352ms step_avg:33ms
step:71300/100000 avg_train_loss:0.9776 train_time:2355ms step_avg:33ms
step:71400/100000 avg_train_loss:0.9970 train_time:2358ms step_avg:33ms
step:71500/100000 avg_train_loss:0.9750 train_time:2362ms step_avg:33ms
step:71500/100000 val_loss:1.0288 train_time:2361.64s step_avg:33ms
step:71600/100000 avg_train_loss:0.9944 train_time:2365ms step_avg:33ms
step:71700/100000 avg_train_loss:1.0044 train_time:2368ms step_avg:33ms
step:71800/100000 avg_train_loss:0.9888 train_time:2371ms step_avg:33ms
step:71900/100000 avg_train_loss:1.0133 train_time:2375ms step_avg:33ms
step:72000/100000 avg_train_loss:0.9903 train_time:2378ms step_avg:33ms
step:72000/100000 val_loss:1.0286 train_time:2378.07s step_avg:33ms
step:72100/100000 avg_train_loss:1.0154 train_time:2381ms step_avg:33ms
step:72200/100000 avg_train_loss:0.9802 train_time:2385ms step_avg:33ms
step:72300/100000 avg_train_loss:0.9840 train_time:2388ms step_avg:33ms
step:72400/100000 avg_train_loss:0.9874 train_time:2391ms step_avg:33ms
step:72500/100000 avg_train_loss:0.9901 train_time:2395ms step_avg:33ms
step:72500/100000 val_loss:1.0284 train_time:2394.53s step_avg:33ms
step:72600/100000 avg_train_loss:0.9893 train_time:2398ms step_avg:33ms
step:72700/100000 avg_train_loss:1.0109 train_time:2401ms step_avg:33ms
step:72800/100000 avg_train_loss:1.0130 train_time:2404ms step_avg:33ms
step:72900/100000 avg_train_loss:1.0064 train_time:2408ms step_avg:33ms
step:73000/100000 avg_train_loss:1.0309 train_time:2411ms step_avg:33ms
step:73000/100000 val_loss:1.0294 train_time:2410.97s step_avg:33ms
step:73100/100000 avg_train_loss:1.0101 train_time:2414ms step_avg:33ms
step:73200/100000 avg_train_loss:1.0024 train_time:2418ms step_avg:33ms
step:73300/100000 avg_train_loss:1.0700 train_time:2421ms step_avg:33ms
step:73400/100000 avg_train_loss:0.9827 train_time:2424ms step_avg:33ms
step:73500/100000 avg_train_loss:1.0094 train_time:2427ms step_avg:33ms
step:73500/100000 val_loss:1.0284 train_time:2427.40s step_avg:33ms
step:73600/100000 avg_train_loss:0.9918 train_time:2431ms step_avg:33ms
step:73700/100000 avg_train_loss:1.0041 train_time:2434ms step_avg:33ms
step:73800/100000 avg_train_loss:1.0035 train_time:2437ms step_avg:33ms
step:73900/100000 avg_train_loss:0.9809 train_time:2441ms step_avg:33ms
step:74000/100000 avg_train_loss:0.9924 train_time:2444ms step_avg:33ms
step:74000/100000 val_loss:1.0281 train_time:2443.86s step_avg:33ms
step:74100/100000 avg_train_loss:0.9903 train_time:2447ms step_avg:33ms
step:74200/100000 avg_train_loss:1.0086 train_time:2450ms step_avg:33ms
step:74300/100000 avg_train_loss:0.9781 train_time:2454ms step_avg:33ms
step:74400/100000 avg_train_loss:1.0232 train_time:2457ms step_avg:33ms
step:74500/100000 avg_train_loss:0.9851 train_time:2460ms step_avg:33ms
step:74500/100000 val_loss:1.0286 train_time:2460.35s step_avg:33ms
step:74600/100000 avg_train_loss:0.9740 train_time:2464ms step_avg:33ms
step:74700/100000 avg_train_loss:0.9751 train_time:2468ms step_avg:33ms
step:74800/100000 avg_train_loss:0.9730 train_time:2471ms step_avg:33ms
step:74900/100000 avg_train_loss:1.0188 train_time:2475ms step_avg:33ms
step:75000/100000 avg_train_loss:1.0425 train_time:2478ms step_avg:33ms
step:75000/100000 val_loss:1.0275 train_time:2478.02s step_avg:33ms
step:75100/100000 avg_train_loss:0.9616 train_time:2481ms step_avg:33ms
step:75200/100000 avg_train_loss:0.9959 train_time:2485ms step_avg:33ms
step:75300/100000 avg_train_loss:0.9841 train_time:2488ms step_avg:33ms
step:75400/100000 avg_train_loss:0.9343 train_time:2491ms step_avg:33ms
step:75500/100000 avg_train_loss:0.9790 train_time:2494ms step_avg:33ms
step:75500/100000 val_loss:1.0264 train_time:2494.45s step_avg:33ms
step:75600/100000 avg_train_loss:0.9744 train_time:2498ms step_avg:33ms
step:75700/100000 avg_train_loss:0.9811 train_time:2501ms step_avg:33ms
step:75800/100000 avg_train_loss:0.9737 train_time:2504ms step_avg:33ms
step:75900/100000 avg_train_loss:1.0076 train_time:2508ms step_avg:33ms
step:76000/100000 avg_train_loss:0.9546 train_time:2511ms step_avg:33ms
step:76000/100000 val_loss:1.0272 train_time:2510.90s step_avg:33ms
step:76100/100000 avg_train_loss:1.0431 train_time:2514ms step_avg:33ms
step:76200/100000 avg_train_loss:0.9840 train_time:2517ms step_avg:33ms
step:76300/100000 avg_train_loss:1.0042 train_time:2521ms step_avg:33ms
step:76400/100000 avg_train_loss:0.9942 train_time:2524ms step_avg:33ms
step:76500/100000 avg_train_loss:1.0077 train_time:2527ms step_avg:33ms
step:76500/100000 val_loss:1.0265 train_time:2527.33s step_avg:33ms
step:76600/100000 avg_train_loss:0.9978 train_time:2531ms step_avg:33ms
step:76700/100000 avg_train_loss:0.9974 train_time:2534ms step_avg:33ms
step:76800/100000 avg_train_loss:0.9759 train_time:2537ms step_avg:33ms
step:76900/100000 avg_train_loss:1.0080 train_time:2540ms step_avg:33ms
step:77000/100000 avg_train_loss:0.9774 train_time:2544ms step_avg:33ms
step:77000/100000 val_loss:1.0271 train_time:2543.77s step_avg:33ms
step:77100/100000 avg_train_loss:0.9781 train_time:2547ms step_avg:33ms
step:77200/100000 avg_train_loss:0.9802 train_time:2550ms step_avg:33ms
step:77300/100000 avg_train_loss:0.9807 train_time:2554ms step_avg:33ms
step:77400/100000 avg_train_loss:0.9774 train_time:2557ms step_avg:33ms
step:77500/100000 avg_train_loss:0.9792 train_time:2560ms step_avg:33ms
step:77500/100000 val_loss:1.0267 train_time:2560.20s step_avg:33ms
step:77600/100000 avg_train_loss:1.0171 train_time:2563ms step_avg:33ms
step:77700/100000 avg_train_loss:0.9513 train_time:2567ms step_avg:33ms
step:77800/100000 avg_train_loss:0.9691 train_time:2570ms step_avg:33ms
step:77900/100000 avg_train_loss:0.9830 train_time:2573ms step_avg:33ms
step:78000/100000 avg_train_loss:0.9818 train_time:2577ms step_avg:33ms
step:78000/100000 val_loss:1.0251 train_time:2576.64s step_avg:33ms
step:78100/100000 avg_train_loss:0.9754 train_time:2580ms step_avg:33ms
step:78200/100000 avg_train_loss:0.9719 train_time:2583ms step_avg:33ms
step:78300/100000 avg_train_loss:1.0407 train_time:2586ms step_avg:33ms
step:78400/100000 avg_train_loss:1.0322 train_time:2590ms step_avg:33ms
step:78500/100000 avg_train_loss:1.0351 train_time:2593ms step_avg:33ms
step:78500/100000 val_loss:1.0255 train_time:2593.08s step_avg:33ms
step:78600/100000 avg_train_loss:0.9590 train_time:2596ms step_avg:33ms
step:78700/100000 avg_train_loss:0.9573 train_time:2600ms step_avg:33ms
step:78800/100000 avg_train_loss:1.0023 train_time:2603ms step_avg:33ms
step:78900/100000 avg_train_loss:0.9830 train_time:2606ms step_avg:33ms
step:79000/100000 avg_train_loss:0.9605 train_time:2610ms step_avg:33ms
step:79000/100000 val_loss:1.0244 train_time:2609.56s step_avg:33ms
step:79100/100000 avg_train_loss:1.0113 train_time:2613ms step_avg:33ms
step:79200/100000 avg_train_loss:1.0095 train_time:2616ms step_avg:33ms
step:79300/100000 avg_train_loss:0.9764 train_time:2619ms step_avg:33ms
step:79400/100000 avg_train_loss:1.0060 train_time:2623ms step_avg:33ms
step:79500/100000 avg_train_loss:0.9808 train_time:2626ms step_avg:33ms
step:79500/100000 val_loss:1.0242 train_time:2626.03s step_avg:33ms
step:79600/100000 avg_train_loss:1.0158 train_time:2629ms step_avg:33ms
step:79700/100000 avg_train_loss:0.9899 train_time:2633ms step_avg:33ms
step:79800/100000 avg_train_loss:0.9936 train_time:2636ms step_avg:33ms
step:79900/100000 avg_train_loss:0.9917 train_time:2639ms step_avg:33ms
step:80000/100000 avg_train_loss:1.0061 train_time:2643ms step_avg:33ms
step:80000/100000 val_loss:1.0250 train_time:2642.51s step_avg:33ms
step:80100/100000 avg_train_loss:0.9895 train_time:2646ms step_avg:33ms
step:80200/100000 avg_train_loss:0.9492 train_time:2649ms step_avg:33ms
step:80300/100000 avg_train_loss:0.9688 train_time:2652ms step_avg:33ms
step:80400/100000 avg_train_loss:0.9667 train_time:2656ms step_avg:33ms
step:80500/100000 avg_train_loss:0.9850 train_time:2659ms step_avg:33ms
step:80500/100000 val_loss:1.0248 train_time:2658.96s step_avg:33ms
step:80600/100000 avg_train_loss:0.9872 train_time:2662ms step_avg:33ms
step:80700/100000 avg_train_loss:0.9775 train_time:2666ms step_avg:33ms
step:80800/100000 avg_train_loss:0.9798 train_time:2669ms step_avg:33ms
step:80900/100000 avg_train_loss:0.9811 train_time:2672ms step_avg:33ms
step:81000/100000 avg_train_loss:0.9965 train_time:2675ms step_avg:33ms
step:81000/100000 val_loss:1.0249 train_time:2675.39s step_avg:33ms
step:81100/100000 avg_train_loss:0.9798 train_time:2679ms step_avg:33ms
step:81200/100000 avg_train_loss:1.0125 train_time:2682ms step_avg:33ms
step:81300/100000 avg_train_loss:0.9907 train_time:2685ms step_avg:33ms
step:81400/100000 avg_train_loss:0.9962 train_time:2689ms step_avg:33ms
step:81500/100000 avg_train_loss:0.9994 train_time:2692ms step_avg:33ms
step:81500/100000 val_loss:1.0242 train_time:2691.85s step_avg:33ms
step:81600/100000 avg_train_loss:0.9687 train_time:2695ms step_avg:33ms
step:81700/100000 avg_train_loss:0.9787 train_time:2698ms step_avg:33ms
step:81800/100000 avg_train_loss:0.9768 train_time:2702ms step_avg:33ms
step:81900/100000 avg_train_loss:0.9862 train_time:2705ms step_avg:33ms
step:82000/100000 avg_train_loss:0.9900 train_time:2708ms step_avg:33ms
step:82000/100000 val_loss:1.0239 train_time:2708.27s step_avg:33ms
step:82100/100000 avg_train_loss:1.0043 train_time:2712ms step_avg:33ms
step:82200/100000 avg_train_loss:1.0091 train_time:2715ms step_avg:33ms
step:82300/100000 avg_train_loss:1.0031 train_time:2718ms step_avg:33ms
step:82400/100000 avg_train_loss:1.0146 train_time:2721ms step_avg:33ms
step:82500/100000 avg_train_loss:0.9974 train_time:2725ms step_avg:33ms
step:82500/100000 val_loss:1.0240 train_time:2724.69s step_avg:33ms
step:82600/100000 avg_train_loss:1.0341 train_time:2728ms step_avg:33ms
step:82700/100000 avg_train_loss:1.0072 train_time:2731ms step_avg:33ms
step:82800/100000 avg_train_loss:0.9992 train_time:2735ms step_avg:33ms
step:82900/100000 avg_train_loss:0.9722 train_time:2738ms step_avg:33ms
step:83000/100000 avg_train_loss:1.0132 train_time:2741ms step_avg:33ms
step:83000/100000 val_loss:1.0239 train_time:2741.15s step_avg:33ms
step:83100/100000 avg_train_loss:0.9961 train_time:2744ms step_avg:33ms
step:83200/100000 avg_train_loss:0.9770 train_time:2748ms step_avg:33ms
step:83300/100000 avg_train_loss:0.9779 train_time:2751ms step_avg:33ms
step:83400/100000 avg_train_loss:0.9932 train_time:2754ms step_avg:33ms
step:83500/100000 avg_train_loss:0.9821 train_time:2758ms step_avg:33ms
step:83500/100000 val_loss:1.0236 train_time:2757.61s step_avg:33ms
step:83600/100000 avg_train_loss:0.9761 train_time:2761ms step_avg:33ms
step:83700/100000 avg_train_loss:1.0205 train_time:2764ms step_avg:33ms
step:83800/100000 avg_train_loss:0.9876 train_time:2767ms step_avg:33ms
step:83900/100000 avg_train_loss:0.9608 train_time:2771ms step_avg:33ms
step:84000/100000 avg_train_loss:0.9809 train_time:2775ms step_avg:33ms
step:84000/100000 val_loss:1.0235 train_time:2775.32s step_avg:33ms
step:84100/100000 avg_train_loss:0.9524 train_time:2779ms step_avg:33ms
step:84200/100000 avg_train_loss:1.0005 train_time:2782ms step_avg:33ms
step:84300/100000 avg_train_loss:1.0357 train_time:2785ms step_avg:33ms
step:84400/100000 avg_train_loss:0.9963 train_time:2788ms step_avg:33ms
step:84500/100000 avg_train_loss:0.9675 train_time:2792ms step_avg:33ms
step:84500/100000 val_loss:1.0237 train_time:2791.77s step_avg:33ms
step:84600/100000 avg_train_loss:0.9650 train_time:2795ms step_avg:33ms
step:84700/100000 avg_train_loss:0.9370 train_time:2798ms step_avg:33ms
step:84800/100000 avg_train_loss:0.9811 train_time:2802ms step_avg:33ms
step:84900/100000 avg_train_loss:0.9467 train_time:2805ms step_avg:33ms
step:85000/100000 avg_train_loss:0.9681 train_time:2808ms step_avg:33ms
step:85000/100000 val_loss:1.0226 train_time:2808.25s step_avg:33ms
step:85100/100000 avg_train_loss:0.9661 train_time:2812ms step_avg:33ms
step:85200/100000 avg_train_loss:1.0101 train_time:2815ms step_avg:33ms
step:85300/100000 avg_train_loss:0.9575 train_time:2818ms step_avg:33ms
step:85400/100000 avg_train_loss:0.9998 train_time:2821ms step_avg:33ms
step:85500/100000 avg_train_loss:1.0064 train_time:2825ms step_avg:33ms
step:85500/100000 val_loss:1.0226 train_time:2824.71s step_avg:33ms
step:85600/100000 avg_train_loss:0.9858 train_time:2828ms step_avg:33ms
step:85700/100000 avg_train_loss:0.9879 train_time:2831ms step_avg:33ms
step:85800/100000 avg_train_loss:1.0062 train_time:2835ms step_avg:33ms
step:85900/100000 avg_train_loss:0.9922 train_time:2838ms step_avg:33ms
step:86000/100000 avg_train_loss:0.9958 train_time:2841ms step_avg:33ms
step:86000/100000 val_loss:1.0226 train_time:2841.20s step_avg:33ms
step:86100/100000 avg_train_loss:0.9676 train_time:2844ms step_avg:33ms
step:86200/100000 avg_train_loss:0.9974 train_time:2848ms step_avg:33ms
step:86300/100000 avg_train_loss:0.9691 train_time:2851ms step_avg:33ms
step:86400/100000 avg_train_loss:0.9672 train_time:2854ms step_avg:33ms
step:86500/100000 avg_train_loss:0.9743 train_time:2858ms step_avg:33ms
step:86500/100000 val_loss:1.0223 train_time:2857.66s step_avg:33ms
step:86600/100000 avg_train_loss:0.9929 train_time:2861ms step_avg:33ms
step:86700/100000 avg_train_loss:0.9603 train_time:2864ms step_avg:33ms
step:86800/100000 avg_train_loss:0.9921 train_time:2868ms step_avg:33ms
step:86900/100000 avg_train_loss:0.9999 train_time:2871ms step_avg:33ms
step:87000/100000 avg_train_loss:0.9454 train_time:2874ms step_avg:33ms
step:87000/100000 val_loss:1.0219 train_time:2874.11s step_avg:33ms
step:87100/100000 avg_train_loss:0.9579 train_time:2877ms step_avg:33ms
step:87200/100000 avg_train_loss:0.9698 train_time:2881ms step_avg:33ms
step:87300/100000 avg_train_loss:0.9770 train_time:2884ms step_avg:33ms
step:87400/100000 avg_train_loss:0.9859 train_time:2887ms step_avg:33ms
step:87500/100000 avg_train_loss:0.9569 train_time:2891ms step_avg:33ms
step:87500/100000 val_loss:1.0212 train_time:2890.57s step_avg:33ms
step:87600/100000 avg_train_loss:1.0234 train_time:2894ms step_avg:33ms
step:87700/100000 avg_train_loss:1.0037 train_time:2897ms step_avg:33ms
step:87800/100000 avg_train_loss:1.0355 train_time:2900ms step_avg:33ms
step:87900/100000 avg_train_loss:0.9770 train_time:2904ms step_avg:33ms
step:88000/100000 avg_train_loss:0.9601 train_time:2907ms step_avg:33ms
step:88000/100000 val_loss:1.0214 train_time:2906.98s step_avg:33ms
step:88100/100000 avg_train_loss:0.9670 train_time:2910ms step_avg:33ms
step:88200/100000 avg_train_loss:0.9890 train_time:2914ms step_avg:33ms
step:88300/100000 avg_train_loss:0.9673 train_time:2917ms step_avg:33ms
step:88400/100000 avg_train_loss:0.9738 train_time:2920ms step_avg:33ms
step:88500/100000 avg_train_loss:1.0114 train_time:2923ms step_avg:33ms
step:88500/100000 val_loss:1.0209 train_time:2923.43s step_avg:33ms
step:88600/100000 avg_train_loss:0.9925 train_time:2927ms step_avg:33ms
step:88700/100000 avg_train_loss:0.9856 train_time:2930ms step_avg:33ms
step:88800/100000 avg_train_loss:0.9745 train_time:2933ms step_avg:33ms
step:88900/100000 avg_train_loss:1.0172 train_time:2937ms step_avg:33ms
step:89000/100000 avg_train_loss:0.9976 train_time:2940ms step_avg:33ms
step:89000/100000 val_loss:1.0210 train_time:2939.89s step_avg:33ms
step:89100/100000 avg_train_loss:0.9837 train_time:2943ms step_avg:33ms
step:89200/100000 avg_train_loss:0.9683 train_time:2946ms step_avg:33ms
step:89300/100000 avg_train_loss:1.0103 train_time:2950ms step_avg:33ms
step:89400/100000 avg_train_loss:0.9764 train_time:2953ms step_avg:33ms
step:89500/100000 avg_train_loss:0.9662 train_time:2956ms step_avg:33ms
step:89500/100000 val_loss:1.0211 train_time:2956.31s step_avg:33ms
step:89600/100000 avg_train_loss:0.9586 train_time:2960ms step_avg:33ms
step:89700/100000 avg_train_loss:0.9262 train_time:2963ms step_avg:33ms
step:89800/100000 avg_train_loss:0.9981 train_time:2966ms step_avg:33ms
step:89900/100000 avg_train_loss:0.9839 train_time:2970ms step_avg:33ms
step:90000/100000 avg_train_loss:0.9746 train_time:2973ms step_avg:33ms
step:90000/100000 val_loss:1.0217 train_time:2972.82s step_avg:33ms
step:90100/100000 avg_train_loss:0.9715 train_time:2976ms step_avg:33ms
step:90200/100000 avg_train_loss:0.9598 train_time:2979ms step_avg:33ms
step:90300/100000 avg_train_loss:0.9905 train_time:2983ms step_avg:33ms
step:90400/100000 avg_train_loss:0.9819 train_time:2986ms step_avg:33ms
step:90500/100000 avg_train_loss:0.9935 train_time:2989ms step_avg:33ms
step:90500/100000 val_loss:1.0201 train_time:2989.27s step_avg:33ms
step:90600/100000 avg_train_loss:0.9951 train_time:2993ms step_avg:33ms
step:90700/100000 avg_train_loss:0.9907 train_time:2996ms step_avg:33ms
step:90800/100000 avg_train_loss:0.9793 train_time:2999ms step_avg:33ms
step:90900/100000 avg_train_loss:0.9882 train_time:3002ms step_avg:33ms
step:91000/100000 avg_train_loss:0.9594 train_time:3006ms step_avg:33ms
step:91000/100000 val_loss:1.0206 train_time:3005.71s step_avg:33ms
step:91100/100000 avg_train_loss:0.9731 train_time:3009ms step_avg:33ms
step:91200/100000 avg_train_loss:0.9761 train_time:3012ms step_avg:33ms
step:91300/100000 avg_train_loss:0.9772 train_time:3016ms step_avg:33ms
step:91400/100000 avg_train_loss:1.0081 train_time:3019ms step_avg:33ms
step:91500/100000 avg_train_loss:0.9844 train_time:3022ms step_avg:33ms
step:91500/100000 val_loss:1.0206 train_time:3022.17s step_avg:33ms
step:91600/100000 avg_train_loss:1.0111 train_time:3025ms step_avg:33ms
step:91700/100000 avg_train_loss:1.0104 train_time:3029ms step_avg:33ms
step:91800/100000 avg_train_loss:0.9981 train_time:3032ms step_avg:33ms
step:91900/100000 avg_train_loss:1.0190 train_time:3035ms step_avg:33ms
step:92000/100000 avg_train_loss:1.0284 train_time:3039ms step_avg:33ms
step:92000/100000 val_loss:1.0209 train_time:3038.60s step_avg:33ms
step:92100/100000 avg_train_loss:0.9733 train_time:3042ms step_avg:33ms
step:92200/100000 avg_train_loss:0.9774 train_time:3045ms step_avg:33ms
step:92300/100000 avg_train_loss:0.9978 train_time:3048ms step_avg:33ms
step:92400/100000 avg_train_loss:0.9735 train_time:3052ms step_avg:33ms
step:92500/100000 avg_train_loss:0.9919 train_time:3055ms step_avg:33ms
step:92500/100000 val_loss:1.0201 train_time:3055.04s step_avg:33ms
step:92600/100000 avg_train_loss:0.9713 train_time:3058ms step_avg:33ms
step:92700/100000 avg_train_loss:0.9916 train_time:3062ms step_avg:33ms
step:92800/100000 avg_train_loss:0.9696 train_time:3065ms step_avg:33ms
step:92900/100000 avg_train_loss:0.9830 train_time:3068ms step_avg:33ms
step:93000/100000 avg_train_loss:0.9945 train_time:3071ms step_avg:33ms
step:93000/100000 val_loss:1.0199 train_time:3071.48s step_avg:33ms
step:93100/100000 avg_train_loss:0.9878 train_time:3075ms step_avg:33ms
step:93200/100000 avg_train_loss:0.9696 train_time:3078ms step_avg:33ms
step:93300/100000 avg_train_loss:0.9740 train_time:3083ms step_avg:33ms
step:93400/100000 avg_train_loss:0.9471 train_time:3086ms step_avg:33ms
step:93500/100000 avg_train_loss:0.9925 train_time:3089ms step_avg:33ms
step:93500/100000 val_loss:1.0207 train_time:3089.18s step_avg:33ms
step:93600/100000 avg_train_loss:1.0014 train_time:3092ms step_avg:33ms
step:93700/100000 avg_train_loss:0.9972 train_time:3096ms step_avg:33ms
step:93800/100000 avg_train_loss:0.9654 train_time:3099ms step_avg:33ms
step:93900/100000 avg_train_loss:0.9897 train_time:3102ms step_avg:33ms
step:94000/100000 avg_train_loss:0.9434 train_time:3106ms step_avg:33ms
step:94000/100000 val_loss:1.0200 train_time:3105.60s step_avg:33ms
step:94100/100000 avg_train_loss:0.9623 train_time:3109ms step_avg:33ms
step:94200/100000 avg_train_loss:0.9331 train_time:3112ms step_avg:33ms
step:94300/100000 avg_train_loss:0.9645 train_time:3115ms step_avg:33ms
step:94400/100000 avg_train_loss:0.9557 train_time:3119ms step_avg:33ms
step:94500/100000 avg_train_loss:0.9769 train_time:3122ms step_avg:33ms
step:94500/100000 val_loss:1.0189 train_time:3122.02s step_avg:33ms
step:94600/100000 avg_train_loss:0.9812 train_time:3125ms step_avg:33ms
step:94700/100000 avg_train_loss:0.9794 train_time:3129ms step_avg:33ms
step:94800/100000 avg_train_loss:1.0009 train_time:3132ms step_avg:33ms
step:94900/100000 avg_train_loss:0.9730 train_time:3135ms step_avg:33ms
step:95000/100000 avg_train_loss:0.9971 train_time:3138ms step_avg:33ms
step:95000/100000 val_loss:1.0192 train_time:3138.48s step_avg:33ms
step:95100/100000 avg_train_loss:0.9889 train_time:3142ms step_avg:33ms
step:95200/100000 avg_train_loss:0.9946 train_time:3145ms step_avg:33ms
step:95300/100000 avg_train_loss:0.9821 train_time:3148ms step_avg:33ms
step:95400/100000 avg_train_loss:0.9687 train_time:3152ms step_avg:33ms
step:95500/100000 avg_train_loss:0.9974 train_time:3155ms step_avg:33ms
step:95500/100000 val_loss:1.0191 train_time:3154.90s step_avg:33ms
step:95600/100000 avg_train_loss:0.9824 train_time:3158ms step_avg:33ms
step:95700/100000 avg_train_loss:0.9620 train_time:3161ms step_avg:33ms
step:95800/100000 avg_train_loss:0.9436 train_time:3165ms step_avg:33ms
step:95900/100000 avg_train_loss:0.9843 train_time:3168ms step_avg:33ms
step:96000/100000 avg_train_loss:0.9753 train_time:3171ms step_avg:33ms
step:96000/100000 val_loss:1.0192 train_time:3171.34s step_avg:33ms
step:96100/100000 avg_train_loss:0.9690 train_time:3175ms step_avg:33ms
step:96200/100000 avg_train_loss:0.9864 train_time:3178ms step_avg:33ms
step:96300/100000 avg_train_loss:0.9567 train_time:3181ms step_avg:33ms
step:96400/100000 avg_train_loss:0.9505 train_time:3184ms step_avg:33ms
step:96500/100000 avg_train_loss:0.9689 train_time:3188ms step_avg:33ms
step:96500/100000 val_loss:1.0187 train_time:3187.77s step_avg:33ms
step:96600/100000 avg_train_loss:0.9777 train_time:3191ms step_avg:33ms
step:96700/100000 avg_train_loss:0.9698 train_time:3194ms step_avg:33ms
step:96800/100000 avg_train_loss:0.9555 train_time:3198ms step_avg:33ms
step:96900/100000 avg_train_loss:1.0030 train_time:3201ms step_avg:33ms
step:97000/100000 avg_train_loss:1.0191 train_time:3204ms step_avg:33ms
step:97000/100000 val_loss:1.0181 train_time:3204.20s step_avg:33ms
step:97100/100000 avg_train_loss:1.0194 train_time:3207ms step_avg:33ms
step:97200/100000 avg_train_loss:0.9795 train_time:3211ms step_avg:33ms
step:97300/100000 avg_train_loss:0.9490 train_time:3214ms step_avg:33ms
step:97400/100000 avg_train_loss:0.9587 train_time:3217ms step_avg:33ms
step:97500/100000 avg_train_loss:1.0021 train_time:3221ms step_avg:33ms
step:97500/100000 val_loss:1.0183 train_time:3220.66s step_avg:33ms
step:97600/100000 avg_train_loss:0.9544 train_time:3224ms step_avg:33ms
step:97700/100000 avg_train_loss:0.9629 train_time:3227ms step_avg:33ms
step:97800/100000 avg_train_loss:1.0028 train_time:3231ms step_avg:33ms
step:97900/100000 avg_train_loss:0.9887 train_time:3234ms step_avg:33ms
step:98000/100000 avg_train_loss:0.9793 train_time:3237ms step_avg:33ms
step:98000/100000 val_loss:1.0179 train_time:3237.10s step_avg:33ms
step:98100/100000 avg_train_loss:0.9715 train_time:3240ms step_avg:33ms
step:98200/100000 avg_train_loss:1.0229 train_time:3244ms step_avg:33ms
step:98300/100000 avg_train_loss:0.9621 train_time:3247ms step_avg:33ms
step:98400/100000 avg_train_loss:0.9950 train_time:3250ms step_avg:33ms
step:98500/100000 avg_train_loss:0.9672 train_time:3254ms step_avg:33ms
step:98500/100000 val_loss:1.0181 train_time:3253.52s step_avg:33ms
step:98600/100000 avg_train_loss:0.9759 train_time:3257ms step_avg:33ms
step:98700/100000 avg_train_loss:0.9907 train_time:3260ms step_avg:33ms
step:98800/100000 avg_train_loss:0.9809 train_time:3263ms step_avg:33ms
step:98900/100000 avg_train_loss:0.9607 train_time:3267ms step_avg:33ms
step:99000/100000 avg_train_loss:0.9097 train_time:3270ms step_avg:33ms
step:99000/100000 val_loss:1.0183 train_time:3269.97s step_avg:33ms
step:99100/100000 avg_train_loss:0.9971 train_time:3273ms step_avg:33ms
step:99200/100000 avg_train_loss:0.9799 train_time:3277ms step_avg:33ms
step:99300/100000 avg_train_loss:0.9663 train_time:3280ms step_avg:33ms
step:99400/100000 avg_train_loss:0.9660 train_time:3283ms step_avg:33ms
step:99500/100000 avg_train_loss:0.9574 train_time:3286ms step_avg:33ms
step:99500/100000 val_loss:1.0178 train_time:3286.44s step_avg:33ms
step:99600/100000 avg_train_loss:0.9877 train_time:3290ms step_avg:33ms
step:99700/100000 avg_train_loss:0.9804 train_time:3293ms step_avg:33ms
step:99800/100000 avg_train_loss:0.9763 train_time:3296ms step_avg:33ms
step:99900/100000 avg_train_loss:0.9838 train_time:3300ms step_avg:33ms
step:100000/100000 avg_train_loss:1.0269 train_time:3303ms step_avg:33ms
step:100000/100000 val_loss:1.0351 train_time:3302.89s step_avg:33ms
Total training time: 3516.61s
